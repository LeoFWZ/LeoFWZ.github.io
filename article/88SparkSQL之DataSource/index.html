<!DOCTYPE html>
<html lang="en">

<!-- Head tag -->
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="google-site-verification" content="xBT4GhYoi5qRD5tr338pgPM5OWHHIDR6mNg1a3euekI" />
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="Every failure is leading towards success">
    <meta name="keyword"  content="Fangwuzhou,BigData, Hadoop,Java,Python,Spark,Flink,Kafka">
    <link rel="shortcut icon" href="/img/ironman-draw.png">
    <!-- Place this tag in your head or just before your close body tag. -->
    <script async defer src="https://buttons.github.io/buttons.js"></script>
    <title>
        
          [SparkSQL之DataSource]   - Leo@FWZ| Blog
        
    </title>

    <link rel="canonical" href="https://leofwz.github.io/article/88SparkSQL之DataSource/">

    <!-- Bootstrap Core CSS -->
    <link rel="stylesheet" href="/css/bootstrap.min.css">

    <!-- Custom CSS --> 
    <link rel="stylesheet" href="/css/beantech.min.css">

    <link rel="stylesheet" href="/css/donate.css">
    
    <!-- Pygments Highlight CSS -->
    <link rel="stylesheet" href="/css/highlight.css">

    <link rel="stylesheet" href="/css/widget.css">

    <link rel="stylesheet" href="/css/rocket.css">

    <link rel="stylesheet" href="/css/signature.css">

    <link rel="stylesheet" href="/css/toc.css">

    <!-- Custom Fonts -->
    <!-- <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css" rel="stylesheet" type="text/css"> -->
    <!-- Hux change font-awesome CDN to qiniu -->
    <link href="https://cdn.staticfile.org/font-awesome/4.5.0/css/font-awesome.min.css" rel="stylesheet" type="text/css">


    <!-- Hux Delete, sad but pending in China
    <link href='http://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
    <link href='http://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/
    css'>
    -->


    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

    <!-- ga & ba script hoook -->
    <script></script>
</head>


<!-- hack iOS CSS :active style -->
<body ontouchstart="">
	<!-- Modified by Yu-Hsuan Yen -->
<!-- Post Header -->
<style type="text/css">
    header.intro-header{
        
            background-image: url('/img/article_header/article_header.png')
            /*post*/
        
    }
    
</style>

<header class="intro-header" >
    <!-- Signature -->
    <div id="signature">
        <div class="container">
            <div class="row">
                <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                
                    <div class="post-heading">
                        <div class="tags">
                            
                              <a class="tag" href="/tags/#Spark" title="Spark">Spark</a>
                            
                        </div>
                        <h1>[SparkSQL之DataSource]  </h1>
                        <h2 class="subheading"></h2>
                        <span class="meta">
                            Posted by FWZ on
                            2018-10-10
                        </span>
                    </div>
                


                </div>
            </div>
        </div>
    </div>
</header>

	
    <!-- Navigation -->
<nav class="navbar navbar-default navbar-custom navbar-fixed-top">
    <div class="container-fluid">
        <!-- Brand and toggle get grouped for better mobile display -->
        <div class="navbar-header page-scroll">
            <button type="button" class="navbar-toggle">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="/">Leo@FWZ</a>
        </div>

        <!-- Collect the nav links, forms, and other content for toggling -->
        <!-- Known Issue, found by Hux:
            <nav>'s height woule be hold on by its content.
            so, when navbar scale out, the <nav> will cover tags.
            also mask any touch event of tags, unfortunately.
        -->
        <div id="huxblog_navbar">
            <div class="navbar-collapse">
                <ul class="nav navbar-nav navbar-right">
                    <li>
                        <a href="/">Home</a>
                    </li>

                    

                        
                    

                        
                        <li>
                            <a href="/about/">About</a>
                        </li>
                        
                    

                        
                        <li>
                            <a href="/tags/">Tags</a>
                        </li>
                        
                    

                        
                        <li>
                            <a href="/archive/">Archives</a>
                        </li>
                        
                    
                    
                </ul>
            </div>
        </div>
        <!-- /.navbar-collapse -->
    </div>
    <!-- /.container -->
</nav>
<script>
    // Drop Bootstarp low-performance Navbar
    // Use customize navbar with high-quality material design animation
    // in high-perf jank-free CSS3 implementation
    var $body   = document.body;
    var $toggle = document.querySelector('.navbar-toggle');
    var $navbar = document.querySelector('#huxblog_navbar');
    var $collapse = document.querySelector('.navbar-collapse');

    $toggle.addEventListener('click', handleMagic)
    function handleMagic(e){
        if ($navbar.className.indexOf('in') > 0) {
        // CLOSE
            $navbar.className = " ";
            // wait until animation end.
            setTimeout(function(){
                // prevent frequently toggle
                if($navbar.className.indexOf('in') < 0) {
                    $collapse.style.height = "0px"
                }
            },400)
        }else{
        // OPEN
            $collapse.style.height = "auto"
            $navbar.className += " in";
        }
    }
</script>


    <!-- Main Content -->
    <!-- Modify by Yu-Hsuan Yen -->

<!-- Post Content -->
<article>
    <div class="container">
        <div class="row">

            <!-- Post Container -->
            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                post-container">

                <p><img src="https://img-blog.csdnimg.cn/2020031712211922.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzIxMjM2NQ==,size_16,color_FFFFFF,t_70" alt="image"></p>
<p>你用MapReduce、Spark、Hive去处理源数据，处理之后输出出来。输入和输出的数据文件的格式可能有很多种，比如文本、json、ORC、Parquet格式的文件等。另外这些数据可能在本地、hdfs、S3上等。另外还有数据是否压缩等。如果以上这些情况都会出现，那么处理起来就比较麻烦，要混合起来处理。</p>
<p>另外字段也可能从10、11、12，一直在变，那样就就要重新实现业务逻辑，这时候就可以用Data SourceAPI来读不同文件系统上面的文件，读进来就变成了一个DataFrame或者Dataset，这时候就可以随便使用SQL/DS DF API，可以读可以写，当然有两种数据源：Build-in、第三方(需要引入jar包)，我们这里先看内置的</p>
<p><strong>这样就有了外部数据源（Spark1.2版本出来的）。</strong></p>
<p>Spark之外有很多数据源，这些数据源要给Spark去处理，就需要通过Spark的DataSource来转换一下，转换之后就变成了DataFrame或Dataset了，后面处理就简单了。另外DataFrame或Dataset也可以输出为外部数据源。可以把DataSource理解为DataFrame和外部数据源的连接点。</p>
<p>Spark SQL通过DataFrame 接口可以操作大量的数据源。DataFrame 可以被相关的算子操作，还可以被用来创建临时的视图。把DataFrame 注册为一张临时的视图，你就可以在这些数据上运行SQL查询了。本节讲的是用Spark的DataSource加载数据源以及保存数据源，以及操作spark内置的数据源。</p>
<h2 id="parquet-文件">Parquet 文件</h2>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">官网：http://spark.apache.org/docs/latest/sql-data-sources-parquet.html#parquet-files</span><br></pre></td></tr></table></figure>
<h3 id="加载parquet文件">加载Parquet文件</h3>
<p>Parquet 是列式存储的一种文件类型，许多数据处理系统都支持Parquet格式。<br>
Spark SQL对于Parquet 文件的写和读都支持。读的话变成DataFrame，写的话变成Parquet 文件。Spark SQL会自动识别Parquet 文件里的schema。写Parquet 文件时，由于兼容性原因，所有列都自动转换为可以为空的列。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">//建议标准写法</span><br><span class="line"></span><br><span class="line">//读取json文件</span><br><span class="line">spark.read.json()    //简化写法，读取json文件</span><br><span class="line">spark.read.format(&quot;json&quot;).load()   //标准写法。你写json，就是读取json文件，写可以写其它格式文件</span><br><span class="line"></span><br><span class="line">//源码，json() 调用的就是format</span><br><span class="line">def json(paths: String*): DataFrame = format(&quot;json&quot;).load(paths : _*)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">peopleDF.write.parquet(&quot;people.parquet&quot;)  //简写</span><br><span class="line">peopleDF.write.format(&quot;parquet&quot;).save(&quot;&quot;)  //标准写法</span><br></pre></td></tr></table></figure>
<p>小知识点：数据源格式需要指定全名（例如：org.apache.spark.sql.parquet），如果数据源格式为内置格式，则只需要指定简称json, parquet, jdbc, orc, libsvm, csv, text来指定数据的格式。</p>
<p>举例：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line">//读取一个json文件</span><br><span class="line">scala&gt; val peopleDF = spark.read.format(&quot;json&quot;).load(&quot;file:///home/hadoop/app/spark-2.4.2-bin-2.6.0-cdh5.7.0/examples/src/main/resources/people.json&quot;)</span><br><span class="line">peopleDF: org.apache.spark.sql.DataFrame = [age: bigint, name: string]</span><br><span class="line"></span><br><span class="line">scala&gt; peopleDF.show</span><br><span class="line">+----+-------+</span><br><span class="line">| age|   name|</span><br><span class="line">+----+-------+</span><br><span class="line">|null|Michael|</span><br><span class="line">|  30|   Andy|</span><br><span class="line">|  19| Justin|</span><br><span class="line">+----+-------+</span><br><span class="line"></span><br><span class="line">//写parquet到本地</span><br><span class="line">scala&gt; peopleDF.write.format(&quot;parquet&quot;).save(&quot;file:///home/hadoop/data/peopleParquet&quot;)</span><br><span class="line"></span><br><span class="line">//本地文件</span><br><span class="line">[hadoop@hadoop001 peopleParquet]$ pwd</span><br><span class="line">/home/hadoop/data/peopleParquet</span><br><span class="line">[hadoop@hadoop001 peopleParquet]$ ls</span><br><span class="line">part-00000-3e25fdbf-d928-4b63-b587-12fe9eb1fc2b-c000.snappy.parquet  _SUCCESS</span><br><span class="line"></span><br><span class="line">//Parquet是看不到的，不过我们可以再读进来，读本地parquet</span><br><span class="line">scala&gt; val parquetDF = spark.read.format(&quot;parquet&quot;).load(&quot;file:///home/hadoop/data/peopleParquet/&quot;)</span><br><span class="line">parquetDF: org.apache.spark.sql.DataFrame = [age: bigint, name: string]</span><br><span class="line">scala&gt; parquetDF.show</span><br><span class="line">+----+-------+</span><br><span class="line">| age|   name|</span><br><span class="line">+----+-------+</span><br><span class="line">|null|Michael|</span><br><span class="line">|  30|   Andy|</span><br><span class="line">|  19| Justin|</span><br><span class="line">+----+-------+</span><br><span class="line"></span><br><span class="line">scala&gt; peopleDF.createOrReplaceTempView(&quot;people&quot;)</span><br><span class="line"></span><br><span class="line">scala&gt; spark.sql(&quot;select * from people&quot;).show</span><br><span class="line">WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException</span><br><span class="line">+----+-------+</span><br><span class="line">| age|   name|</span><br><span class="line">+----+-------+</span><br><span class="line">|null|Michael|</span><br><span class="line">|  30|   Andy|</span><br><span class="line">|  19| Justin|</span><br><span class="line">+----+-------+</span><br><span class="line"></span><br><span class="line">scala&gt; peopleDF.map(x =&gt; &quot;name&quot; + x(1)).show</span><br><span class="line">+-----------+</span><br><span class="line">|      value|</span><br><span class="line">+-----------+</span><br><span class="line">|nameMichael|</span><br><span class="line">|   nameAndy|</span><br><span class="line">| nameJustin|</span><br><span class="line">+-----------+</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">//另外，spark.sql可以直接作用在文件之上</span><br><span class="line">scala&gt; spark.sql(&quot;SELECT * FROM parquet.`hdfs://hadoop001:9000/tmp/users.parquet`&quot;).show</span><br><span class="line">19/07/04 22:28:18 WARN ObjectStore: Failed to get database parquet, returning NoSuchObjectException</span><br><span class="line">+------+--------------+----------------+</span><br><span class="line">|  name|favorite_color|favorite_numbers|</span><br><span class="line">+------+--------------+----------------+</span><br><span class="line">|Alyssa|          null|  [3, 9, 15, 20]|</span><br><span class="line">|   Ben|           red|              []|</span><br><span class="line">+------+--------------+----------------+</span><br></pre></td></tr></table></figure>
<p>这里想一个问题saprk.read.load(&quot;&quot;)里面发生了什么</p>
<p><img src="https://img-blog.csdnimg.cn/20200318132546461.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzIxMjM2NQ==,size_16,color_FFFFFF,t_70" alt="image"></p>
<p>load里面是一个option，然后跟上一个路径，然后load,上面的路径在我们自己定义的时候也是写一个路径</p>
<p><img src="https://img-blog.csdnimg.cn/20200318132651875.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzIxMjM2NQ==,size_16,color_FFFFFF,t_70" alt="image"></p>
<p>我们再把load点进去是一个可变参数，也就是可以传多个</p>
<p>首先判断一下source，这个source是什么呢</p>
<p>是一个默认的datasource，那Spark默认的datasource是什么呢，我们点进去看看</p>
<p><img src="https://img-blog.csdnimg.cn/20200318132932379.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzIxMjM2NQ==,size_16,color_FFFFFF,t_70" alt="image"></p>
<p>然后我们会发现一个参数spark.sql.sources.default</p>
<p>这里可以发现SparkSQL里面默认的source是parquet</p>
<h3 id="partition-discovery-分区探测">Partition Discovery 分区探测</h3>
<p>像Hive里面的表分区，表的分区是一种通用的优化方法。在一个分区表里，数据存储在不同的目录下，分区的列值作为每个分区目录的一部分。Spark所有内置的数据源（包括Text/CSV/JSON/ORC/Parquet），都能够被探测，而且自动推断分区信息。</p>
<p>举例：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">//创建分区</span><br><span class="line">[hadoop@hadoop001 data]$ hdfs dfs -mkdir -p /sparksql/table/gender=male/county=US </span><br><span class="line">[hadoop@hadoop001 data]$ hdfs dfs -mkdir -p /sparksql/table/gender=male/county=CN</span><br><span class="line">[hadoop@hadoop001 data]$ hdfs dfs -mkdir -p /sparksql/table/gender=female/county=CN</span><br><span class="line">[hadoop@hadoop001 data]$ hdfs dfs -mkdir -p /sparksql/table/gender=female/county=US</span><br><span class="line"></span><br><span class="line">//把parquet数据放进某个分区下面</span><br><span class="line">[hadoop@hadoop001 resources]$ hdfs dfs -put users.parquet  /sparksql/table/gender=male/county=US/</span><br><span class="line"></span><br><span class="line">//从这里看到前面三列是数据文件本身有的列</span><br><span class="line">//后面两列是根据分区目录名自动推导的列名</span><br><span class="line">scala&gt; spark.read.format(&quot;parquet&quot;).load(&quot;hdfs://hadoop001:9000/sparksql/table&quot;).printSchema</span><br><span class="line">root</span><br><span class="line"> |-- name: string (nullable = true)</span><br><span class="line"> |-- favorite_color: string (nullable = true)</span><br><span class="line"> |-- favorite_numbers: array (nullable = true)</span><br><span class="line"> |    |-- element: integer (containsNull = true)</span><br><span class="line"> |-- gender: string (nullable = true)</span><br><span class="line"> |-- county: string (nullable = true)</span><br><span class="line"></span><br><span class="line">scala&gt; spark.read.format(&quot;parquet&quot;).load(&quot;hdfs://hadoop001:9000/sparksql/table&quot;)..show</span><br><span class="line">+------+--------------+----------------+------+------+</span><br><span class="line">|  name|favorite_color|favorite_numbers|gender|county|</span><br><span class="line">+------+--------------+----------------+------+------+</span><br><span class="line">|Alyssa|          null|  [3, 9, 15, 20]|  male|    US|</span><br><span class="line">|   Ben|           red|              []|  male|    US|</span><br><span class="line">+------+--------------+----------------+------+------+</span><br></pre></td></tr></table></figure>
<p>把path/to/table这个路径传递给 SparkSession.read.parquet或者SparkSession.read.load，Spark SQL会自动从路径中提取partition分区的信息。</p>
<p><img src="https://img-blog.csdnimg.cn/20190930160719358.png" alt="image"></p>
<p>请注意，分区列的数据类型是自动推断的。目前，支持数字数据类型、日期、时间戳和字符串类型。有时，用户可能不想自动推断分区列的数据类型。对于这些用例，可以通过spark.sql.sources.partitionColumnTypeInference.enabled配置自动类型推断，默认值为true。当类型推断被禁用时，字符串类型将用于分区列。</p>
<p>从spark 1.6.0开始，默认情况下，分区探测仅查找给定路径下的分区，就给定的路径下往下查找。对于上面的示例，如果用户将path/to/table/gender=male传递给SparkSession.read.parquet或SparkSession.read.load，则gender不会被视为分区列。如果用户需要指定分区发现应该使用的基本路径，则可以在数据源选项中设置基本路径。例如，当path/to/table/gender=male是数据的路径，并且用户将basepath设置为path/to/table/时，gender将是分区列。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">//如果你继续写路径到table的下一层gender=male，那么可看到如下：</span><br><span class="line">scala&gt; spark.read.format(&quot;parquet&quot;).load(&quot;hdfs://hadoop001:9000/sparksql/table/gender=male&quot;).printSchema</span><br><span class="line">root</span><br><span class="line"> |-- name: string (nullable = true)</span><br><span class="line"> |-- favorite_color: string (nullable = true)</span><br><span class="line"> |-- favorite_numbers: array (nullable = true)</span><br><span class="line"> |    |-- element: integer (containsNull = true)</span><br><span class="line"> |-- county: string (nullable = true)</span><br><span class="line"></span><br><span class="line">scala&gt; spark.read.format(&quot;parquet&quot;).load(&quot;hdfs://hadoop001:9000/sparksql/table/gender=male&quot;).show</span><br><span class="line">+------+--------------+----------------+------+</span><br><span class="line">|  name|favorite_color|favorite_numbers|county|</span><br><span class="line">+------+--------------+----------------+------+</span><br><span class="line">|Alyssa|          null|  [3, 9, 15, 20]|    US|</span><br><span class="line">|   Ben|           red|              []|    US|</span><br><span class="line">+------+--------------+----------------+------+</span><br></pre></td></tr></table></figure>
<h3 id="schema-合并">Schema 合并</h3>
<p>可以把多个parquet文件的schema合并，了解下，具体看官网。不建议用，因为它是 expensive operation。用户可以获取多个有不同Schema但相互兼容的Parquet文件。现在Parquet数据源能自动检测这种情况，并合并这些文件的schemas。</p>
<p>因为Schema合并是一个高消耗的操作，在大多数情况下并不需要，所以Spark SQL从1.5.0 开始默认关闭了该功能。如何开启该功能，具体看官网。</p>
<p>至于如何操作可以参考官网：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">val squaresDF = spark.sparkContext.makeRDD(1 to 5).map(i =&gt; (i, i * i)).toDF(&quot;value&quot;, &quot;square&quot;)</span><br><span class="line"></span><br><span class="line">val cubesDF = spark.sparkContext.makeRDD(6 to 10).map(i =&gt; (i, i * i * i)).toDF(&quot;value&quot;, &quot;cube&quot;)</span><br><span class="line">cubesDF.write.parquet(&quot;data/test_table/key=2&quot;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">val mergedDF = spark.read.option(&quot;mergeSchema&quot;, &quot;true&quot;).parquet(&quot;data/test_table&quot;)</span><br><span class="line">mergedDF.printSchema()</span><br></pre></td></tr></table></figure>
<h2 id="orc文件">ORC文件</h2>
<p>了解一下。<br>
Since Spark 2.3, Spark supports a vectorized ORC reader with a new ORC file format for ORC files. To do that, the following configurations are newly added. The vectorized reader is used for the native ORC tables (e.g., the ones created using the clause USING ORC) when spark.sql.orc.impl is set to native and spark.sql.orc.enableVectorizedReader is set to true. For the Hive ORC serde tables (e.g., the ones created using the clause USING HIVE OPTIONS (fileFormat ‘ORC’)), the vectorized reader is used when spark.sql.hive.convertMetastoreOrc is also set to true.</p>
<p>vectorized ORC向量化可以了解一下</p>
<p>实际上，一般用这种方式就可以了：</p>
<p>spark.read.format(“orc”).load(orc文件的路径)</p>
<h2 id="hive表">Hive表</h2>
<p>参考官网：<a href="http://spark.apache.org/docs/latest/sql-data-sources-hive-tables.html#hive-tables" target="_blank" rel="noopener">http://spark.apache.org/docs/latest/sql-data-sources-hive-tables.html#hive-tables</a></p>
<blockquote>
<p>下面这段话引用：<a href="https://blog.51cto.com/14309075/2411816" target="_blank" rel="noopener">https://blog.51cto.com/14309075/2411816</a></p>
</blockquote>
<p>Apache Hive是Hadoop上的SQL引擎，Spark SQL编译时可以包含Hive支持，也可以不包含。包含Hive支持的Spark SQL可以支持Hive表访问、UDF(用户自定义函数)以及 Hive 查询语言(HiveQL/HQL)等。需要强调的 一点是，如果要在Spark SQL中包含Hive的库，并不需要事先安装Hive。一般来说，最好还是在编译Spark SQL时引入Hive支持，这样就可以使用这些特性了。如果你下载的是二进制版本的 Spark，它应该已经在编译时添加了 Hive 支持。</p>
<p>若要把Spark SQL连接到一个部署好的Hive上，你必须把hive-site.xml复制到 Spark的配置文件目录中($SPARK_HOME/conf)。即使没有部署好Hive，Spark SQL也可以运行。</p>
<p>需要注意的是，如果你没有部署好Hive，Spark SQL会在当前的工作目录中创建出自己的Hive 元数据仓库，叫作 metastore_db。此外，如果你尝试使用 HiveQL 中的 CREATE TABLE (并非 CREATE EXTERNAL TABLE)语句来创建表，这些表会被放在你默认的文件系统中的 /user/hive/warehouse 目录中(如果你的 classpath 中有配好的 hdfs-site.xml，默认的文件系统就是 HDFS，否则就是本地文件系统)。</p>
<p>举例：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br></pre></td><td class="code"><pre><span class="line">import java.io.File</span><br><span class="line">import org.apache.spark.sql.&#123;Row, SaveMode, SparkSession&#125;</span><br><span class="line"></span><br><span class="line">case class Record(key: Int, value: String)</span><br><span class="line"></span><br><span class="line">// warehouseLocation points to the default location for managed databases and tables</span><br><span class="line">val warehouseLocation = new File(&quot;spark-warehouse&quot;).getAbsolutePath</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">//enableHiveSupport()要开启,本地不要运行，会有一堆问题</span><br><span class="line">val spark = SparkSession</span><br><span class="line">  .builder()</span><br><span class="line">  .appName(&quot;Spark Hive Example&quot;)</span><br><span class="line">  .config(&quot;spark.sql.warehouse.dir&quot;, warehouseLocation)  //warehouseLocation)这一步不一定要指定路径</span><br><span class="line">  .enableHiveSupport()</span><br><span class="line">  .getOrCreate()</span><br><span class="line"></span><br><span class="line">import spark.implicits._</span><br><span class="line">import spark.sql</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">sql(&quot;CREATE TABLE IF NOT EXISTS src (key INT, value STRING) USING hive&quot;)</span><br><span class="line">sql(&quot;LOAD DATA LOCAL INPATH &apos;examples/src/main/resources/kv1.txt&apos; INTO TABLE src&quot;)</span><br><span class="line"></span><br><span class="line">// Queries are expressed in HiveQL</span><br><span class="line">sql(&quot;SELECT * FROM src&quot;).show()</span><br><span class="line">// +---+-------+</span><br><span class="line">// |key|  value|</span><br><span class="line">// +---+-------+</span><br><span class="line">// |238|val_238|</span><br><span class="line">// | 86| val_86|</span><br><span class="line">// |311|val_311|</span><br><span class="line">// ...</span><br><span class="line"></span><br><span class="line">// Aggregation queries are also supported.</span><br><span class="line">sql(&quot;SELECT COUNT(*) FROM src&quot;).show()</span><br><span class="line">// +--------+</span><br><span class="line">// |count(1)|</span><br><span class="line">// +--------+</span><br><span class="line">// |    500 |</span><br><span class="line">// +--------+</span><br><span class="line"></span><br><span class="line">// The results of SQL queries are themselves DataFrames and support all normal functions.</span><br><span class="line">val sqlDF = sql(&quot;SELECT key, value FROM src WHERE key &lt; 10 ORDER BY key&quot;)</span><br><span class="line"></span><br><span class="line">// The items in DataFrames are of type Row, which allows you to access each column by ordinal.</span><br><span class="line">val stringsDS = sqlDF.map &#123;</span><br><span class="line">  case Row(key: Int, value: String) =&gt; s&quot;Key: $key, Value: $value&quot;</span><br><span class="line">&#125;</span><br><span class="line">stringsDS.show()</span><br><span class="line">// +--------------------+</span><br><span class="line">// |               value|</span><br><span class="line">// +--------------------+</span><br><span class="line">// |Key: 0, Value: val_0|</span><br><span class="line">// |Key: 0, Value: val_0|</span><br><span class="line">// |Key: 0, Value: val_0|</span><br><span class="line">// ...</span><br><span class="line"></span><br><span class="line">// You can also use DataFrames to create temporary views within a SparkSession.</span><br><span class="line">val recordsDF = spark.createDataFrame((1 to 100).map(i =&gt; Record(i, s&quot;val_$i&quot;)))</span><br><span class="line">recordsDF.createOrReplaceTempView(&quot;records&quot;)</span><br><span class="line"></span><br><span class="line">// Queries can then join DataFrame data with data stored in Hive.</span><br><span class="line">sql(&quot;SELECT * FROM records r JOIN src s ON r.key = s.key&quot;).show()</span><br><span class="line">// +---+------+---+------+</span><br><span class="line">// |key| value|key| value|</span><br><span class="line">// +---+------+---+------+</span><br><span class="line">// |  2| val_2|  2| val_2|</span><br><span class="line">// |  4| val_4|  4| val_4|</span><br><span class="line">// |  5| val_5|  5| val_5|</span><br><span class="line">// ...</span><br><span class="line"></span><br><span class="line">// Create a Hive managed Parquet table, with HQL syntax instead of the Spark SQL native syntax</span><br><span class="line">// `USING hive`</span><br><span class="line">sql(&quot;CREATE TABLE hive_records(key int, value string) STORED AS PARQUET&quot;)</span><br><span class="line">// Save DataFrame to the Hive managed table</span><br><span class="line">val df = spark.table(&quot;src&quot;)</span><br><span class="line">df.write.mode(SaveMode.Overwrite).saveAsTable(&quot;hive_records&quot;)</span><br><span class="line">// After insertion, the Hive managed table has data now</span><br><span class="line">sql(&quot;SELECT * FROM hive_records&quot;).show()</span><br><span class="line">// +---+-------+</span><br><span class="line">// |key|  value|</span><br><span class="line">// +---+-------+</span><br><span class="line">// |238|val_238|</span><br><span class="line">// | 86| val_86|</span><br><span class="line">// |311|val_311|</span><br><span class="line">// ...</span><br><span class="line"></span><br><span class="line">// Prepare a Parquet data directory</span><br><span class="line">val dataDir = &quot;/tmp/parquet_data&quot;</span><br><span class="line">spark.range(10).write.parquet(dataDir)</span><br><span class="line">// Create a Hive external Parquet table</span><br><span class="line">sql(s&quot;CREATE EXTERNAL TABLE hive_ints(key int) STORED AS PARQUET LOCATION &apos;$dataDir&apos;&quot;)</span><br><span class="line">// The Hive external table should already have data</span><br><span class="line">sql(&quot;SELECT * FROM hive_ints&quot;).show()</span><br><span class="line">// +---+</span><br><span class="line">// |key|</span><br><span class="line">// +---+</span><br><span class="line">// |  0|</span><br><span class="line">// |  1|</span><br><span class="line">// |  2|</span><br><span class="line">// ...</span><br><span class="line"></span><br><span class="line">// Turn on flag for Hive Dynamic Partitioning</span><br><span class="line">spark.sqlContext.setConf(&quot;hive.exec.dynamic.partition&quot;, &quot;true&quot;)</span><br><span class="line">spark.sqlContext.setConf(&quot;hive.exec.dynamic.partition.mode&quot;, &quot;nonstrict&quot;)</span><br><span class="line">// Create a Hive partitioned table using DataFrame API</span><br><span class="line">df.write.partitionBy(&quot;key&quot;).format(&quot;hive&quot;).saveAsTable(&quot;hive_part_tbl&quot;)</span><br><span class="line">// Partitioned column `key` will be moved to the end of the schema.</span><br><span class="line">sql(&quot;SELECT * FROM hive_part_tbl&quot;).show()</span><br><span class="line">// +-------+---+</span><br><span class="line">// |  value|key|</span><br><span class="line">// +-------+---+</span><br><span class="line">// |val_238|238|</span><br><span class="line">// | val_86| 86|</span><br><span class="line">// |val_311|311|</span><br><span class="line">// ...</span><br><span class="line"></span><br><span class="line">spark.stop()</span><br></pre></td></tr></table></figure>
<p>spark-shell中测试：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; import org.apache.spark.sql.&#123;Row, SaveMode, SparkSession&#125;</span><br><span class="line">import org.apache.spark.sql.&#123;Row, SaveMode, SparkSession&#125;</span><br><span class="line"></span><br><span class="line">scala&gt; import spark.implicits._</span><br><span class="line">import spark.implicits._</span><br><span class="line"></span><br><span class="line">scala&gt; import spark.sql</span><br><span class="line">import spark.sql</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">//自动的找到hive的warehouse，然后把表放进去</span><br><span class="line">scala&gt; sql(&quot;CREATE TABLE IF NOT EXISTS src (key INT, value STRING) USING hive&quot;)</span><br><span class="line"> WARN HiveMetaStore: Location: hdfs://hadoop001:9000/user/hive/warehouse/src specified for non-external table:src</span><br><span class="line">Warning: fs.defaultFS is not set when running &quot;chgrp&quot; command.</span><br><span class="line">Warning: fs.defaultFS is not set when running &quot;chmod&quot; command.</span><br><span class="line">res0: org.apache.spark.sql.DataFrame = []</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">scala&gt; sql(&quot;LOAD DATA LOCAL INPATH &apos;/home/hadoop/app/spark-2.4.2-bin-2.6.0-cdh5.7.0/examples/src/main/resources/kv1.txt&apos; INTO TABLE src&quot;)</span><br><span class="line">Warning: fs.defaultFS is not set when running &quot;chgrp&quot; command.</span><br><span class="line">Warning: fs.defaultFS is not set when running &quot;chmod&quot; command.</span><br><span class="line">19/07/04 00:14:58 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException</span><br><span class="line">res1: org.apache.spark.sql.DataFrame = []</span><br><span class="line"></span><br><span class="line">scala&gt; sql(&quot;SELECT * FROM src&quot;).show()</span><br><span class="line">+---+-------+</span><br><span class="line">|key|  value|</span><br><span class="line">+---+-------+</span><br><span class="line">|238|val_238|</span><br><span class="line">| 86| val_86|</span><br><span class="line">|311|val_311|</span><br><span class="line">| 27| val_27|</span><br><span class="line">|165|val_165|</span><br><span class="line">|409|val_409|</span><br><span class="line">|255|val_255|</span><br><span class="line">|278|val_278|</span><br><span class="line">| 98| val_98|</span><br><span class="line">|484|val_484|</span><br><span class="line">|265|val_265|</span><br><span class="line">|193|val_193|</span><br><span class="line">|401|val_401|</span><br><span class="line">|150|val_150|</span><br><span class="line">|273|val_273|</span><br><span class="line">|224|val_224|</span><br><span class="line">|369|val_369|</span><br><span class="line">| 66| val_66|</span><br><span class="line">|128|val_128|</span><br><span class="line">|213|val_213|</span><br><span class="line">+---+-------+</span><br><span class="line">only showing top 20 rows</span><br><span class="line"></span><br><span class="line">scala&gt; sql(&quot;show tables&quot;).show</span><br><span class="line">+--------+------------+-----------+</span><br><span class="line">|database|   tableName|isTemporary|</span><br><span class="line">+--------+------------+-----------+</span><br><span class="line">| default|        dept|      false|</span><br><span class="line">| default|         emp|      false|</span><br><span class="line">| default|sparksqltest|      false|</span><br><span class="line">| default|         src|      false|</span><br><span class="line">+--------+------------+-----------+</span><br><span class="line"></span><br><span class="line">scala&gt; sql(&quot;select count(*) from src&quot;).show</span><br><span class="line">+--------+</span><br><span class="line">|count(1)|</span><br><span class="line">+--------+</span><br><span class="line">|     500|</span><br><span class="line">+--------+</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">scala&gt; val sqlDF = sql(&quot;SELECT key, value FROM src WHERE key &lt; 10 ORDER BY key&quot;)</span><br><span class="line">sqlDF: org.apache.spark.sql.DataFrame = [key: int, value: string]</span><br><span class="line"></span><br><span class="line">scala&gt; sqlDF.show</span><br><span class="line">+---+-----+</span><br><span class="line">|key|value|</span><br><span class="line">+---+-----+</span><br><span class="line">|  0|val_0|</span><br><span class="line">|  0|val_0|</span><br><span class="line">|  0|val_0|</span><br><span class="line">|  2|val_2|</span><br><span class="line">|  4|val_4|</span><br><span class="line">|  5|val_5|</span><br><span class="line">|  5|val_5|</span><br><span class="line">|  5|val_5|</span><br><span class="line">|  8|val_8|</span><br><span class="line">|  9|val_9|</span><br><span class="line">+---+-----+</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">//DataFrame中的项属于Row类型，允许您按序号访问每一列。</span><br><span class="line">scala&gt; val stringsDS = sqlDF.map &#123;</span><br><span class="line">     |   case Row(key: Int, value: String) =&gt; s&quot;Key: $key, Value: $value&quot;</span><br><span class="line">     | &#125;</span><br><span class="line">stringsDS: org.apache.spark.sql.Dataset[String] = [value: string]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">scala&gt; stringsDS.show()</span><br><span class="line">+--------------------+</span><br><span class="line">|               value|</span><br><span class="line">+--------------------+</span><br><span class="line">|Key: 0, Value: val_0|</span><br><span class="line">|Key: 0, Value: val_0|</span><br><span class="line">|Key: 0, Value: val_0|</span><br><span class="line">|Key: 2, Value: val_2|</span><br><span class="line">|Key: 4, Value: val_4|</span><br><span class="line">|Key: 5, Value: val_5|</span><br><span class="line">|Key: 5, Value: val_5|</span><br><span class="line">|Key: 5, Value: val_5|</span><br><span class="line">|Key: 8, Value: val_8|</span><br><span class="line">|Key: 9, Value: val_9|</span><br><span class="line">+--------------------+</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">scala&gt; val recordDF = spark.createDataFrame( (1 to 10).map(x =&gt; Record( x , s&quot;value_$x&quot;) ) )</span><br><span class="line">recordDF: org.apache.spark.sql.DataFrame = [key: int, value: string]</span><br><span class="line"></span><br><span class="line">scala&gt; recordDF.show</span><br><span class="line">+---+--------+</span><br><span class="line">|key|   value|</span><br><span class="line">+---+--------+</span><br><span class="line">|  1| value_1|</span><br><span class="line">|  2| value_2|</span><br><span class="line">|  3| value_3|</span><br><span class="line">|  4| value_4|</span><br><span class="line">|  5| value_5|</span><br><span class="line">|  6| value_6|</span><br><span class="line">|  7| value_7|</span><br><span class="line">|  8| value_8|</span><br><span class="line">|  9| value_9|</span><br><span class="line">| 10|value_10|</span><br><span class="line">+---+--------+</span><br><span class="line"></span><br><span class="line">scala&gt; recordDF.createOrReplaceTempView(&quot;records&quot;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">scala&gt; sql(&quot;select * from src&quot;).show</span><br><span class="line">+---+-------+</span><br><span class="line">|key|  value|</span><br><span class="line">+---+-------+</span><br><span class="line">|238|val_238|</span><br><span class="line">| 86| val_86|</span><br><span class="line">|311|val_311|</span><br><span class="line">| 27| val_27|</span><br><span class="line">|165|val_165|</span><br><span class="line">|409|val_409|</span><br><span class="line">|255|val_255|</span><br><span class="line">|278|val_278|</span><br><span class="line">| 98| val_98|</span><br><span class="line">|484|val_484|</span><br><span class="line">|265|val_265|</span><br><span class="line">|193|val_193|</span><br><span class="line">|401|val_401|</span><br><span class="line">|150|val_150|</span><br><span class="line">|273|val_273|</span><br><span class="line">|224|val_224|</span><br><span class="line">|369|val_369|</span><br><span class="line">| 66| val_66|</span><br><span class="line">|128|val_128|</span><br><span class="line">|213|val_213|</span><br><span class="line">+---+-------+</span><br><span class="line">only showing top 20 rows</span><br><span class="line"></span><br><span class="line">scala&gt; sql(&quot;select * from records r join src s on r.key=s.key&quot;).show</span><br><span class="line">+---+--------+---+------+</span><br><span class="line">|key|   value|key| value|</span><br><span class="line">+---+--------+---+------+</span><br><span class="line">|  4| value_4|  4| val_4|</span><br><span class="line">|  8| value_8|  8| val_8|</span><br><span class="line">| 10|value_10| 10|val_10|</span><br><span class="line">|  5| value_5|  5| val_5|</span><br><span class="line">|  5| value_5|  5| val_5|</span><br><span class="line">|  2| value_2|  2| val_2|</span><br><span class="line">|  5| value_5|  5| val_5|</span><br><span class="line">|  9| value_9|  9| val_9|</span><br><span class="line">+---+--------+---+------+</span><br><span class="line"></span><br><span class="line">//创建一张hive表，存储格式为parquet，另外可以看到它自动创建到hive数据库里了</span><br><span class="line">scala&gt; sql(&quot;CREATE TABLE hive_records(key int, value string) STORED AS PARQUET&quot;)</span><br><span class="line">19/07/04 23:46:44 WARN HiveMetaStore: Location: hdfs://hadoop001:9000/user/hive/warehouse/hive_records specified for non-external table:hive_records</span><br><span class="line">Warning: fs.defaultFS is not set when running &quot;chgrp&quot; command.</span><br><span class="line">Warning: fs.defaultFS is not set when running &quot;chmod&quot; command.</span><br><span class="line">res44: org.apache.spark.sql.DataFrame = []</span><br><span class="line"></span><br><span class="line">//把hivesrc表转换成一个DataFrame</span><br><span class="line">scala&gt; val df = spark.table(&quot;src&quot;)</span><br><span class="line">df: org.apache.spark.sql.DataFrame = [key: int, value: string]</span><br><span class="line"></span><br><span class="line">//保存模式为重写（有好几种，可以看官网），把上面的df这个DataFrame数据保存到hive表里</span><br><span class="line">scala&gt; df.write.mode(SaveMode.Overwrite).saveAsTable(&quot;hive_records&quot;)</span><br><span class="line">Warning: fs.defaultFS is not set when running &quot;chgrp&quot; command.</span><br><span class="line">Warning: fs.defaultFS is not set when running &quot;chmod&quot; command.</span><br><span class="line"></span><br><span class="line">scala&gt; sql(&quot;SELECT * FROM hive_records&quot;).show()</span><br><span class="line">+---+-------+</span><br><span class="line">|key|  value|</span><br><span class="line">+---+-------+</span><br><span class="line">|238|val_238|</span><br><span class="line">| 86| val_86|</span><br><span class="line">|311|val_311|</span><br><span class="line">| 27| val_27|</span><br><span class="line">|165|val_165|</span><br><span class="line">|409|val_409|</span><br><span class="line">|255|val_255|</span><br><span class="line">|278|val_278|</span><br><span class="line">| 98| val_98|</span><br><span class="line">|484|val_484|</span><br><span class="line">|265|val_265|</span><br><span class="line">|193|val_193|</span><br><span class="line">|401|val_401|</span><br><span class="line">|150|val_150|</span><br><span class="line">|273|val_273|</span><br><span class="line">|224|val_224|</span><br><span class="line">|369|val_369|</span><br><span class="line">| 66| val_66|</span><br><span class="line">|128|val_128|</span><br><span class="line">|213|val_213|</span><br><span class="line">+---+-------+</span><br><span class="line">only showing top 20 rows</span><br><span class="line"></span><br><span class="line">//准备一个parquet数据目录</span><br><span class="line">scala&gt; val dataDir = &quot;/home/hadoop/data/parquet_data&quot;</span><br><span class="line">dataDir: String = /home/hadoop/data/parquet_data</span><br><span class="line"></span><br><span class="line">把1到9折9个数字，分别写到上面parquet文件目录中去</span><br><span class="line">scala&gt; spark.range(10).write.parquet(dataDir)</span><br><span class="line"></span><br><span class="line">scala&gt; spark.read.parquet(&quot;/home/hadoop/data/parquet_data&quot;).show</span><br><span class="line">+---+</span><br><span class="line">| id|</span><br><span class="line">+---+</span><br><span class="line">|  5|</span><br><span class="line">|  6|</span><br><span class="line">|  7|</span><br><span class="line">|  8|</span><br><span class="line">|  9|</span><br><span class="line">|  0|</span><br><span class="line">|  1|</span><br><span class="line">|  2|</span><br><span class="line">|  3|</span><br><span class="line">|  4|</span><br><span class="line">+---+</span><br><span class="line"></span><br><span class="line">//创建一张存储格式为parquet的外部表，指定路径为上面准备好的路径</span><br><span class="line">scala&gt; sql(s&quot;CREATE EXTERNAL TABLE hive_ints(key int) STORED AS PARQUET LOCATION &apos;$dataDir&apos;&quot;)</span><br><span class="line">res64: org.apache.spark.sql.DataFrame = []</span><br><span class="line"></span><br><span class="line">//这里全是null值，不知道为什么</span><br><span class="line">scala&gt; sql(&quot;SELECT * FROM hive_ints&quot;).show()</span><br><span class="line">+----+</span><br><span class="line">| key|</span><br><span class="line">+----+</span><br><span class="line">|null|</span><br><span class="line">|null|</span><br><span class="line">|null|</span><br><span class="line">|null|</span><br><span class="line">|null|</span><br><span class="line">|null|</span><br><span class="line">|null|</span><br><span class="line">|null|</span><br><span class="line">|null|</span><br><span class="line">|null|</span><br><span class="line">+----+</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">scala&gt; df.show</span><br><span class="line">+---+-------+</span><br><span class="line">|key|  value|</span><br><span class="line">+---+-------+</span><br><span class="line">|238|val_238|</span><br><span class="line">| 86| val_86|</span><br><span class="line">|311|val_311|</span><br><span class="line">| 27| val_27|</span><br><span class="line">|165|val_165|</span><br><span class="line">|409|val_409|</span><br><span class="line">|255|val_255|</span><br><span class="line">|278|val_278|</span><br><span class="line">| 98| val_98|</span><br><span class="line">|484|val_484|</span><br><span class="line">|265|val_265|</span><br><span class="line">|193|val_193|</span><br><span class="line">|401|val_401|</span><br><span class="line">|150|val_150|</span><br><span class="line">|273|val_273|</span><br><span class="line">|224|val_224|</span><br><span class="line">|369|val_369|</span><br><span class="line">| 66| val_66|</span><br><span class="line">|128|val_128|</span><br><span class="line">|213|val_213|</span><br><span class="line">+---+-------+</span><br><span class="line">only showing top 20 rows</span><br><span class="line"></span><br><span class="line">//把hive动态分区开关打开</span><br><span class="line">scala&gt; spark.sqlContext.setConf(&quot;hive.exec.dynamic.partition&quot;, &quot;true&quot;)</span><br><span class="line"></span><br><span class="line">scala&gt; spark.sqlContext.setConf(&quot;hive.exec.dynamic.partition.mode&quot;, &quot;nonstrict&quot;)</span><br><span class="line"></span><br><span class="line">//用DataFrame API创建动态分区表</span><br><span class="line">scala&gt;df.write.partitionBy(&quot;key&quot;).format(&quot;hive&quot;).saveAsTable(&quot;hive_part_tbl&quot;)</span><br><span class="line">。。。。</span><br><span class="line">Warning: fs.defaultFS is not set when running &quot;chgrp&quot; command.</span><br><span class="line">Warning: fs.defaultFS is not set when running &quot;chmod&quot; command.</span><br><span class="line">Warning: fs.defaultFS is not set when running &quot;chgrp&quot; command.</span><br><span class="line">Warning: fs.defaultFS is not set when running &quot;chmod&quot; command.</span><br><span class="line"> WARN log: Updating partition stats fast for: hive_part_tbl</span><br><span class="line"> WARN log: Updated size to 8</span><br><span class="line">Warning: fs.defaultFS is not set when running &quot;chgrp&quot; command.</span><br><span class="line">Warning: fs.defaultFS is not set when running &quot;chmod&quot; command.</span><br><span class="line">Warning: fs.defaultFS is not set when running &quot;chgrp&quot; command.</span><br><span class="line">Warning: fs.defaultFS is not set when running &quot;chmod&quot; command.</span><br><span class="line">。。。。</span><br><span class="line"></span><br><span class="line">//分区的列 (这里是&apos;key&apos;，将会移动到schema的最后结尾</span><br><span class="line">scala&gt; sql(&quot;SELECT * FROM hive_part_tbl&quot;).show()</span><br><span class="line">+-------+---+</span><br><span class="line">|  value|key|</span><br><span class="line">+-------+---+</span><br><span class="line">|val_125|125|</span><br><span class="line">|val_125|125|</span><br><span class="line">|val_374|374|</span><br><span class="line">|  val_8|  8|</span><br><span class="line">|val_432|432|</span><br><span class="line">|val_200|200|</span><br><span class="line">|val_200|200|</span><br><span class="line">|val_497|497|</span><br><span class="line">| val_41| 41|</span><br><span class="line">|val_164|164|</span><br><span class="line">|val_164|164|</span><br><span class="line">|val_316|316|</span><br><span class="line">|val_316|316|</span><br><span class="line">|val_316|316|</span><br><span class="line">|val_417|417|</span><br><span class="line">|val_417|417|</span><br><span class="line">|val_417|417|</span><br><span class="line">| val_37| 37|</span><br><span class="line">| val_37| 37|</span><br><span class="line">|val_338|338|</span><br><span class="line">+-------+---+</span><br><span class="line">only showing top 20 rows</span><br><span class="line"></span><br><span class="line">[hadoop@hadoop001 data]$ hdfs dfs -ls hdfs://hadoop001:9000/user/hive/warehouse/hive_part_tbl</span><br><span class="line">Found 309 items</span><br><span class="line">drwxr-xr-x   - hadoop supergroup          0  hdfs://hadoop001:9000/user/hive/warehouse/hive_part_tbl/key=0</span><br><span class="line">drwxr-xr-x   - hadoop supergroup          0  hdfs://hadoop001:9000/user/hive/warehouse/hive_part_tbl/key=10</span><br><span class="line">drwxr-xr-x   - hadoop supergroup          0  hdfs://hadoop001:9000/user/hive/warehouse/hive_part_tbl/key=100</span><br><span class="line">drwxr-xr-x   - hadoop supergroup          0  hdfs://hadoop001:9000/user/hive/warehouse/hive_part_tbl/key=103</span><br><span class="line">.......</span><br></pre></td></tr></table></figure>
<h3 id="hive表的存储格式">Hive表的存储格式</h3>
<p>具体参考官网。<br>
When you create a Hive table, you need to define how this table should read/write data from/to file system, i.e. the “input format” and “output format”. You also need to define how this table should deserialize the data to rows, or serialize rows to data, i.e. the “serde”. The following options can be used to specify the storage format(“serde”, “input format”, “output format”), e.g. CREATE TABLE src(id int) USING hive OPTIONS(fileFormat ‘parquet’). By default, we will read the table files as plain text.</p>
<p>你创建一个Hive表，你需要指定从哪里读取数据，这个数据是什么格式的；你创建这张表的存储格式又是什么。还有序列化反序列化等。</p>
<p>比如：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sql(&quot;CREATE TABLE hive_records(key int, value string) STORED AS PARQUET&quot;)</span><br><span class="line">val df = spark.table(&quot;src&quot;)</span><br><span class="line">df.write.mode(SaveMode.Overwrite).saveAsTable(&quot;hive_records&quot;)</span><br><span class="line"></span><br><span class="line">sql(s&quot;CREATE EXTERNAL TABLE hive_ints(key int) STORED AS PARQUET LOCATION &apos;$dataDir&apos;&quot;)</span><br></pre></td></tr></table></figure>
<h3 id="用jdbc读其它数据库">用JDBC读其它数据库</h3>
<blockquote>
<p>参考官网：<a href="http://spark.apache.org/docs/latest/sql-data-sources-jdbc.html" target="_blank" rel="noopener">http://spark.apache.org/docs/latest/sql-data-sources-jdbc.html</a></p>
</blockquote>
<p>Spark SQL也可以用JDBC去从其它数据库去读取数据。读取的结果返回的是DataFrame ，可以在Spark SQL很容易的去处理，以及与其它数据源进行join。<br>
需要保证 数据库的驱动对driver端和executor端可见。<br>
提前需要把你要连接的数据库驱动放到spark classpath里，用下面这种方式，</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/spark-shell --driver-class-path +跟上相关数据库的驱动</span><br></pre></td></tr></table></figure>
<p>举例：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line">scala&gt;     val dbsjdbcDF = spark.read.format(&quot;jdbc&quot;)</span><br><span class="line">.option(&quot;url&quot;,&quot;jdbc:mysql://hadoop001:3306/hive&quot;)</span><br><span class="line">.option(&quot;dbtable&quot;,&quot;dbs&quot;)</span><br><span class="line">.option(&quot;user&quot;,&quot;root&quot;)</span><br><span class="line">.option(&quot;password&quot;,&quot;123456&quot;)</span><br><span class="line">.load()</span><br><span class="line"></span><br><span class="line">jdbcDF: org.apache.spark.sql.DataFrame = [DB_ID: bigint, DESC: string ... 4 more fields]</span><br><span class="line"></span><br><span class="line">scala&gt; dbsjdbcDF.show</span><br><span class="line">+-----+--------------------+--------------------+-------+----------+----------+</span><br><span class="line">|DB_ID|                DESC|     DB_LOCATION_URI|   NAME|OWNER_NAME|OWNER_TYPE|</span><br><span class="line">+-----+--------------------+--------------------+-------+----------+----------+</span><br><span class="line">|    1|Default Hive data...|hdfs://hadoop001:...|default|    public|      ROLE|</span><br><span class="line">|    2|                null|hdfs://hadoop001:...|     g6|    hadoop|      USER|</span><br><span class="line">+-----+--------------------+--------------------+-------+----------+----------+</span><br><span class="line"></span><br><span class="line">scala&gt;  val tblsjdbcDF = spark.read.format(&quot;jdbc&quot;).option(&quot;url&quot;,&quot;jdbc:mysql://hadoop001:3306/ruoze_hive&quot;).option(&quot;dbtable&quot;,&quot;tbls&quot;).option(&quot;user&quot;,&quot;root&quot;).option(&quot;password&quot;,&quot;123456&quot;).load()</span><br><span class="line">tblsjdbcDF: org.apache.spark.sql.DataFrame = [TBL_ID: bigint, CREATE_TIME: int ... 9 more fields]</span><br><span class="line"></span><br><span class="line">scala&gt; tblsjdbcDF.show</span><br><span class="line">+------+-----------+-----+----------------+------+---------+-----+-------------+--------------+------------------+------------------+</span><br><span class="line">|TBL_ID|CREATE_TIME|DB_ID|LAST_ACCESS_TIME| OWNER|RETENTION|SD_ID|     TBL_NAME|      TBL_TYPE|VIEW_EXPANDED_TEXT|VIEW_ORIGINAL_TEXT|</span><br><span class="line">+------+-----------+-----+----------------+------+---------+-----+-------------+--------------+------------------+------------------+</span><br><span class="line">|     1| 1561877732|    2|               0|hadoop|        0|    1|         test| MANAGED_TABLE|              null|              null|</span><br><span class="line">|     2| 1561880911|    1|               0|hadoop|        0|    2|          emp| MANAGED_TABLE|              null|              null|</span><br><span class="line">|     3| 1561880919|    1|               0|hadoop|        0|    3|         dept| MANAGED_TABLE|              null|              null|</span><br><span class="line">|     6| 1561884678|    1|               0|hadoop|        0|    6| sparksqltest| MANAGED_TABLE|              null|              null|</span><br><span class="line">|    11| 1562170275|    1|               0|hadoop|        0|   11|          src| MANAGED_TABLE|              null|              null|</span><br><span class="line">|    17| 1562255263|    1|               0|hadoop|        0|   17| hive_records| MANAGED_TABLE|              null|              null|</span><br><span class="line">|    20| 1562256521|    1|               0|hadoop|        0|   20|    hive_ints|EXTERNAL_TABLE|              null|              null|</span><br><span class="line">|    21| 1562256772|    1|               0|hadoop|        0|   21|hive_part_tbl| MANAGED_TABLE|              null|              null|</span><br><span class="line">+------+-----------+-----+----------------+------+---------+-----+-------------+--------------+------------------+------------------+</span><br><span class="line"></span><br><span class="line">//实现两个表的join;</span><br><span class="line">scala&gt; dbsjdbcDF.join(tblsjdbcDF,&quot;db_id&quot;).select(&quot;db_id&quot;,&quot;name&quot;,&quot;TBL_NAME&quot;,&quot;DB_LOCATION_URI&quot;).show(false)</span><br><span class="line">+-----+-------+-------------+-----------------------------------------------+</span><br><span class="line">|db_id|name   |TBL_NAME     |DB_LOCATION_URI                                |</span><br><span class="line">+-----+-------+-------------+-----------------------------------------------+</span><br><span class="line">|1    |default|dept         |hdfs://hadoop001:9000/user/hive/warehouse      |</span><br><span class="line">|1    |default|emp          |hdfs://hadoop001:9000/user/hive/warehouse      |</span><br><span class="line">|1    |default|hive_ints    |hdfs://hadoop001:9000/user/hive/warehouse      |</span><br><span class="line">|1    |default|hive_part_tbl|hdfs://hadoop001:9000/user/hive/warehouse      |</span><br><span class="line">|1    |default|hive_records |hdfs://hadoop001:9000/user/hive/warehouse      |</span><br><span class="line">|1    |default|sparksqltest |hdfs://hadoop001:9000/user/hive/warehouse      |</span><br><span class="line">|1    |default|src          |hdfs://hadoop001:9000/user/hive/warehouse      |</span><br><span class="line">|2    |g6     |test         |hdfs://hadoop001:9000/user/hive/warehouse/g6.db|</span><br><span class="line">+-----+-------+-------------+-----------------------------------------------+</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">/./或者这样：</span><br><span class="line">scala&gt;     dbsjdbcDF.join(tblsjdbcDF,dbsjdbcDF(&quot;db_id&quot;)===tblsjdbcDF(&quot;db_id&quot;)).select(&quot;name&quot;,&quot;TBL_NAME&quot;,&quot;DB_LOCATION_URI&quot;,&quot;TBL_TYPE&quot;).show(false)</span><br><span class="line">+-------+-------------+-----------------------------------------------+--------------+</span><br><span class="line">|name   |TBL_NAME     |DB_LOCATION_URI                                |TBL_TYPE      |</span><br><span class="line">+-------+-------------+-----------------------------------------------+--------------+</span><br><span class="line">|default|emp          |hdfs://hadoop001:9000/user/hive/warehouse      |MANAGED_TABLE |</span><br><span class="line">|default|dept         |hdfs://hadoop001:9000/user/hive/warehouse      |MANAGED_TABLE |</span><br><span class="line">|default|sparksqltest |hdfs://hadoop001:9000/user/hive/warehouse      |MANAGED_TABLE |</span><br><span class="line">|default|src          |hdfs://hadoop001:9000/user/hive/warehouse      |MANAGED_TABLE |</span><br><span class="line">|default|hive_records |hdfs://hadoop001:9000/user/hive/warehouse      |MANAGED_TABLE |</span><br><span class="line">|default|hive_ints    |hdfs://hadoop001:9000/user/hive/warehouse      |EXTERNAL_TABLE|</span><br><span class="line">|default|hive_part_tbl|hdfs://hadoop001:9000/user/hive/warehouse      |MANAGED_TABLE |</span><br><span class="line">|g6     |test         |hdfs://hadoop001:9000/user/hive/warehouse/g6.db|MANAGED_TABLE |</span><br><span class="line">+-------+-------------+-----------------------------------------------+--------------+</span><br></pre></td></tr></table></figure>
<p>此时多个数据源来的DataFrame就可以做相应的关联操作（关系型数据库、Hive数据等）。</p>
<p>当然这里的写还是不建议用，在生产上还是有各种各样的问题，还是用之前讲的JDBC，老老实实写过去（普通的方式）</p>
<p>Troubleshooting</p>
<p>The JDBC driver class must be visible to the primordial class loader on the client session and on all executors.</p>
<p>必须可见，也就能访问得到，无外乎driver和executor，加上去就行了</p>
<p>Some databases, such as H2, convert all names to upper case. You’ll need to use upper case to refer to those names in Spark SQL.</p>
<p>Users can specify vendor-specific JDBC connection properties in the data source options to do special treatment. For example, spark.read.format(“jdbc”).option(“url”, oracleJdbcUrl).option(“oracle.jdbc.mapDateToTimestamp”, “false”). oracle.jdbc.mapDateToTimestamp defaults to true, users often need to disable this flag to avoid Oracle date being resolved as timestamp.<br>
Some databases, such as H2, convert all names to upper case. You’ll need to use upper case to refer to those names in Spark SQL.</p>
<p>Users can specify vendor-specific JDBC connection properties in the data source options to do special treatment. For example, spark.read.format(“jdbc”).option(“url”, oracleJdbcUrl).option(“oracle.jdbc.mapDateToTimestamp”, “false”). oracle.jdbc.mapDateToTimestamp defaults to true, users often need to disable this flag to avoid Oracle date being resolved as timestamp.</p>
<h3 id="performance-tuning性能优化">Performance Tuning性能优化</h3>
<p>通过把数据缓存在内存中或配置一些选项可以提高Spark工作的性能。</p>
<h3 id="caching-data-in-memory">Caching Data In Memory</h3>
<p>Spark SQL可以通过调用spark.catalog.cacheTable(“tableName”) 或者 dataFrame.cache() 来把数据缓存到内存中。<br>
为了减少内存的使用和GC的压力， Spark SQL会浏览你仅仅需要的列，而且自动优化压缩。<br>
你可以使用 spark.catalog.uncacheTable(“tableName”) 来移除内存中的表。<br>
内存缓存的配置可以使用SparkSession 的setConf 方法或使用SQL运行SET key=value命令来完成。</p>
<p><img src="https://img-blog.csdnimg.cn/20190930161211927.png" alt="image"></p>
<p>这些一般默认就行,生产上够了</p>
<h3 id="other-configuration-options">Other Configuration Options</h3>
<p>The following options can also be used to tune the performance of query execution. It is possible that these options will be deprecated in future release as more optimizations are performed automatically.<br>
以下的参数也可以用于优化查询执行的性能。<br>
在将来的版本中，这些选项可能会被弃用，因为会自动执行更多的优化。</p>
<p><img src="https://img-blog.csdnimg.cn/20190930161256507.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzIxMjM2NQ==,size_16,color_FFFFFF,t_70" alt="image"></p>
<p>从上面的参数设置可以看出，默认小于10M的就会作为小表，广播出去；广播多少时间超时，这个值够了，不过小表的10M肯定不够，需要根据生产上来调</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">spark.sql.autoBroadcastJoinThreshold改掉的话</span><br><span class="line"></span><br><span class="line">set spark.sql.autoBroadcastJoinThreshold=-1;</span><br></pre></td></tr></table></figure>
<p>性能差别还是很大的</p>
<h3 id="broadcast-hint-for-sql-queries">Broadcast Hint for SQL Queries</h3>
<p>为SQL查询广播提示。<br>
当一个表去和其它表或视图做join的时候，BROADCAST hint会让Spark去广播这个表。</p>
<p><img src="https://img-blog.csdnimg.cn/20190930161326717.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzIxMjM2NQ==,size_16,color_FFFFFF,t_70" alt="image"></p>
<p>有问题，万一把大表广播出去就有问题</p>

                

                <hr>
                <!-- Pager -->
                <ul class="pager">
                    
                        <li class="previous">
                            <a href="/article/89SparkSQL内置函数、UDF函数/" data-toggle="tooltip" data-placement="top" title="[SparkSQL内置函数、UDF函数]  ">&larr; Previous Post</a>
                        </li>
                    
                    
                        <li class="next">
                            <a href="/article/87SparkSQL之DataFrame以及ShuffleManager补充/" data-toggle="tooltip" data-placement="top" title="[SparkSQL之DataFrame以及ShuffleManager补充]  ">Next Post &rarr;</a>
                        </li>
                    
                </ul>

                <br>

                <!--打赏-->
                
                <!--打赏-->

                <br>
                <!--分享-->
                
                    <div class="social-share"  data-wechat-qrcode-helper="" align="center"></div>
                    <!--  css & js -->
                    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/social-share.js/1.0.16/css/share.min.css">
                    <script src="https://cdnjs.cloudflare.com/ajax/libs/social-share.js/1.0.16/js/social-share.min.js"></script>
                
                <!--分享-->
                <br>                       
                
                <!-- require APlayer -->
                

                <!-- duoshuo Share start -->
                
                <!-- 多说 Share end-->

                <!-- 多说评论框 start -->
                
                <!-- 多说评论框 end -->

                <!-- disqus comment start -->
                
                <!-- disqus comment end -->

                

            </div>
            
            <!-- Tabe of Content -->
            <!-- Table of Contents -->

  
    
      <aside id="sidebar">
        <div id="toc" class="toc-article">
        <strong class="toc-title">Contents</strong>
        
          <ol class="toc-nav"><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#parquet-文件"><span class="toc-nav-number">1.</span> <span class="toc-nav-text">Parquet &#x6587;&#x4EF6;</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#加载parquet文件"><span class="toc-nav-number">1.1.</span> <span class="toc-nav-text">&#x52A0;&#x8F7D;Parquet&#x6587;&#x4EF6;</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#partition-discovery-分区探测"><span class="toc-nav-number">1.2.</span> <span class="toc-nav-text">Partition Discovery &#x5206;&#x533A;&#x63A2;&#x6D4B;</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#schema-合并"><span class="toc-nav-number">1.3.</span> <span class="toc-nav-text">Schema &#x5408;&#x5E76;</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#orc文件"><span class="toc-nav-number">2.</span> <span class="toc-nav-text">ORC&#x6587;&#x4EF6;</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#hive表"><span class="toc-nav-number">3.</span> <span class="toc-nav-text">Hive&#x8868;</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#hive表的存储格式"><span class="toc-nav-number">3.1.</span> <span class="toc-nav-text">Hive&#x8868;&#x7684;&#x5B58;&#x50A8;&#x683C;&#x5F0F;</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#用jdbc读其它数据库"><span class="toc-nav-number">3.2.</span> <span class="toc-nav-text">&#x7528;JDBC&#x8BFB;&#x5176;&#x5B83;&#x6570;&#x636E;&#x5E93;</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#performance-tuning性能优化"><span class="toc-nav-number">3.3.</span> <span class="toc-nav-text">Performance Tuning&#x6027;&#x80FD;&#x4F18;&#x5316;</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#caching-data-in-memory"><span class="toc-nav-number">3.4.</span> <span class="toc-nav-text">Caching Data In Memory</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#other-configuration-options"><span class="toc-nav-number">3.5.</span> <span class="toc-nav-text">Other Configuration Options</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#broadcast-hint-for-sql-queries"><span class="toc-nav-number">3.6.</span> <span class="toc-nav-text">Broadcast Hint for SQL Queries</span></a></li></ol></li></ol>
        
        </div>
      </aside>
    

                
            <!-- Sidebar Container -->
            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                sidebar-container">

                <!-- Featured Tags -->
                
                <section>
                    <!-- no hr -->
                    <h5><a href="/tags/">FEATURED TAGS</a></h5>
                    <div class="tags">
                       
                          <a class="tag" href="/tags/#Spark" title="Spark">Spark</a>
                        
                    </div>
                </section>
                

                <!-- Friends Blog -->
                
                <hr>
                <h5>FRIENDS</h5>
                <ul class="list-inline">

                    
                        <li><a href="https://sxwanggit126.github.io" target="_blank">DoubleHappy</a></li>
                    
                </ul>
                
            </div>
        </div>
    </div>
</article>








<!-- async load function -->
<script>
    function async(u, c) {
      var d = document, t = 'script',
          o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
      s.parentNode.insertBefore(o, s);
    }
</script>
<!-- anchor-js, Doc:http://bryanbraun.github.io/anchorjs/ -->
<script>
    async("https://cdn.bootcss.com/anchor-js/1.1.1/anchor.min.js",function(){
        anchors.options = {
          visible: 'hover',
          placement: 'left',
          icon: 'rz'
        };
        anchors.add().remove('.intro-header h1').remove('.subheading').remove('.sidebar-container h5');
    })
</script>
<style>
    /* place left on bigger screen */
    @media all and (min-width: 800px) {
        .anchorjs-link{
            position: absolute;
            left: -0.75em;
            font-size: 1.1em;
            margin-top : -0.1em;
        }
    }
</style>


<!-- chrome Firefox 中文锚点定位失效-->
<script src="https://cdn.bootcss.com/jquery/3.3.1/jquery.js"></script>
<!-- smooth scroll behavior polyfill  -->
<script type="text/javascript" src="/js/smoothscroll.js"></script>
<script>
        $('#toc').on('click','a',function(a){
            // var isChrome = window.navigator.userAgent.indexOf("Chrome") !== -1;
            // console.log(window.navigator.userAgent,isChrome)
                // if(isChrome) {
                    // console.log(a.currentTarget.outerHTML);
                    // console.log($(a.currentTarget).attr("href"));
                    //跳转到指定锚点
                    // document.getElementById(a.target.innerText.toLowerCase()).scrollIntoView(true);
                    document.getElementById($(a.currentTarget).attr("href").replace("#","")).scrollIntoView({behavior: 'smooth' });
                // }
        })  
</script>


    <!-- Footer -->
    <!-- Footer -->
<footer>
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <ul class="list-inline text-center">
                
                
                

                

                

                
                    <li>
                        <a target="_blank"  href="https://github.com/LeoFWZ">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                

                

                </ul>
                <p class="copyright text-muted">
                    Copyright &copy; FWZ 2020 
                    By <a href="https://leofwz.github.io/">LeoFWZ</a> | BigData
                </p>
            </div>
        </div>
    </div>
</footer>

<!-- jQuery -->
<script src="/js/jquery.min.js"></script>

<!-- Bootstrap Core JavaScript -->
<script src="/js/bootstrap.min.js"></script>

<!-- Custom Theme JavaScript -->
<script src="/js/hux-blog.min.js"></script>


<!-- async load function -->
<script>
    function async(u, c) {
      var d = document, t = 'script',
          o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
      s.parentNode.insertBefore(o, s);
    }
</script>

<!-- 
     Because of the native support for backtick-style fenced code blocks 
     right within the Markdown is landed in Github Pages, 
     From V1.6, There is no need for Highlight.js, 
     so Huxblog drops it officially.

     - https://github.com/blog/2100-github-pages-now-faster-and-simpler-with-jekyll-3-0  
     - https://help.github.com/articles/creating-and-highlighting-code-blocks/    
-->
<!--
    <script>
        async("http://cdn.bootcss.com/highlight.js/8.6/highlight.min.js", function(){
            hljs.initHighlightingOnLoad();
        })
    </script>
    <link href="http://cdn.bootcss.com/highlight.js/8.6/styles/github.min.css" rel="stylesheet">
-->


<!-- jquery.tagcloud.js -->
<script>
    // only load tagcloud.js in tag.html
    if($('#tag_cloud').length !== 0){
        async("https://leofwz.github.io/js/jquery.tagcloud.js",function(){
            $.fn.tagcloud.defaults = {
                //size: {start: 1, end: 1, unit: 'em'},
                color: {start: '#bbbbee', end: '#0085a1'},
            };
            $('#tag_cloud a').tagcloud();
        })
    }
</script>

<!--fastClick.js -->
<script>
    async("https://cdn.bootcss.com/fastclick/1.0.6/fastclick.min.js", function(){
        var $nav = document.querySelector("nav");
        if($nav) FastClick.attach($nav);
    })
</script>


<!-- Google Analytics -->


<script>
    // dynamic User by Hux
    var _gaId = 'UA-XXXXXXXX-X';
    var _gaDomain = 'yoursite';

    // Originial
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', _gaId, _gaDomain);
    ga('send', 'pageview');
</script>




<!-- Baidu Tongji -->

<script>
    // dynamic User by Hux
    var _baId = 'xxx';

    // Originial
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "//hm.baidu.com/hm.js?" + _baId;
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
</script>






	<a id="rocket" href="#top" class=""></a>
	<script type="text/javascript" src="/js/totop.js?v=1.0.0" async=""></script>
    <script type="text/javascript" src="/js/toc.js?v=1.0.0" async=""></script>
<!-- Image to hack wechat -->
<img src="https://leofwz.github.io/img/icon_wechat.png" width="0" height="0" />
<!-- Migrate from head to bottom, no longer block render and still work -->

</body>

</html>
