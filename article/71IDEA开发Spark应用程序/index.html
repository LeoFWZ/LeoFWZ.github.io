<!DOCTYPE html>
<html lang="en">

<!-- Head tag -->
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="google-site-verification" content="xBT4GhYoi5qRD5tr338pgPM5OWHHIDR6mNg1a3euekI" />
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="Every failure is leading towards success">
    <meta name="keyword"  content="Fangwuzhou,BigData, Hadoop,Java,Python,Spark,Flink,Kafka">
    <link rel="shortcut icon" href="/img/ironman-draw.png">
    <!-- Place this tag in your head or just before your close body tag. -->
    <script async defer src="https://buttons.github.io/buttons.js"></script>
    <title>
        
          [IDEA开发Spark应用程序]   - Leo@FWZ| Blog
        
    </title>

    <link rel="canonical" href="https://leofwz.github.io/article/71IDEA开发Spark应用程序/">

    <!-- Bootstrap Core CSS -->
    <link rel="stylesheet" href="/css/bootstrap.min.css">

    <!-- Custom CSS --> 
    <link rel="stylesheet" href="/css/beantech.min.css">

    <link rel="stylesheet" href="/css/donate.css">
    
    <!-- Pygments Highlight CSS -->
    <link rel="stylesheet" href="/css/highlight.css">

    <link rel="stylesheet" href="/css/widget.css">

    <link rel="stylesheet" href="/css/rocket.css">

    <link rel="stylesheet" href="/css/signature.css">

    <link rel="stylesheet" href="/css/toc.css">

    <!-- Custom Fonts -->
    <!-- <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css" rel="stylesheet" type="text/css"> -->
    <!-- Hux change font-awesome CDN to qiniu -->
    <link href="https://cdn.staticfile.org/font-awesome/4.5.0/css/font-awesome.min.css" rel="stylesheet" type="text/css">


    <!-- Hux Delete, sad but pending in China
    <link href='http://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
    <link href='http://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/
    css'>
    -->


    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

    <!-- ga & ba script hoook -->
    <script></script>
</head>


<!-- hack iOS CSS :active style -->
<body ontouchstart="">
	<!-- Modified by Yu-Hsuan Yen -->
<!-- Post Header -->
<style type="text/css">
    header.intro-header{
        
            background-image: url('/img/article_header/article_header.png')
            /*post*/
        
    }
    
</style>

<header class="intro-header" >
    <!-- Signature -->
    <div id="signature">
        <div class="container">
            <div class="row">
                <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                
                    <div class="post-heading">
                        <div class="tags">
                            
                              <a class="tag" href="/tags/#Spark" title="Spark">Spark</a>
                            
                        </div>
                        <h1>[IDEA开发Spark应用程序]  </h1>
                        <h2 class="subheading"></h2>
                        <span class="meta">
                            Posted by FWZ on
                            2018-08-12
                        </span>
                    </div>
                


                </div>
            </div>
        </div>
    </div>
</header>

	
    <!-- Navigation -->
<nav class="navbar navbar-default navbar-custom navbar-fixed-top">
    <div class="container-fluid">
        <!-- Brand and toggle get grouped for better mobile display -->
        <div class="navbar-header page-scroll">
            <button type="button" class="navbar-toggle">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="/">Leo@FWZ</a>
        </div>

        <!-- Collect the nav links, forms, and other content for toggling -->
        <!-- Known Issue, found by Hux:
            <nav>'s height woule be hold on by its content.
            so, when navbar scale out, the <nav> will cover tags.
            also mask any touch event of tags, unfortunately.
        -->
        <div id="huxblog_navbar">
            <div class="navbar-collapse">
                <ul class="nav navbar-nav navbar-right">
                    <li>
                        <a href="/">Home</a>
                    </li>

                    

                        
                    

                        
                        <li>
                            <a href="/about/">About</a>
                        </li>
                        
                    

                        
                        <li>
                            <a href="/tags/">Tags</a>
                        </li>
                        
                    

                        
                        <li>
                            <a href="/archive/">Archives</a>
                        </li>
                        
                    
                    
                </ul>
            </div>
        </div>
        <!-- /.navbar-collapse -->
    </div>
    <!-- /.container -->
</nav>
<script>
    // Drop Bootstarp low-performance Navbar
    // Use customize navbar with high-quality material design animation
    // in high-perf jank-free CSS3 implementation
    var $body   = document.body;
    var $toggle = document.querySelector('.navbar-toggle');
    var $navbar = document.querySelector('#huxblog_navbar');
    var $collapse = document.querySelector('.navbar-collapse');

    $toggle.addEventListener('click', handleMagic)
    function handleMagic(e){
        if ($navbar.className.indexOf('in') > 0) {
        // CLOSE
            $navbar.className = " ";
            // wait until animation end.
            setTimeout(function(){
                // prevent frequently toggle
                if($navbar.className.indexOf('in') < 0) {
                    $collapse.style.height = "0px"
                }
            },400)
        }else{
        // OPEN
            $collapse.style.height = "auto"
            $navbar.className += " in";
        }
    }
</script>


    <!-- Main Content -->
    <!-- Modify by Yu-Hsuan Yen -->

<!-- Post Content -->
<article>
    <div class="container">
        <div class="row">

            <!-- Post Container -->
            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                post-container">

                <h1 id="前言">前言</h1>
<p>spark-shell适合做测试</p>
<p>如果是开发的话是用IDEA+Maven+Scala来开发Spark应用程序</p>
<h1 id="创建一个maven项目添加依赖">创建一个Maven项目，添加依赖</h1>
<p>前面创建的过程省略。</p>
<p>创建了一个Scala的Maven项目后，需要添加依赖什么的。</p>
<p>我这边pom.xml文件添加代码如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">&lt;properties&gt;</span><br><span class="line">   &lt;scala.version&gt;2.11.8&lt;/scala.version&gt;</span><br><span class="line">   &lt;hadoop.version&gt;2.6.0-cdh5.7.0&lt;/hadoop.version&gt;</span><br><span class="line">   &lt;spark.version&gt;2.4.0&lt;/spark.version&gt;</span><br><span class="line"> &lt;/properties&gt;</span><br><span class="line"></span><br><span class="line"> &lt;dependencies&gt;</span><br><span class="line">   &lt;!--添加Spark依赖--&gt;</span><br><span class="line">   &lt;dependency&gt;</span><br><span class="line">     &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</span><br><span class="line">     &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt;</span><br><span class="line">     &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt;</span><br><span class="line">   &lt;/dependency&gt;</span><br><span class="line">   &lt;!--添加Scala依赖--&gt;</span><br><span class="line">   &lt;dependency&gt;</span><br><span class="line">     &lt;groupId&gt;org.scala-lang&lt;/groupId&gt;</span><br><span class="line">     &lt;artifactId&gt;scala-library&lt;/artifactId&gt;</span><br><span class="line">     &lt;version&gt;$&#123;scala.version&#125;&lt;/version&gt;</span><br><span class="line">   &lt;/dependency&gt;</span><br><span class="line">   &lt;!--添加Hadoop依赖--&gt;</span><br><span class="line">   &lt;dependency&gt;</span><br><span class="line">     &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;</span><br><span class="line">     &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt;</span><br><span class="line">     &lt;version&gt;$&#123;hadoop.version&#125;&lt;/version&gt;</span><br><span class="line">   &lt;/dependency&gt;</span><br><span class="line"> &lt;/dependencies&gt;</span><br></pre></td></tr></table></figure>
<p>添加之后，点击Reimport，重新导入一下</p>
<p><img src="https://img-blog.csdnimg.cn/20190610153645733.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzIxMjM2NQ==,size_16,color_FFFFFF,t_70" alt="image"></p>
<p>但是导入之后，Hadoop的有些没有导进来，红色波浪线，报错了：</p>
<p><img src="https://img-blog.csdnimg.cn/2019061015372346.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzIxMjM2NQ==,size_16,color_FFFFFF,t_70" alt="image"></p>
<p>需要加一个仓库repository：<br>
<a href="http://repository.cloudera.com/artifactory/cloudera-repos/" target="_blank" rel="noopener">http://repository.cloudera.com/artifactory/cloudera-repos/</a></p>
<p><img src="https://img-blog.csdnimg.cn/2019061015382775.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzIxMjM2NQ==,size_16,color_FFFFFF,t_70" alt="image"></p>
<p>继续点击到如下目录：</p>
<p><a href="http://repository.cloudera.com/artifactory/cloudera-repos/org/apache/hadoop/hadoop-client/2.6.0-cdh5.7.0/" target="_blank" rel="noopener">http://repository.cloudera.com/artifactory/cloudera-repos/org/apache/hadoop/hadoop-client/2.6.0-cdh5.7.0/</a></p>
<p>这个就是上面我们添加的2.6.0-cdh5.7.0/</p>
<p>如果写的是2.6.0-cdh5.7.0，就要加这个仓库，如果写的是2.6.0，不说cdh的就不用加这个仓库。</p>
<p><img src="https://img-blog.csdnimg.cn/20190610153951143.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzIxMjM2NQ==,size_16,color_FFFFFF,t_70" alt="image"></p>
<p>然后再pom.xml里添加一下</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&lt;repositories&gt;</span><br><span class="line">  &lt;repository&gt;</span><br><span class="line">    &lt;id&gt;cloudera&lt;/id&gt;</span><br><span class="line">    &lt;name&gt;cloudera&lt;/name&gt;</span><br><span class="line">    &lt;url&gt;http://repository.cloudera.com/artifactory/cloudera-repos/&lt;/url&gt;</span><br><span class="line">  &lt;/repository&gt;</span><br><span class="line">&lt;/repositories&gt;</span><br></pre></td></tr></table></figure>
<p>再重新导入一下，Reimport，发现还是红色波浪线，报错</p>
<p>clean一下：</p>
<p><img src="https://img-blog.csdnimg.cn/20190610154043859.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzIxMjM2NQ==,size_16,color_FFFFFF,t_70" alt="image"><br>
<img src="https://img-blog.csdnimg.cn/20190610154105562.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzIxMjM2NQ==,size_16,color_FFFFFF,t_70" alt="image"></p>
<p>发现BUILD SUCCESS，没问题，说明没有影响，可能是其它的一些jar包没有导进来，不过没有影响的。</p>
<p>好了，有了上面的操作，看源码就很方便了，直接搜索就可以看了。<br>
比如，想看Hadoop的入口类：FileSystem的源码</p>
<p><img src="https://img-blog.csdnimg.cn/20190610154942683.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzIxMjM2NQ==,size_16,color_FFFFFF,t_70" alt="image"></p>
<p>点击一下，就可以看到源码了：</p>
<p><img src="https://img-blog.csdnimg.cn/20190610155008182.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzIxMjM2NQ==,size_16,color_FFFFFF,t_70" alt="image"></p>
<h2 id="词频统计案例开发及上传jar包到服务器并准备测试数据">词频统计案例开发及上传jar包到服务器并准备测试数据</h2>
<p>建个包：</p>
<p><img src="https://img-blog.csdnimg.cn/20190610155043340.png" alt="image"></p>
<p>建个scala对象</p>
<p><img src="https://img-blog.csdnimg.cn/20190610155108504.png" alt="image"></p>
<p><img src="https://img-blog.csdnimg.cn/2019061015511734.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzIxMjM2NQ==,size_16,color_FFFFFF,t_70" alt="image"></p>
<p>开发代码：</p>
<p>一开始的三步骤：首先构建一个sparkConf，再构建一个sc，然后关掉sc</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">package com.ruozedata.spark.com.ruozedata.spark.core</span><br><span class="line">import org.apache.spark.&#123;SparkConf, SparkContext&#125;</span><br><span class="line"></span><br><span class="line">object WordCountApp &#123;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val sparkConf = new SparkConf()</span><br><span class="line">    val sc = new SparkContext(sparkConf)</span><br><span class="line">    //业务逻辑</span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>开发业务逻辑代码：wordcount代码</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">package com.ruozedata.spark.com.ruozedata.spark.core</span><br><span class="line"></span><br><span class="line">import org.apache.spark.&#123;SparkConf, SparkContext&#125;</span><br><span class="line"></span><br><span class="line">object WordCountApp &#123;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val sparkConf = new SparkConf()</span><br><span class="line">    val sc = new SparkContext(sparkConf)</span><br><span class="line">	</span><br><span class="line">	//args(0)表示传进来的第一个参数</span><br><span class="line">    val textFile = sc.textFile(args(0))</span><br><span class="line">    val wc = textFile.flatMap(line =&gt; line.split(&quot;\t&quot;)).map((_,1)).reduceByKey(_+_)</span><br><span class="line">	</span><br><span class="line">	//打印出来</span><br><span class="line">    wc.collect().foreach(println)</span><br><span class="line"></span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>开发完成，代码打包<br>
<img src="https://img-blog.csdnimg.cn/20190610155251413.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzIxMjM2NQ==,size_16,color_FFFFFF,t_70" alt="image"></p>
<p>等一会，就打包成功了：<br>
<img src="https://img-blog.csdnimg.cn/20190610155320271.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzIxMjM2NQ==,size_16,color_FFFFFF,t_70" alt="image"></p>
<p>jjar包在这个目录：spark-train\target\spark-train-1.0.jar<br>
然后把jar上传到虚拟机或者服务器上，</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 lib]$ pwd</span><br><span class="line">/home/hadoop/lib</span><br><span class="line">[hadoop@hadoop001 lib]$ ls</span><br><span class="line">spark-train-1.0.jar</span><br></pre></td></tr></table></figure>
<p>wordcount数据准备：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 data]$ hdfs dfs -text /data/wordcount.txt</span><br><span class="line">19/06/06 12:06:00 INFO lzo.GPLNativeCodeLoader: Loaded native gpl library from the embedded binaries</span><br><span class="line">19/06/06 12:06:00 INFO lzo.LzoCodec: Successfully loaded &amp; initialized native-lzo library [hadoop-lzo rev f1deea9a313f4017dd5323cb8bbb3732c1aaccc5]</span><br><span class="line">world   world   hello</span><br><span class="line">China   hello</span><br><span class="line">people  person</span><br><span class="line">love</span><br></pre></td></tr></table></figure>
<p>jar包提交Spark应用程序运行<br>
<img src="https://img-blog.csdnimg.cn/20190610155411323.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzIxMjM2NQ==,size_16,color_FFFFFF,t_70" alt="image"></p>
<p>如何使用spark-submit，看帮助，spark-submit --help 看一下。</p>
<p><img src="https://img-blog.csdnimg.cn/20190610155442135.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzIxMjM2NQ==,size_16,color_FFFFFF,t_70" alt="image"></p>
<p>–class的获得，如下：<br>
<img src="https://img-blog.csdnimg.cn/20190610155504476.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzIxMjM2NQ==,size_16,color_FFFFFF,t_70" alt="image"></p>
<p>得到：–class com.ruozedata.spark.com.ruozedata.spark.core.WordCountApp<br>
–master 为loacl[2]</p>
<p>application-jar的位置：/home/hadoop/lib/spark-train-1.0.jar</p>
<p>[application-arguments] 参数：hdfs://hadoop001:9000/data/wordcount.txt</p>
<p>汇总一下，如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">./spark-submit \</span><br><span class="line">  --class com.ruozedata.spark.com.ruozedata.spark.core.WordCountApp \</span><br><span class="line">  --master local[2] \</span><br><span class="line">  /home/hadoop/lib/spark-train-1.0.jar \</span><br><span class="line">  hdfs://hadoop001:9000/data/wordcount.txt</span><br></pre></td></tr></table></figure>
<p>运行结果：<br>
<img src="https://img-blog.csdnimg.cn/20190610155559242.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzIxMjM2NQ==,size_16,color_FFFFFF,t_70" alt="image"></p>
<h2 id="词频统计案例迭代之输出结果到hdfs">词频统计案例迭代之输出结果到HDFS</h2>
<p>加一个 wc.saveAsTextFile(args(1))：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">package com.ruozedata.spark.com.ruozedata.spark.core</span><br><span class="line"></span><br><span class="line">import org.apache.spark.&#123;SparkConf, SparkContext&#125;</span><br><span class="line"></span><br><span class="line">object WordCountApp &#123;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val sparkConf = new SparkConf()</span><br><span class="line">    val sc = new SparkContext(sparkConf)</span><br><span class="line"></span><br><span class="line">    val textFile = sc.textFile(args(0))</span><br><span class="line">    val wc = textFile.flatMap(line =&gt; line.split(&quot;\t&quot;)).map((_,1)).reduceByKey(_+_)</span><br><span class="line"></span><br><span class="line">    //wc.collect().foreach(println)</span><br><span class="line">    wc.saveAsTextFile(args(1))</span><br><span class="line"></span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>再打包上传，替换上面的jar包，再运行代码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">./spark-submit \</span><br><span class="line">  --class com.ruozedata.spark.com.ruozedata.spark.core.WordCountApp \</span><br><span class="line">  --master local[2] \</span><br><span class="line">  /home/hadoop/lib/spark-train-1.0.jar \</span><br><span class="line">  hdfs://hadoop001:9000/data/wordcount.txt \</span><br><span class="line">  hdfs://hadoop001:9000/data/wcoutput/</span><br></pre></td></tr></table></figure>
<p>结果：<br>
<img src="https://img-blog.csdnimg.cn/20190610155658606.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzIxMjM2NQ==,size_16,color_FFFFFF,t_70" alt="image"></p>
<h2 id="词频统计案例迭代之处理多个输入文件">词频统计案例迭代之处理多个输入文件</h2>
<p>多个文件如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 lib]$ hdfs dfs -ls /data/wordcount/</span><br><span class="line">Found 5 items</span><br><span class="line">-rw-r--r--   1 hadoop supergroup         49 2019-0 /data/wordcount/wordcount1.txt</span><br><span class="line">-rw-r--r--   1 hadoop supergroup         49 2019-0 /data/wordcount/wordcount2.txt</span><br><span class="line">-rw-r--r--   1 hadoop supergroup         49 2019-0 /data/wordcount/wordcount3.txt</span><br><span class="line">-rw-r--r--   1 hadoop supergroup         49 2019-0 /data/wordcount/wordcount4.txt</span><br><span class="line">-rw-r--r--   1 hadoop supergroup         49 2019-0 /data/wordcount/wordcount5.txt</span><br></pre></td></tr></table></figure>
<p>执行代码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">./spark-submit \</span><br><span class="line">  --class com.ruozedata.spark.com.ruozedata.spark.core.WordCountApp \</span><br><span class="line">  --master local[2] \</span><br><span class="line">  /home/hadoop/lib/spark-train-1.0.jar \</span><br><span class="line">  hdfs://hadoop001:9000/data/wordcount/ \</span><br><span class="line">  hdfs://hadoop001:9000/data/wcoutput2/</span><br></pre></td></tr></table></figure>
<p>执行结果如下：<br>
<img src="https://img-blog.csdnimg.cn/20190610155743605.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzIxMjM2NQ==,size_16,color_FFFFFF,t_70" alt="image"></p>
<h2 id="词频统计案例迭代之输入文件规则匹配">词频统计案例迭代之输入文件规则匹配</h2>
<p>文件如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 lib]$ hdfs dfs -ls /data/wordcount/</span><br><span class="line">Found 6 items</span><br><span class="line">-rw-r--r--   1 hadoop supergroup         49 2019-0 /data/wordcount/test</span><br><span class="line">-rw-r--r--   1 hadoop supergroup         49 2019-0 /data/wordcount/wordcount1.txt</span><br><span class="line">-rw-r--r--   1 hadoop supergroup         49 2019-0 /data/wordcount/wordcount2.txt</span><br><span class="line">-rw-r--r--   1 hadoop supergroup         49 2019-0 /data/wordcount/wordcount3.txt</span><br><span class="line">-rw-r--r--   1 hadoop supergroup         49 2019-0 /data/wordcount/wordcount4.txt</span><br><span class="line">-rw-r--r--   1 hadoop supergroup         49 2019-0 /data/wordcount/wordcount5.txt</span><br></pre></td></tr></table></figure>
<p>执行代码</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">./spark-submit \</span><br><span class="line">  --class com.ruozedata.spark.com.ruozedata.spark.core.WordCountApp \</span><br><span class="line">  --master local[2] \</span><br><span class="line">  /home/hadoop/lib/spark-train-1.0.jar \</span><br><span class="line">  hdfs://hadoop001:9000/data/wordcount/*.txt \      //这里是文件规则匹配</span><br><span class="line">  hdfs://hadoop001:9000/data/wcoutput3/</span><br></pre></td></tr></table></figure>
<p>结果如下：</p>
<p>还是5个，说明上面的那个test文件并没有作为输入<br>
<img src="https://img-blog.csdnimg.cn/20190610155832640.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzIxMjM2NQ==,size_16,color_FFFFFF,t_70" alt="image"></p>
<h2 id="带排序的词频统计案例开发及运行过程深度剖析">带排序的词频统计案例开发及运行过程深度剖析</h2>
<p>代码如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">package com.ruozedata.spark.com.ruozedata.spark.core</span><br><span class="line"></span><br><span class="line">import org.apache.spark.&#123;SparkConf, SparkContext&#125;</span><br><span class="line"></span><br><span class="line">object WordCountApp &#123;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val sparkConf = new SparkConf()</span><br><span class="line">    val sc = new SparkContext(sparkConf)</span><br><span class="line"></span><br><span class="line">    val textFile = sc.textFile(args(0))</span><br><span class="line">    val wc = textFile.flatMap(line =&gt; line.split(&quot;\t&quot;)).map((_,1)).reduceByKey(_+_)</span><br><span class="line">    val sorted = wc.map(x =&gt; (x._2,x._1)).sortByKey(false).map(x =&gt; (x._2,x._1))</span><br><span class="line">//这个在上一节已经有详细介绍</span><br><span class="line">    //wc.collect().foreach(println)</span><br><span class="line">    sorted.saveAsTextFile(args(1))</span><br><span class="line"></span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>打包上传，替换原有jar包，再执行代码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">./spark-submit \</span><br><span class="line">  --class com.ruozedata.spark.com.ruozedata.spark.core.WordCountApp \</span><br><span class="line">  --master local[2] \</span><br><span class="line">  /home/hadoop/lib/spark-train-1.0.jar \</span><br><span class="line">  hdfs://hadoop001:9000/data/wordcount/*.txt \</span><br><span class="line">  hdfs://hadoop001:9000/data/wcoutput4/</span><br></pre></td></tr></table></figure>
<p>执行结果如下：</p>
<p>降序：</p>
<p><img src="https://img-blog.csdnimg.cn/20190610155936441.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzIxMjM2NQ==,size_16,color_FFFFFF,t_70" alt="image"></p>
<h2 id="求用户访问量的topn的hive实现以及spark-core实现过程分析">求用户访问量的TopN的Hive实现以及Spark Core实现过程分析</h2>
<p>先用hive来实现：</p>
<p>创建一张表：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">create table page_views(</span><br><span class="line">track_time string,</span><br><span class="line">url string,</span><br><span class="line">session_id string,</span><br><span class="line">referer string,</span><br><span class="line">ip string,</span><br><span class="line">end_user_id string,</span><br><span class="line">city_id string</span><br><span class="line">) row format delimited fields terminated by &apos;\t&apos;;</span><br></pre></td></tr></table></figure>
<p>加载数据进来：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">load data local inpath &apos;/home/hadoop/data/page_views.dat&apos; overwrite into table page_views;</span><br></pre></td></tr></table></figure>
<p>查询结果：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">hive (g6)&gt; select end_user_id,count(*) count  from page_views group by end_user_id order by count desc limit 5;</span><br><span class="line">.......省略....</span><br><span class="line">end_user_id     count</span><br><span class="line">NULL    60871</span><br><span class="line">123626648       40</span><br><span class="line">116191447       38</span><br><span class="line">122764680       34</span><br><span class="line">85252419        30</span><br><span class="line">Time taken: 43.963 seconds, Fetched: 5 row(s)</span><br><span class="line">hive (g6)&gt;</span><br></pre></td></tr></table></figure>
<p>可以看到每个用户的id以及相应的访问数量。</p>
<p><strong>再用Spark Core实现：求用户访问量的 top5 。</strong></p>
<p>拿到需求，进行分析，然后功能拆解（中文描述出来，详细设计说明书），最后是代码的开发实现。</p>
<p>首先拿到一个文件，里面有很多行，每行好几个字段，用户的id在第六个字段，每个字段之间按照\t键进行分割。</p>
<p>所以需要拿到用户id，拿到了之后，后面就跟wordcount案例一样了，首先每个用户赋值为1，然后根据用户id分组求总次数，就是根据相同的key进行相加，然后反转，降序，最后再反转，取top5输出。</p>
<p>代码实现：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">package com.ruozedata.spark.com.ruozedata.spark.core</span><br><span class="line">import org.apache.spark.&#123;SparkConf, SparkContext&#125;</span><br><span class="line"></span><br><span class="line">object PageViewsApp &#123;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val sparkConf = new SparkConf()</span><br><span class="line">    val sc = new SparkContext(sparkConf)</span><br><span class="line">    val pageViews = sc.textFile(args(0))</span><br><span class="line"></span><br><span class="line">    //获取用户id</span><br><span class="line">    //按照tab键分割，分割之后拿到第六个字段(从0开始)，</span><br><span class="line">    //然后每个元素赋值1，形成tuple元组</span><br><span class="line">    val userids = pageViews.map(x =&gt; (x.split(&quot;\t&quot;)(5),1))</span><br><span class="line"></span><br><span class="line">	//和wordcount一样</span><br><span class="line">    val result = userids.reduceByKey(_+_).map(x =&gt; (x._2,x._1)).sortByKey(false).map(x =&gt; (x._2,x._1)).take(5).foreach(println)</span><br><span class="line"></span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>然后打包上传，运行代码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">spark-submit \</span><br><span class="line">  --class com.ruozedata.spark.com.ruozedata.spark.core.PageViewsApp \</span><br><span class="line">  --master local[2] \</span><br><span class="line">  /home/hadoop/lib/spark-train-1.0.jar \</span><br><span class="line">  /home/hadoop/data/page_views.dat</span><br></pre></td></tr></table></figure>
<p>运行结果，和上面hive运行的结果一样：<br>
<img src="https://img-blog.csdnimg.cn/20190610160149964.png" alt="image"></p>
<p>就像上面一样，工作中很多场景都可以看到wordcount的影子。</p>
<h2 id="求平均年龄的spark-core实现">求平均年龄的Spark Core实现</h2>
<p>数据格式为：ID + &quot; &quot; + 年龄，如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 data]$ cat agedata.txt</span><br><span class="line">1 12</span><br><span class="line">2 34</span><br><span class="line">3 54</span><br><span class="line">4 3</span><br><span class="line">5 33</span><br><span class="line">6 23</span><br><span class="line">7 12</span><br><span class="line">8 54</span><br><span class="line">9 45</span><br><span class="line">10 28</span><br></pre></td></tr></table></figure>
<p>分析步骤：</p>
<p>①取出年龄</p>
<p>②求出人数</p>
<p>③年龄相加/人数</p>
<p>开发代码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">package com.ruozedata.spark.com.ruozedata.spark.core</span><br><span class="line"></span><br><span class="line">import org.apache.spark.&#123;SparkConf, SparkContext&#125;</span><br><span class="line"></span><br><span class="line">object AvgAgeCalculatorApp &#123;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val sparkConf = new SparkConf()</span><br><span class="line">    val sc = new SparkContext(sparkConf)</span><br><span class="line">    val dataFile = sc.textFile(args(0))</span><br><span class="line"></span><br><span class="line">    val ageData = dataFile.map(x =&gt; x.split(&quot; &quot;)(1))</span><br><span class="line">    val totalAge = ageData.map(x =&gt; x.toInt).reduce(_+_)</span><br><span class="line">    val count = ageData.count()</span><br><span class="line">    val avgAge = totalAge/count</span><br><span class="line">    println(avgAge)</span><br><span class="line"></span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>打包上传，执行命令：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">spark-submit \</span><br><span class="line">  --class com.ruozedata.spark.com.ruozedata.spark.core.AvgAgeCalculatorApp \</span><br><span class="line">  --master local[2] \</span><br><span class="line">  /home/hadoop/lib/spark-train-1.0.jar \</span><br><span class="line">  /home/hadoop/data/agedata.txt</span><br></pre></td></tr></table></figure>
<p>即可得到结果。</p>
<h2 id="求男女人数以及最高和最低身高">求男女人数以及最高和最低身高</h2>
<p>需求：</p>
<p>1）统计男女人数</p>
<p>2）男性中最高身高和最低身高</p>
<p>3）女性中最高身高和最低身高</p>
<p>数据格式为：ID + &quot; &quot; + 性别 + &quot; &quot; + 身高</p>
<p>如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 data]$ cat people_info.txt </span><br><span class="line">1 M 178</span><br><span class="line">2 M 156</span><br><span class="line">3 F 165</span><br><span class="line">4 M 178</span><br><span class="line">5 F 154</span><br><span class="line">6 M 167</span><br><span class="line">7 M 189</span><br><span class="line">8 F 210</span><br><span class="line">9 F 209</span><br><span class="line">10 F 190</span><br><span class="line">11 M 176</span><br><span class="line">12 M 165</span><br><span class="line">13 F 159</span><br><span class="line">14 M 155</span><br><span class="line">15 M 164</span><br></pre></td></tr></table></figure>
<p>代码如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">package com.ruozedata.spark.com.ruozedata.spark.core</span><br><span class="line"></span><br><span class="line">import org.apache.spark.&#123;SparkConf, SparkContext&#125;</span><br><span class="line"></span><br><span class="line">object peopleApp &#123;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val sparkConf = new SparkConf()</span><br><span class="line">    val sc = new SparkContext(sparkConf)</span><br><span class="line">    val peopleData = sc.textFile(args(0))</span><br><span class="line"></span><br><span class="line">    //第一步分别取出 男性+身高  以及  女性+身高</span><br><span class="line">    val mRdd = peopleData.map(x =&gt;x.split(&quot; &quot;)).map(line =&gt; (line(1),line(2))).filter(_._1 == &quot;M&quot;)</span><br><span class="line">    val fRdd = peopleData.map(x =&gt;x.split(&quot; &quot;)).map(line =&gt; (line(1),line(2))).filter(_._1 == &quot;F&quot;)</span><br><span class="line"></span><br><span class="line">    //求男女人数</span><br><span class="line">    val mCount = mRdd.count()</span><br><span class="line">    val fCount = fRdd.count()</span><br><span class="line"></span><br><span class="line">    //求男性中最高身高和最低身高</span><br><span class="line">    val mMaxHeight =mRdd.map(x =&gt; (x._2).toInt)max()</span><br><span class="line">    //也可以这样求：peopleData.map(line =&gt;line.split(&quot; &quot;)).filter(line =&gt; line(1)==&quot;M&quot;).sortBy(x =&gt; x(2),false).take(1)</span><br><span class="line">    //peopleData.map(line =&gt;line.split(&quot; &quot;)).filter(line =&gt; line(1)==&quot;M&quot;).map(x =&gt;x(2).toInt).max</span><br><span class="line">    val mMinHeight = mRdd.map(x =&gt; (x._2).toInt).min()</span><br><span class="line"></span><br><span class="line">    //求女性中最高身高和最低身高</span><br><span class="line">    val fMaxHeight = fRdd.map(x =&gt; (x._2).toInt).max()</span><br><span class="line">    //也可以这样求：peopleData.map(line =&gt;line.split(&quot; &quot;)).filter(line =&gt; line(1)==&quot;F&quot;).sortBy(x =&gt; x(2),false).take(1)</span><br><span class="line">    //peopleData.map(line =&gt;line.split(&quot; &quot;)).filter(line =&gt; line(1)==&quot;F&quot;).map(x =&gt;x(2).toInt).max</span><br><span class="line">    val fMinHeight = fRdd.map(x =&gt; (x._2).toInt).min()</span><br><span class="line"></span><br><span class="line">    println(&quot;男性人数:&quot; + mCount + &quot;&quot; + &quot;女性人数:&quot; + fCount)</span><br><span class="line">    println(&quot;男性中最高身高为:&quot; + mMaxHeight + &quot;男性中最低身高为:&quot; + mMinHeight)</span><br><span class="line">    println(&quot;女性中最高身高为:&quot; + fMaxHeight + &quot;女性中最低身高为:&quot; + fMinHeight)</span><br><span class="line"></span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>下图只是部分测试结果，可参考：<br>
<img src="https://img-blog.csdnimg.cn/20190610160348992.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzIxMjM2NQ==,size_16,color_FFFFFF,t_70" alt="image"></p>

                

                <hr>
                <!-- Pager -->
                <ul class="pager">
                    
                        <li class="previous">
                            <a href="/article/72SparkCore的Topn的代码书写案例/" data-toggle="tooltip" data-placement="top" title="[SparkCore的Topn的代码书写案例]  ">&larr; Previous Post</a>
                        </li>
                    
                    
                        <li class="next">
                            <a href="/article/70RDD深入讲解/" data-toggle="tooltip" data-placement="top" title="[RDD深入讲解]  ">Next Post &rarr;</a>
                        </li>
                    
                </ul>

                <br>

                <!--打赏-->
                
                <!--打赏-->

                <br>
                <!--分享-->
                
                    <div class="social-share"  data-wechat-qrcode-helper="" align="center"></div>
                    <!--  css & js -->
                    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/social-share.js/1.0.16/css/share.min.css">
                    <script src="https://cdnjs.cloudflare.com/ajax/libs/social-share.js/1.0.16/js/social-share.min.js"></script>
                
                <!--分享-->
                <br>                       
                
                <!-- require APlayer -->
                

                <!-- duoshuo Share start -->
                
                <!-- 多说 Share end-->

                <!-- 多说评论框 start -->
                
                <!-- 多说评论框 end -->

                <!-- disqus comment start -->
                
                <!-- disqus comment end -->

                

            </div>
            
            <!-- Tabe of Content -->
            <!-- Table of Contents -->

  
    
      <aside id="sidebar">
        <div id="toc" class="toc-article">
        <strong class="toc-title">Contents</strong>
        
          <ol class="toc-nav"><li class="toc-nav-item toc-nav-level-1"><a class="toc-nav-link" href="#前言"><span class="toc-nav-number">1.</span> <span class="toc-nav-text">&#x524D;&#x8A00;</span></a></li><li class="toc-nav-item toc-nav-level-1"><a class="toc-nav-link" href="#创建一个maven项目添加依赖"><span class="toc-nav-number">2.</span> <span class="toc-nav-text">&#x521B;&#x5EFA;&#x4E00;&#x4E2A;Maven&#x9879;&#x76EE;&#xFF0C;&#x6DFB;&#x52A0;&#x4F9D;&#x8D56;</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#词频统计案例开发及上传jar包到服务器并准备测试数据"><span class="toc-nav-number">2.1.</span> <span class="toc-nav-text">&#x8BCD;&#x9891;&#x7EDF;&#x8BA1;&#x6848;&#x4F8B;&#x5F00;&#x53D1;&#x53CA;&#x4E0A;&#x4F20;jar&#x5305;&#x5230;&#x670D;&#x52A1;&#x5668;&#x5E76;&#x51C6;&#x5907;&#x6D4B;&#x8BD5;&#x6570;&#x636E;</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#词频统计案例迭代之输出结果到hdfs"><span class="toc-nav-number">2.2.</span> <span class="toc-nav-text">&#x8BCD;&#x9891;&#x7EDF;&#x8BA1;&#x6848;&#x4F8B;&#x8FED;&#x4EE3;&#x4E4B;&#x8F93;&#x51FA;&#x7ED3;&#x679C;&#x5230;HDFS</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#词频统计案例迭代之处理多个输入文件"><span class="toc-nav-number">2.3.</span> <span class="toc-nav-text">&#x8BCD;&#x9891;&#x7EDF;&#x8BA1;&#x6848;&#x4F8B;&#x8FED;&#x4EE3;&#x4E4B;&#x5904;&#x7406;&#x591A;&#x4E2A;&#x8F93;&#x5165;&#x6587;&#x4EF6;</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#词频统计案例迭代之输入文件规则匹配"><span class="toc-nav-number">2.4.</span> <span class="toc-nav-text">&#x8BCD;&#x9891;&#x7EDF;&#x8BA1;&#x6848;&#x4F8B;&#x8FED;&#x4EE3;&#x4E4B;&#x8F93;&#x5165;&#x6587;&#x4EF6;&#x89C4;&#x5219;&#x5339;&#x914D;</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#带排序的词频统计案例开发及运行过程深度剖析"><span class="toc-nav-number">2.5.</span> <span class="toc-nav-text">&#x5E26;&#x6392;&#x5E8F;&#x7684;&#x8BCD;&#x9891;&#x7EDF;&#x8BA1;&#x6848;&#x4F8B;&#x5F00;&#x53D1;&#x53CA;&#x8FD0;&#x884C;&#x8FC7;&#x7A0B;&#x6DF1;&#x5EA6;&#x5256;&#x6790;</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#求用户访问量的topn的hive实现以及spark-core实现过程分析"><span class="toc-nav-number">2.6.</span> <span class="toc-nav-text">&#x6C42;&#x7528;&#x6237;&#x8BBF;&#x95EE;&#x91CF;&#x7684;TopN&#x7684;Hive&#x5B9E;&#x73B0;&#x4EE5;&#x53CA;Spark Core&#x5B9E;&#x73B0;&#x8FC7;&#x7A0B;&#x5206;&#x6790;</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#求平均年龄的spark-core实现"><span class="toc-nav-number">2.7.</span> <span class="toc-nav-text">&#x6C42;&#x5E73;&#x5747;&#x5E74;&#x9F84;&#x7684;Spark Core&#x5B9E;&#x73B0;</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#求男女人数以及最高和最低身高"><span class="toc-nav-number">2.8.</span> <span class="toc-nav-text">&#x6C42;&#x7537;&#x5973;&#x4EBA;&#x6570;&#x4EE5;&#x53CA;&#x6700;&#x9AD8;&#x548C;&#x6700;&#x4F4E;&#x8EAB;&#x9AD8;</span></a></li></ol></li></ol>
        
        </div>
      </aside>
    

                
            <!-- Sidebar Container -->
            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                sidebar-container">

                <!-- Featured Tags -->
                
                <section>
                    <!-- no hr -->
                    <h5><a href="/tags/">FEATURED TAGS</a></h5>
                    <div class="tags">
                       
                          <a class="tag" href="/tags/#Spark" title="Spark">Spark</a>
                        
                    </div>
                </section>
                

                <!-- Friends Blog -->
                
                <hr>
                <h5>FRIENDS</h5>
                <ul class="list-inline">

                    
                        <li><a href="https://sxwanggit126.github.io" target="_blank">DoubleHappy</a></li>
                    
                </ul>
                
            </div>
        </div>
    </div>
</article>








<!-- async load function -->
<script>
    function async(u, c) {
      var d = document, t = 'script',
          o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
      s.parentNode.insertBefore(o, s);
    }
</script>
<!-- anchor-js, Doc:http://bryanbraun.github.io/anchorjs/ -->
<script>
    async("https://cdn.bootcss.com/anchor-js/1.1.1/anchor.min.js",function(){
        anchors.options = {
          visible: 'hover',
          placement: 'left',
          icon: 'rz'
        };
        anchors.add().remove('.intro-header h1').remove('.subheading').remove('.sidebar-container h5');
    })
</script>
<style>
    /* place left on bigger screen */
    @media all and (min-width: 800px) {
        .anchorjs-link{
            position: absolute;
            left: -0.75em;
            font-size: 1.1em;
            margin-top : -0.1em;
        }
    }
</style>


<!-- chrome Firefox 中文锚点定位失效-->
<script src="https://cdn.bootcss.com/jquery/3.3.1/jquery.js"></script>
<!-- smooth scroll behavior polyfill  -->
<script type="text/javascript" src="/js/smoothscroll.js"></script>
<script>
        $('#toc').on('click','a',function(a){
            // var isChrome = window.navigator.userAgent.indexOf("Chrome") !== -1;
            // console.log(window.navigator.userAgent,isChrome)
                // if(isChrome) {
                    // console.log(a.currentTarget.outerHTML);
                    // console.log($(a.currentTarget).attr("href"));
                    //跳转到指定锚点
                    // document.getElementById(a.target.innerText.toLowerCase()).scrollIntoView(true);
                    document.getElementById($(a.currentTarget).attr("href").replace("#","")).scrollIntoView({behavior: 'smooth' });
                // }
        })  
</script>


    <!-- Footer -->
    <!-- Footer -->
<footer>
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <ul class="list-inline text-center">
                
                
                

                

                

                
                    <li>
                        <a target="_blank"  href="https://github.com/LeoFWZ">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                

                

                </ul>
                <p class="copyright text-muted">
                    Copyright &copy; FWZ 2020 
                    By <a href="https://leofwz.github.io/">LeoFWZ</a> | BigData
                </p>
            </div>
        </div>
    </div>
</footer>

<!-- jQuery -->
<script src="/js/jquery.min.js"></script>

<!-- Bootstrap Core JavaScript -->
<script src="/js/bootstrap.min.js"></script>

<!-- Custom Theme JavaScript -->
<script src="/js/hux-blog.min.js"></script>


<!-- async load function -->
<script>
    function async(u, c) {
      var d = document, t = 'script',
          o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
      s.parentNode.insertBefore(o, s);
    }
</script>

<!-- 
     Because of the native support for backtick-style fenced code blocks 
     right within the Markdown is landed in Github Pages, 
     From V1.6, There is no need for Highlight.js, 
     so Huxblog drops it officially.

     - https://github.com/blog/2100-github-pages-now-faster-and-simpler-with-jekyll-3-0  
     - https://help.github.com/articles/creating-and-highlighting-code-blocks/    
-->
<!--
    <script>
        async("http://cdn.bootcss.com/highlight.js/8.6/highlight.min.js", function(){
            hljs.initHighlightingOnLoad();
        })
    </script>
    <link href="http://cdn.bootcss.com/highlight.js/8.6/styles/github.min.css" rel="stylesheet">
-->


<!-- jquery.tagcloud.js -->
<script>
    // only load tagcloud.js in tag.html
    if($('#tag_cloud').length !== 0){
        async("https://leofwz.github.io/js/jquery.tagcloud.js",function(){
            $.fn.tagcloud.defaults = {
                //size: {start: 1, end: 1, unit: 'em'},
                color: {start: '#bbbbee', end: '#0085a1'},
            };
            $('#tag_cloud a').tagcloud();
        })
    }
</script>

<!--fastClick.js -->
<script>
    async("https://cdn.bootcss.com/fastclick/1.0.6/fastclick.min.js", function(){
        var $nav = document.querySelector("nav");
        if($nav) FastClick.attach($nav);
    })
</script>


<!-- Google Analytics -->


<script>
    // dynamic User by Hux
    var _gaId = 'UA-XXXXXXXX-X';
    var _gaDomain = 'yoursite';

    // Originial
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', _gaId, _gaDomain);
    ga('send', 'pageview');
</script>




<!-- Baidu Tongji -->

<script>
    // dynamic User by Hux
    var _baId = 'xxx';

    // Originial
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "//hm.baidu.com/hm.js?" + _baId;
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
</script>






	<a id="rocket" href="#top" class=""></a>
	<script type="text/javascript" src="/js/totop.js?v=1.0.0" async=""></script>
    <script type="text/javascript" src="/js/toc.js?v=1.0.0" async=""></script>
<!-- Image to hack wechat -->
<img src="https://leofwz.github.io/img/icon_wechat.png" width="0" height="0" />
<!-- Migrate from head to bottom, no longer block render and still work -->

</body>

</html>
