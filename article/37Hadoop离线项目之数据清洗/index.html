<!DOCTYPE html>
<html lang="en">

<!-- Head tag -->
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="google-site-verification" content="xBT4GhYoi5qRD5tr338pgPM5OWHHIDR6mNg1a3euekI" />
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="Every failure is leading towards success">
    <meta name="keyword"  content="Fangwuzhou,BigData, Hadoop,Java,Python,Spark,Flink,Kafka">
    <link rel="shortcut icon" href="/img/ironman-draw.png">
    <!-- Place this tag in your head or just before your close body tag. -->
    <script async defer src="https://buttons.github.io/buttons.js"></script>
    <title>
        
          [Hadoop离线项目之数据清洗]   - Leo@FWZ| Blog
        
    </title>

    <link rel="canonical" href="https://leofwz.github.io/article/37Hadoop离线项目之数据清洗/">

    <!-- Bootstrap Core CSS -->
    <link rel="stylesheet" href="/css/bootstrap.min.css">

    <!-- Custom CSS --> 
    <link rel="stylesheet" href="/css/beantech.min.css">

    <link rel="stylesheet" href="/css/donate.css">
    
    <!-- Pygments Highlight CSS -->
    <link rel="stylesheet" href="/css/highlight.css">

    <link rel="stylesheet" href="/css/widget.css">

    <link rel="stylesheet" href="/css/rocket.css">

    <link rel="stylesheet" href="/css/signature.css">

    <link rel="stylesheet" href="/css/toc.css">

    <!-- Custom Fonts -->
    <!-- <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css" rel="stylesheet" type="text/css"> -->
    <!-- Hux change font-awesome CDN to qiniu -->
    <link href="https://cdn.staticfile.org/font-awesome/4.5.0/css/font-awesome.min.css" rel="stylesheet" type="text/css">


    <!-- Hux Delete, sad but pending in China
    <link href='http://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
    <link href='http://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/
    css'>
    -->


    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

    <!-- ga & ba script hoook -->
    <script></script>
</head>


<!-- hack iOS CSS :active style -->
<body ontouchstart="">
	<!-- Modified by Yu-Hsuan Yen -->
<!-- Post Header -->
<style type="text/css">
    header.intro-header{
        
            background-image: url('/img/article_header/article_header.png')
            /*post*/
        
    }
    
</style>

<header class="intro-header" >
    <!-- Signature -->
    <div id="signature">
        <div class="container">
            <div class="row">
                <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                
                    <div class="post-heading">
                        <div class="tags">
                            
                              <a class="tag" href="/tags/#Hadoop" title="Hadoop">Hadoop</a>
                            
                        </div>
                        <h1>[Hadoop离线项目之数据清洗]  </h1>
                        <h2 class="subheading"></h2>
                        <span class="meta">
                            Posted by FWZ on
                            2018-02-28
                        </span>
                    </div>
                


                </div>
            </div>
        </div>
    </div>
</header>

	
    <!-- Navigation -->
<nav class="navbar navbar-default navbar-custom navbar-fixed-top">
    <div class="container-fluid">
        <!-- Brand and toggle get grouped for better mobile display -->
        <div class="navbar-header page-scroll">
            <button type="button" class="navbar-toggle">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="/">Leo@FWZ</a>
        </div>

        <!-- Collect the nav links, forms, and other content for toggling -->
        <!-- Known Issue, found by Hux:
            <nav>'s height woule be hold on by its content.
            so, when navbar scale out, the <nav> will cover tags.
            also mask any touch event of tags, unfortunately.
        -->
        <div id="huxblog_navbar">
            <div class="navbar-collapse">
                <ul class="nav navbar-nav navbar-right">
                    <li>
                        <a href="/">Home</a>
                    </li>

                    

                        
                    

                        
                        <li>
                            <a href="/about/">About</a>
                        </li>
                        
                    

                        
                        <li>
                            <a href="/tags/">Tags</a>
                        </li>
                        
                    

                        
                        <li>
                            <a href="/archive/">Archives</a>
                        </li>
                        
                    
                    
                </ul>
            </div>
        </div>
        <!-- /.navbar-collapse -->
    </div>
    <!-- /.container -->
</nav>
<script>
    // Drop Bootstarp low-performance Navbar
    // Use customize navbar with high-quality material design animation
    // in high-perf jank-free CSS3 implementation
    var $body   = document.body;
    var $toggle = document.querySelector('.navbar-toggle');
    var $navbar = document.querySelector('#huxblog_navbar');
    var $collapse = document.querySelector('.navbar-collapse');

    $toggle.addEventListener('click', handleMagic)
    function handleMagic(e){
        if ($navbar.className.indexOf('in') > 0) {
        // CLOSE
            $navbar.className = " ";
            // wait until animation end.
            setTimeout(function(){
                // prevent frequently toggle
                if($navbar.className.indexOf('in') < 0) {
                    $collapse.style.height = "0px"
                }
            },400)
        }else{
        // OPEN
            $collapse.style.height = "auto"
            $navbar.className += " in";
        }
    }
</script>


    <!-- Main Content -->
    <!-- Modify by Yu-Hsuan Yen -->

<!-- Post Content -->
<article>
    <div class="container">
        <div class="row">

            <!-- Post Container -->
            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                post-container">

                <h1 id="1企业级大数据项目开发流程">1.企业级大数据项目开发流程</h1>
<ul>
<li>项目调研：以业务为导向而不是技术。这个一般是非常熟悉业务的产品经理、项目经理去做。</li>
<li>需求分析：明确要做什么 ，最后做成什么样子，不关心怎么做、不关心用什么技术，当然也需要考虑用户提出来的显示需求以及可以挖掘的隐式需求（甘特图）</li>
<li>方案设计：概要设计、详细设计、系统设计（可否容错、可否定制化）</li>
<li>功能开发：开发、测试（单元测试 CICD）</li>
<li>测试：功能、联调、性能、用户试用</li>
<li>部署上线：试运行（新老系统并行运行，DIFF 、稳定性）、正式上线 、 灰度</li>
<li>后期 ：2期、3期、4期等，运维保障，功能开发 ，bug修复</li>
</ul>
<h1 id="2企业级大数据应用分类方向">2.企业级大数据应用分类(方向)</h1>
<p>企业级大数据应用平台：</p>
<ul>
<li>数据分析：比如BI等。数据分析可以分为两大类：自研和商业。自研是自己公司开发的，基于开源的框架进行二次开发，自研的好处是数据都在自己公司，后面可以很方便构建自己的用户画像、用户推荐、精准营销等。商业是公司用阿里、腾讯等其他企业的产品，这些对于小公司来说方便些，出故障不需要自己去维护，它有后勤保障。中大型公司一般都是自研。重要的是数据都在自己这边。</li>
<li>搜索/爬虫 :elk、solr、lucence、es、爬虫</li>
<li>机器学习/深度学习 :对个人的学历要求门槛很高，重点学校研究生毕业才可以</li>
<li>人工智能 :对个人的学历要求门槛很高，重点学校研究生毕业才可以，数据分析具体分为离线和实时两条线。根据已有的数据数据发现更多的价值。</li>
<li>离线处理</li>
<li>实时处理</li>
</ul>
<h1 id="3基于maven构建大数据开发项目">3.基于Maven构建大数据开发项目</h1>
<ul>
<li>对CDN的点击日志数据清洗</li>
<li>进行MR编程</li>
<li>对日志文件数据进行清洗</li>
<li>并将清洗后的数据加载到hive外部表</li>
</ul>
<h2 id="31创建基于maven的hadoop项目">3.1创建基于Maven的hadoop项目</h2>
<p>现在你去访问某个产品或者某个web页面或者某个直播视频会产生某个日志信息。比如：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">baidu	CN	A	E	[17/Jul/2018:17:07:50 +0800]	2	223.104.18.110	-	112.29.213.35:80	0	v2.go2yd.com	GET	http://v1.go2yd.com/user_upload/1531633977627104fdecdc68fe7a2c4b96b2226fd3f4c.mp4_bd.mp4	HTTP/1.1	-	bytes 13869056-13885439/25136186	TCP_HIT/206	112.29.213.35	video/mp4	17168	16384	-:0	0	0	-	-	-	11451601	-	&quot;JSP3/2.0.14&quot;	&quot;-&quot;	&quot;-&quot;	&quot;-&quot;	http	-	2	v1.go2yd.com	0.002	25136186	16384	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	1531818470104-11451601-112.29.213.66#2705261172	644514568</span><br></pre></td></tr></table></figure>
<p>这是一条日志记录，里面有很多字段，第一个字段是：baidu是cdn的厂商；CN是中国；E是级别；后面是访问的时候所产生的时间；223.104.18.110是访问的ip；112.29.213.35:80是服务端的ip；v2.go2yd.com是域名；后面http…是url；TCP_HIT/206命中缓存（看一个视频的时候，先去缓存里你看一下有没有，如果没有再去服务器上拿）；17168是所需要的流量。</p>
<p>现在我们可以手动造一批日志信息的数据，然后对这些日志信息进行清洗</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line">package com.leo.hadoop.utils;</span><br><span class="line"></span><br><span class="line">import java.io.*;</span><br><span class="line">import java.text.DateFormat;</span><br><span class="line">import java.text.ParseException;</span><br><span class="line">import java.text.SimpleDateFormat;</span><br><span class="line">import java.util.Date;</span><br><span class="line">import java.util.Locale;</span><br><span class="line">import java.util.Random;</span><br><span class="line"></span><br><span class="line">public class insertdata &#123;</span><br><span class="line">    public static void main(String[] args) throws IOException, ParseException &#123;</span><br><span class="line"></span><br><span class="line">        int i  =0;</span><br><span class="line">        File file = new File(&quot;20190406.txt&quot;);</span><br><span class="line">        Writer out = new FileWriter(file);</span><br><span class="line">       while (i&lt;20000) &#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">           out.write(newData());</span><br><span class="line">           out.write(&quot;\n&quot;);</span><br><span class="line">           i++;</span><br><span class="line">       &#125;</span><br><span class="line">       out.close();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public static String newData() throws ParseException &#123;</span><br><span class="line">        DateFormat sourceFormat = new SimpleDateFormat(&quot;dd/MMM/yyyy:HH:mm:ss&quot;, Locale.ENGLISH);</span><br><span class="line"></span><br><span class="line">        Random random = new Random();</span><br><span class="line"></span><br><span class="line">        String log = &quot;baidu\tCN\tA\tE\t[17/Jul/2018:17:07:50 +0800]\t2\t223.104.18.110\t-\t112.29.213.35:80\t0\tv2.go2yd.com\tGET\thttp://v1.go2yd.com/user_upload/1531633977627104fdecdc68fe7a2c4b96b2226fd3f4c.mp4_bd.mp4\tHTTP/1.1\t-\tbytes 13869056-13885439/25136186\tTCP_HIT/206\t112.29.213.35\tvideo/mp4\t17168\t16384\t-:0\t0\t0\t-\t-\t-\t11451601\t-\t\&quot;JSP3/2.0.14\&quot;\t\&quot;-\&quot;\t\&quot;-\&quot;\t\&quot;-\&quot;\thttp\t-\t2\tv1.go2yd.com\t0.002\t25136186\t16384\t-\t-\t-\t-\t-\t-\t-\t-\t-\t-\t-\t-\t-\t-\t-\t-\t-\t-\t-\t-\t-\t-\t-\t-\t-\t-\t-\t-\t-\t-\t1531818470104-11451601-112.29.213.66#2705261172\t644514568&quot;;</span><br><span class="line"></span><br><span class="line">        String[] split = log.split(&quot;\t&quot;);</span><br><span class="line"></span><br><span class="line">        StringBuilder builder = new StringBuilder(&quot;&quot;);</span><br><span class="line">        String oldTime = split[4];</span><br><span class="line">        Date parse = sourceFormat.parse(&quot;17/Jul/2018:00:00:00&quot;);</span><br><span class="line">        long time = parse.getTime();</span><br><span class="line">        long ts = time + random.nextInt(60)*6000+600000*random.nextInt(35);</span><br><span class="line">        String newTime = &quot;[&quot;+sourceFormat.format(ts)+&quot; &quot;+&quot;+0800]&quot;;</span><br><span class="line">        split[4] = newTime;</span><br><span class="line">//        System.out.println(newTime);</span><br><span class="line"></span><br><span class="line">        String oldIp = split[6];</span><br><span class="line">        String newIp = random.nextInt(255)+&quot;.&quot;+random.nextInt(255)+&quot;.&quot;+random.nextInt(255)+&quot;.&quot;+random.nextInt(255);</span><br><span class="line">        split[6] = newIp;</span><br><span class="line">//        System.out.println(newIp);</span><br><span class="line"></span><br><span class="line">        String oldDomain = split[10];</span><br><span class="line">        String newDomain = &quot;v&quot;+(random.nextInt(4)+1)+&quot;.go2yd.com&quot;;</span><br><span class="line">        split[10] = newDomain;</span><br><span class="line">//        System.out.println(newDomain);</span><br><span class="line"></span><br><span class="line">        String oldTraffic = split[20];</span><br><span class="line">        String newTraffic = String.valueOf(random.nextInt(100000));</span><br><span class="line">        split[20] = newTraffic;</span><br><span class="line">//        System.out.println(newTraffic);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        for (int i = 0 ;i&lt;72;i++)&#123;</span><br><span class="line">            if(i == 71)&#123;</span><br><span class="line">                builder.append(split[i]);</span><br><span class="line">            &#125;else&#123;</span><br><span class="line">                builder.append(split[i]).append(&quot;\t&quot;);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        String newLog = builder.toString();</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">//        System.out.println(newLog);</span><br><span class="line">        return newLog;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>通过上述代码后我们就有了一批测试数据，先用IDEA创建一个maven项目：（maven是需要联网的）</p>
<p>java quickstart</p>
<p><img src="https://img-blog.csdnimg.cn/20190406091340360.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xpd2VpaG9wZQ==,size_16,color_FFFFFF,t_70" alt="image"></p>
<p>下一步：<br>
<img src="https://img-blog.csdnimg.cn/20190406091451378.png" alt="iamge"></p>
<p>下一步：<br>
<img src="https://img-blog.csdnimg.cn/20190406091859365.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xpd2VpaG9wZQ==,size_16,color_FFFFFF,t_70" alt="image"></p>
<p>下一步：<br>
<img src="https://img-blog.csdnimg.cn/20190406092040990.png" alt="iamge"></p>
<p>然后确定完成，第一次建立这个项目的时候会比较慢，因为要下载很多包什么的。</p>
<p>下一步：</p>
<p><img src="https://img-blog.csdnimg.cn/20190406093007696.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xpd2VpaG9wZQ==,size_16,color_FFFFFF,t_70" alt="iamge"></p>
<p>pom.xml是maven的一个配置文件</p>
<p>main下面是开发代码的；test下面是测试代码的。两个下面的java颜色是不一样的。</p>
<p>在$MAVEN_HOME/conf/setting.xml中添加这个，让maven的仓库放在C盘以外，不然时间长了，存储越来越多。(Windows本地装的maven环境)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;localRepository&gt;D:\\software\\maven_repository&lt;/localRepository&gt;</span><br></pre></td></tr></table></figure>
<p>在pom.xml的properties里添加hadoop的依赖：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;properties&gt;</span><br><span class="line">	&lt;hadoop.version&gt;2.6.0-cdh5.7.0&lt;/hadoop.version&gt; </span><br><span class="line">&lt;/properties&gt;</span><br></pre></td></tr></table></figure>
<p>这里面添加的版本只需要和你的这个hadoop版本近似就行，比如生产上用CDH的，这里也可以用apache的，因为到时候我们打的是瘦包（仅仅打包你开发的代码），Hadoop的包是不会打到里面去的，只是开发的时候用到的一个说明而已。</p>
<p>由于我们选择是CDH版本的，所以还要添加一个仓库：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&lt;!--添加CDH的仓库--&gt;</span><br><span class="line">&lt;repositories&gt;</span><br><span class="line">  &lt;repository&gt;</span><br><span class="line">    &lt;id&gt;cloudera&lt;/id&gt;</span><br><span class="line">    &lt;url&gt;https://repository.cloudera.com/artifactory/cloudera-repos&lt;/url&gt;</span><br><span class="line">  &lt;/repository&gt;</span><br><span class="line">&lt;/repositories&gt;</span><br></pre></td></tr></table></figure>
<p>在pom.xml中添加</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&lt;!--添加Hadoop的依赖--&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">  &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;</span><br><span class="line">  &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt;</span><br><span class="line">  &lt;version&gt;$&#123;hadoop.version&#125;&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure>
<p><strong>为什么要添加hadoop.version这个变量呢？</strong></p>
<p>为了以后重构的需要，以后可能还要添加其他版本的Hadoop，如果每次修改就可以直接用这个变量${hadoop.version}就行了。</p>
<p>到此已经把工程建好了。</p>
<h2 id="32日志解析功能开发">3.2日志解析功能开发</h2>
<p>main下面建立两个包，utils下面再建立一个java类：LogUtils;test测试代码中建立一个utils包，包下面建立一个java类：TestLogUtils 单元测试</p>
<p><img src="https://img-blog.csdnimg.cn/20190406094807117.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xpd2VpaG9wZQ==,size_16,color_FFFFFF,t_70" alt="image"></p>
<p>在LogUtils类中添加如下代码：用于解析日志，清洗数据</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line">package com.ruozedata.hadoop.utils;</span><br><span class="line"></span><br><span class="line">import java.text.DateFormat;</span><br><span class="line">import java.text.ParseException;</span><br><span class="line">import java.text.SimpleDateFormat;</span><br><span class="line">import java.util.Locale;</span><br><span class="line"></span><br><span class="line">public class LogUtils &#123;</span><br><span class="line">    DateFormat sourceFormat = new SimpleDateFormat(&quot;dd/MMM/yyyy:HH:mm:ss&quot;, Locale.ENGLISH);</span><br><span class="line">    DateFormat targetFormat = new SimpleDateFormat(&quot;yyyyMMddHHmmss&quot;);</span><br><span class="line"></span><br><span class="line">    /**</span><br><span class="line">     * 日志文件解析，对内容字段的处理</span><br><span class="line">     * 按\t分割</span><br><span class="line">     */</span><br><span class="line">    public String parse(String log)&#123;</span><br><span class="line">        String result=&quot;&quot;;</span><br><span class="line">        try &#123;</span><br><span class="line">            String[] splits = log.split(&quot;\t&quot;);</span><br><span class="line">            String cdn = splits[0];</span><br><span class="line">            String region = splits[1];</span><br><span class="line">            String level = splits[3];</span><br><span class="line">            String timeStr = splits[4];</span><br><span class="line">            String time = timeStr.substring(1,timeStr.length()-7);</span><br><span class="line">            time = targetFormat.format(sourceFormat.parse(time));</span><br><span class="line">            String ip = splits[6];</span><br><span class="line">            String domain = splits[10];</span><br><span class="line">            String url = splits[12];</span><br><span class="line">            String traffic = splits[20];</span><br><span class="line"></span><br><span class="line">//            System.out.println(cdn);</span><br><span class="line">//            System.out.println(region);</span><br><span class="line">//            System.out.println(level);</span><br><span class="line">//            System.out.println(time);</span><br><span class="line">//            System.out.println(ip);</span><br><span class="line">//            System.out.println(domain);</span><br><span class="line">//            System.out.println(url);</span><br><span class="line">//            System.out.println(traffic);</span><br><span class="line"></span><br><span class="line">            //面试题：StringBuilder和StringBuffer的区别：线程安全不安全</span><br><span class="line">            //解析出来的日志 → external table location是给外部表用的，所以用\t键隔开,用append拼接</span><br><span class="line">            StringBuilder builder = new StringBuilder(&quot;&quot;);</span><br><span class="line">            builder.append(cdn).append(&quot;\t&quot;)</span><br><span class="line">                    .append(region).append(&quot;\t&quot;)</span><br><span class="line">                    .append(level).append(&quot;\t&quot;)</span><br><span class="line">                    .append(time).append(&quot;\t&quot;)</span><br><span class="line">                    .append(ip).append(&quot;\t&quot;)</span><br><span class="line">                    .append(domain).append(&quot;\t&quot;)</span><br><span class="line">                    .append(url).append(&quot;\t&quot;)</span><br><span class="line">                    .append(traffic);</span><br><span class="line"></span><br><span class="line">            result = builder.toString();</span><br><span class="line">        &#125; catch (ParseException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        return result;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在单元测试TestLogUtils 里添加如下代码：进行测试</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">package com.ruozedata.hadoop.utils;</span><br><span class="line"></span><br><span class="line">import org.junit.After;</span><br><span class="line">import org.junit.Before;</span><br><span class="line">import org.junit.Test;</span><br><span class="line"></span><br><span class="line">public class TestLogUtils &#123;</span><br><span class="line"></span><br><span class="line">    private LogUtils utils;</span><br><span class="line">    @Before</span><br><span class="line">    public void setUp()&#123;</span><br><span class="line">        utils=new LogUtils();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    @After</span><br><span class="line">    public void tearDown()&#123;</span><br><span class="line">        utils=null;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    @Test</span><br><span class="line">    public void testLogParse()&#123;</span><br><span class="line">        String log=&quot;baidu\tCN\tA\tE\t[17/Jul/2018:17:07:50 +0800]\t2\t223.104.18.110\t-\t112.29.213.35:80\t0\tv2.go2yd.com\tGET\thttp://v1.go2yd.com/user_upload/1531633977627104fdecdc68fe7a2c4b96b2226fd3f4c.mp4_bd.mp4\tHTTP/1.1\t-\tbytes 13869056-13885439/25136186\tTCP_HIT/206\t112.29.213.35\tvideo/mp4\t17168\t16384\t-:0\t0\t0\t-\t-\t-\t11451601\t-\t\&quot;JSP3/2.0.14\&quot;\t\&quot;-\&quot;\t\&quot;-\&quot;\t\&quot;-\&quot;\thttp\t-\t2\tv1.go2yd.com\t0.002\t25136186\t16384\t-\t-\t-\t-\t-\t-\t-\t-\t-\t-\t-\t-\t-\t-\t-\t-\t-\t-\t-\t-\t-\t-\t-\t-\t-\t-\t-\t-\t-\t-\t1531818470104-11451601-112.29.213.66#2705261172\t644514568&quot;;</span><br><span class="line">        String result = utils.parse(log);</span><br><span class="line">        System.out.println(result);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>单元测试的结果：（仅仅测试了一条记录）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">baidu	CN	E	20180717170750	223.104.18.110	v2.go2yd.com	http://v1.go2yd.com/user_upload/1531633977627104fdecdc68fe7a2c4b96b2226fd3f4c.mp4_bd.mp4	16384</span><br><span class="line"></span><br><span class="line">Process finished with exit code 0</span><br></pre></td></tr></table></figure>
<p>现在还需要开发一个mapreduce。</p>
<h2 id="33数据清洗etl功能本地测试">3.3数据清洗ETL功能本地测试</h2>
<p>建个mapper包，包下面建个LogETLMapper类；建个driver包，包下面建个LogETLDriver类。这些代码从driver作为入口。里面有mian方法，里面配置MapReduce的输入输出。（只有map，没有reduce）</p>
<p><img src="https://img-blog.csdnimg.cn/20190406115558192.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xpd2VpaG9wZQ==,size_16,color_FFFFFF,t_70" alt="iamge"></p>
<p>代码如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">package com.ruozedata.hadoop.mapreduce.driver;</span><br><span class="line"></span><br><span class="line">import com.ruozedata.hadoop.mapreduce.mapper.LogETLMapper;</span><br><span class="line">import org.apache.hadoop.conf.Configuration;</span><br><span class="line">import org.apache.hadoop.fs.FileSystem;</span><br><span class="line">import org.apache.hadoop.fs.Path;</span><br><span class="line">import org.apache.hadoop.io.NullWritable;</span><br><span class="line">import org.apache.hadoop.io.Text;</span><br><span class="line">import org.apache.hadoop.mapreduce.Job;</span><br><span class="line">import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line">import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line">import java.io.FileInputStream;</span><br><span class="line"></span><br><span class="line">public class LogETLDriver &#123;</span><br><span class="line">    public static void main(String[] args) throws Exception &#123;</span><br><span class="line">        if(args.length != 2)&#123;</span><br><span class="line">            System.err.println(&quot;please input 2 params: input output&quot;);</span><br><span class="line">            System.exit(0);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        String input = args[0];</span><br><span class="line">        String output = args[1];  //&quot;output/d=20180717&quot;</span><br><span class="line"></span><br><span class="line">        //在本地运行的window环境需要加上 而打包到服务器注释掉本行</span><br><span class="line">        System.setProperty(&quot;hadoop.home.dir&quot;,&quot;D:/IDEAMaven/hadoop-2.6.0-cdh5.7.0&quot;);</span><br><span class="line"></span><br><span class="line">        Configuration configuration = new Configuration();</span><br><span class="line"></span><br><span class="line">        FileSystem fileSystem = FileSystem.get(configuration);</span><br><span class="line">        Path outputPath = new Path(output);</span><br><span class="line">        if(fileSystem.exists(outputPath))&#123;</span><br><span class="line">            fileSystem.delete(outputPath,true);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        Job job = Job.getInstance(configuration);</span><br><span class="line">        job.setJarByClass(LogETLDriver.class);</span><br><span class="line">        job.setMapperClass(LogETLMapper.class);</span><br><span class="line">        job.setMapOutputKeyClass(NullWritable.class);</span><br><span class="line">        job.setMapOutputValueClass(Text.class);</span><br><span class="line"></span><br><span class="line">        FileInputFormat.setInputPaths(job,new Path(input));</span><br><span class="line">        FileOutputFormat.setOutputPath(job,new Path(output));</span><br><span class="line"></span><br><span class="line">        job.waitForCompletion(true);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>然后加上参数：</p>
<p><img src="https://img-blog.csdnimg.cn/20190414100850224.png" alt="iamge"></p>
<p><img src="https://img-blog.csdnimg.cn/2019041410094067.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xpd2VpaG9wZQ==,size_16,color_FFFFFF,t_70" alt="image"></p>
<p>然后直接运行main方法，看一下output里面的结果。<br>
若出现以下异常：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">log4j:WARN No appenders could be found for logger (org.apache.hadoop.metrics2.lib.MutableMetricsFactory).</span><br><span class="line">log4j:WARN Please initialize the log4j system properly.</span><br><span class="line">log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.</span><br><span class="line">Exception in thread &quot;main&quot; java.lang.NullPointerException</span><br><span class="line">	at java.lang.ProcessBuilder.start(ProcessBuilder.java:1012)</span><br><span class="line">	at org.apache.hadoop.util.Shell.runCommand(Shell.java:505)</span><br><span class="line">	at org.apache.hadoop.util.Shell.run(Shell.java:478)</span><br><span class="line">	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:738)</span><br></pre></td></tr></table></figure>
<p>解决方案：(前提是在Windows本地先下载hadoop-2.6.0-cdh5.7.0包，然后解压)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1）、在https://github.com/4ttty/winutils 下载hadoop.dll和winutils.exe 文件。</span><br><span class="line">2）、配置hadoop家目录:System.setProperty(&quot;hadoop.home.dir&quot;,&quot;D:/IDEAMaven/hadoop-2.6.0-cdh5.7.0&quot;);</span><br><span class="line">3）、把hadoop.dll拷贝到C:\Windows\System32下面</span><br><span class="line">4）、把winutils.exe文件拷贝到$&#123;HADOOP_HOME&#125;/bin目录下</span><br></pre></td></tr></table></figure>
<p>上面配置完之后然后再运行就可以了：</p>
<p><img src="https://img-blog.csdnimg.cn/20190414130357816.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xpd2VpaG9wZQ==,size_16,color_FFFFFF,t_70" alt="iamge"></p>
<h2 id="34数据清洗功能服务器测试">3.4数据清洗功能服务器测试</h2>
<p>将代码打包过程：<br>
打包之前先把这句代码注释掉：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">//在本地运行的window环境需要加上 而打包到服务器注释掉本行</span><br><span class="line">//System.setProperty(&quot;hadoop.home.dir&quot;,&quot;D:/IDEAMaven/hadoop-2.6.0-cdh5.7.0&quot;)</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/20190414101316484.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xpd2VpaG9wZQ==,size_16,color_FFFFFF,t_70" alt="iamge"></p>
<p><img src="https://img-blog.csdnimg.cn/20190414101752972.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xpd2VpaG9wZQ==,size_16,color_FFFFFF,t_70" alt="iamge"></p>
<p>点击运行</p>
<p><img src="https://img-blog.csdnimg.cn/20190414101936497.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xpd2VpaG9wZQ==,size_16,color_FFFFFF,t_70" alt="iamge"></p>
<p>把本地jar包上传到服务器上</p>
<p><img src="https://img-blog.csdnimg.cn/20190414102315418.png" alt="iamge"></p>
<p>把日志文件传到服务器上</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@10-9-140-90 data]$ rz</span><br><span class="line">rz waiting to receive.</span><br><span class="line">Starting zmodem transfer.  Press Ctrl+C to cancel.</span><br><span class="line">Transferring 20180717.log...</span><br><span class="line">  100%    9573 KB    4786 KB/sec    00:00:02       0 Errors</span><br><span class="line">[hadoop@10-9-140-90 data]$ hdfs dfs -put /home/hadoop/data/20180717.log /data/</span><br><span class="line">19/04/14 10:35:29 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">[hadoop@10-9-140-90 data]$ hdfs dfs -ls /data</span><br><span class="line">19/04/14 10:35:46 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">Found 1 items</span><br><span class="line">-rw-r--r--   1 hadoop supergroup    9803062 2019-04-14 10:35 /data/20180717.log</span><br><span class="line">[hadoop@10-9-140-90 data]$</span><br></pre></td></tr></table></figure>
<p>然后运行</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop jar /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/lib/g6-hadoop-1.0.jar com.ruozedata.hadoop.mapreduce.driver.LogETLDriver /data/20180717.log /output</span><br></pre></td></tr></table></figure>
<p>运行完之后可以查看一下output目录</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@10-9-140-90 data]$ hdfs dfs -ls /output</span><br><span class="line">19/04/14 10:41:49 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">Found 2 items</span><br><span class="line">-rw-r--r--   1 hadoop supergroup          0 2019-04-14 10:40 /output/_SUCCESS</span><br><span class="line">-rw-r--r--   1 hadoop supergroup    2963062 2019-04-14 10:40 /output/part-r-00000</span><br><span class="line">[hadoop@10-9-140-90 data]$ hadoop fs -du -s -h /output</span><br><span class="line">19/04/14 10:43:00 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">2.8 M  2.8 M  /output</span><br><span class="line">[hadoop@10-9-140-90 data]$</span><br></pre></td></tr></table></figure>
<p>上面这个运行语句不会每次去手动输入，要放在shell脚本里，调度程序去调度这个shell脚本。<br>
现在创建一个g6-train-hadoop.sh脚本。添加如下代码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line"></span><br><span class="line">process_date=20180717</span><br><span class="line"></span><br><span class="line">echo &quot;step1:mapreduce etl&quot;</span><br><span class="line">hadoop jar /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/lib/g6-hadoop-1.0.jar com.ruozedata.hadoop.mapreduce.driver.LogETLDriver /data/$process_date.log /output/day=$process_date</span><br></pre></td></tr></table></figure>
<p>然后运行脚本：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@10-9-140-90 shell]$ chmod u+x g6-train-hadoop.sh </span><br><span class="line">[hadoop@10-9-140-90 shell]$ ./g6-train-hadoop.sh</span><br></pre></td></tr></table></figure>
<p>完了之后看结果：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@10-9-140-90 shell]$ hadoop fs -ls /output/day=20180717</span><br><span class="line">19/04/14 11:29:45 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">Found 2 items</span><br><span class="line">-rw-r--r--   1 hadoop supergroup          0 2019-04-14 11:28 /output/day=20180717/_SUCCESS</span><br><span class="line">-rw-r--r--   1 hadoop supergroup    2963062 2019-04-14 11:28 /output/day=20180717/part-r-00000</span><br><span class="line">[hadoop@10-9-140-90 shell]$</span><br></pre></td></tr></table></figure>
<h2 id="35使用hive完成最基本的统计分析功能">3.5使用Hive完成最基本的统计分析功能</h2>
<p>进入hive创建一张外部表</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">create external table g6_access (</span><br><span class="line">cdn string,</span><br><span class="line">region string,</span><br><span class="line">level string,</span><br><span class="line">time string,</span><br><span class="line">ip string,</span><br><span class="line">domain string,</span><br><span class="line">url string,</span><br><span class="line">traffic bigint</span><br><span class="line">) partitioned by (day string)</span><br><span class="line">ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;\t&apos;</span><br><span class="line">LOCATION &apos;/d6_hive/external/access/clear&apos;</span><br></pre></td></tr></table></figure>
<p>看一下</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@10-9-140-90 data]$ hdfs dfs -ls /d6_hive/external/access/clear</span><br><span class="line">19/04/14 11:52:36 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">[hadoop@10-9-140-90 data]$</span><br></pre></td></tr></table></figure>
<p>然后把上一节跑的结果 /output/day=20180717/part-r-00000 移动到hive的分区表里<br>
/d6_hive/external/access/clear/day=20180717 ：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@10-9-140-90 data]$ hdfs dfs -mkdir -p /d6_hive/external/access/clear/day=20180717</span><br><span class="line">19/04/14 11:58:10 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">[hadoop@10-9-140-90 data]$ hdfs dfs -mv /output/day=20180717/part-r-00000 /d6_hive/external/access/clear/day=20180717</span><br><span class="line">19/04/14 11:59:21 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">[hadoop@10-9-140-90 data]$ hdfs dfs -ls /d6_hive/external/access/clear/day=20180717</span><br><span class="line">19/04/14 11:59:41 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">Found 1 items</span><br><span class="line">-rw-r--r--   1 hadoop supergroup    2963062 2019-04-14 11:28 /d6_hive/external/access/clear/day=20180717/part-r-00000</span><br><span class="line">[hadoop@10-9-140-90 data]$</span><br></pre></td></tr></table></figure>
<p>现在在hive的g6_access表里还查不到，还需要刷一下元数据信息。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive (d6_test)&gt; alter table g6_access add if not exists partition(day=&apos;20180717&apos;);</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.2 seconds</span><br></pre></td></tr></table></figure>
<p>然后再查一下这张表，就可以查到数据了：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">hive (d6_test)&gt; select * from g6_access limit 10;</span><br><span class="line">OK</span><br><span class="line">cdn     region  level   time    ip      domain  url     traffic day</span><br><span class="line">baidu   CN      E       20180717042142  156.89.48.178   v2.go2yd.com    http://v1.go2yd.com/user_upload/1531633977627104fdecdc68fe7a2c4b96b2226fd3f4c.mp4_bd.mp4       62109   20180717</span><br><span class="line">baidu   CN      E       20180717042548  220.33.176.204  v4.go2yd.com    http://v1.go2yd.com/user_upload/1531633977627104fdecdc68fe7a2c4b96b2226fd3f4c.mp4_bd.mp4       15855   20180717</span><br><span class="line">baidu   CN      E       20180717035042  106.57.68.100   v4.go2yd.com    http://v1.go2yd.com/user_upload/1531633977627104fdecdc68fe7a2c4b96b2226fd3f4c.mp4_bd.mp4       93710   20180717</span><br><span class="line">baidu   CN      E       20180717032512  20.193.134.67   v1.go2yd.com    http://v1.go2yd.com/user_upload/1531633977627104fdecdc68fe7a2c4b96b2226fd3f4c.mp4_bd.mp4       65513   20180717</span><br><span class="line">baidu   CN      E       20180717022018  5.23.216.117    v1.go2yd.com    http://v1.go2yd.com/user_upload/1531633977627104fdecdc68fe7a2c4b96b2226fd3f4c.mp4_bd.mp4       643     20180717</span><br><span class="line">baidu   CN      E       20180717043206  217.63.184.100  v2.go2yd.com    http://v1.go2yd.com/user_upload/1531633977627104fdecdc68fe7a2c4b96b2226fd3f4c.mp4_bd.mp4       99433   20180717</span><br><span class="line">baidu   CN      E       20180717001518  245.160.115.101 v4.go2yd.com    http://v1.go2yd.com/user_upload/1531633977627104fdecdc68fe7a2c4b96b2226fd3f4c.mp4_bd.mp4       46296   20180717</span><br><span class="line">baidu   CN      E       20180717040324  238.38.219.35   v4.go2yd.com    http://v1.go2yd.com/user_upload/1531633977627104fdecdc68fe7a2c4b96b2226fd3f4c.mp4_bd.mp4       49385   20180717</span><br><span class="line">baidu   CN      E       20180717011300  230.141.140.80  v4.go2yd.com    http://v1.go2yd.com/user_upload/1531633977627104fdecdc68fe7a2c4b96b2226fd3f4c.mp4_bd.mp4       13311   20180717</span><br><span class="line">baidu   CN      E       20180717054030  223.3.4.174     v4.go2yd.com    http://v1.go2yd.com/user_upload/1531633977627104fdecdc68fe7a2c4b96b2226fd3f4c.mp4_bd.mp4       4959    20180717</span><br><span class="line">Time taken: 0.132 seconds, Fetched: 10 row(s)</span><br><span class="line">hive (d6_test)&gt;</span><br></pre></td></tr></table></figure>
<p>然后就可以进行统计分析，写sql了。<br>
例如，现在要求：统计每个domain的traffic之和：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">hive (d6_test)&gt; select domain,sum(traffic) from g6_access group by domain;</span><br><span class="line">Query ID = hadoop_20190414115050_17d05247-7e1f-455c-a018-ffb626e1d555</span><br><span class="line">Total jobs = 1</span><br><span class="line">Launching Job 1 out of 1</span><br><span class="line">Number of reduce tasks not specified. Estimated from input data size: 1</span><br><span class="line">In order to change the average load for a reducer (in bytes):</span><br><span class="line">  set hive.exec.reducers.bytes.per.reducer=&lt;number&gt;</span><br><span class="line">In order to limit the maximum number of reducers:</span><br><span class="line">  set hive.exec.reducers.max=&lt;number&gt;</span><br><span class="line">In order to set a constant number of reducers:</span><br><span class="line">  set mapreduce.job.reduces=&lt;number&gt;</span><br><span class="line">Starting Job = job_1551521482026_0058, Tracking URL = http://10-9-140-90:18088/proxy/application_1551521482026_0058/</span><br><span class="line">Kill Command = /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/bin/hadoop job  -kill job_1551521482026_0058</span><br><span class="line">Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1</span><br><span class="line">2019-04-14 12:08:25,157 Stage-1 map = 0%,  reduce = 0%</span><br><span class="line">2019-04-14 12:08:35,115 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.77 sec</span><br><span class="line">2019-04-14 12:08:46,160 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 3.49 sec</span><br><span class="line">MapReduce Total cumulative CPU time: 3 seconds 490 msec</span><br><span class="line">Ended Job = job_1551521482026_0058</span><br><span class="line">MapReduce Jobs Launched: </span><br><span class="line">Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 3.49 sec   HDFS Read: 2970723 HDFS Write: 92 SUCCESS</span><br><span class="line">Total MapReduce CPU Time Spent: 3 seconds 490 msec</span><br><span class="line">OK</span><br><span class="line">domain  _c1</span><br><span class="line">v1.go2yd.com    252434700</span><br><span class="line">v2.go2yd.com    252076506</span><br><span class="line">v3.go2yd.com    250212070</span><br><span class="line">v4.go2yd.com    248064592</span><br><span class="line">Time taken: 37.966 seconds, Fetched: 4 row(s)</span><br><span class="line">hive (d6_test)&gt;</span><br></pre></td></tr></table></figure>

                

                <hr>
                <!-- Pager -->
                <ul class="pager">
                    
                        <li class="previous">
                            <a href="/article/38HDFS HA 架构/" data-toggle="tooltip" data-placement="top" title="[HDFS HA 架构]  ">&larr; Previous Post</a>
                        </li>
                    
                    
                        <li class="next">
                            <a href="/article/36Hadoop生态离线项目/" data-toggle="tooltip" data-placement="top" title="[Hadoop生态离线项目]  ">Next Post &rarr;</a>
                        </li>
                    
                </ul>

                <br>

                <!--打赏-->
                
                <!--打赏-->

                <br>
                <!--分享-->
                
                    <div class="social-share"  data-wechat-qrcode-helper="" align="center"></div>
                    <!--  css & js -->
                    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/social-share.js/1.0.16/css/share.min.css">
                    <script src="https://cdnjs.cloudflare.com/ajax/libs/social-share.js/1.0.16/js/social-share.min.js"></script>
                
                <!--分享-->
                <br>                       
                
                <!-- require APlayer -->
                

                <!-- duoshuo Share start -->
                
                <!-- 多说 Share end-->

                <!-- 多说评论框 start -->
                
                <!-- 多说评论框 end -->

                <!-- disqus comment start -->
                
                <!-- disqus comment end -->

                

            </div>
            
            <!-- Tabe of Content -->
            <!-- Table of Contents -->

  
    
      <aside id="sidebar">
        <div id="toc" class="toc-article">
        <strong class="toc-title">Contents</strong>
        
          <ol class="toc-nav"><li class="toc-nav-item toc-nav-level-1"><a class="toc-nav-link" href="#1企业级大数据项目开发流程"><span class="toc-nav-number">1.</span> <span class="toc-nav-text">1.&#x4F01;&#x4E1A;&#x7EA7;&#x5927;&#x6570;&#x636E;&#x9879;&#x76EE;&#x5F00;&#x53D1;&#x6D41;&#x7A0B;</span></a></li><li class="toc-nav-item toc-nav-level-1"><a class="toc-nav-link" href="#2企业级大数据应用分类方向"><span class="toc-nav-number">2.</span> <span class="toc-nav-text">2.&#x4F01;&#x4E1A;&#x7EA7;&#x5927;&#x6570;&#x636E;&#x5E94;&#x7528;&#x5206;&#x7C7B;(&#x65B9;&#x5411;)</span></a></li><li class="toc-nav-item toc-nav-level-1"><a class="toc-nav-link" href="#3基于maven构建大数据开发项目"><span class="toc-nav-number">3.</span> <span class="toc-nav-text">3.&#x57FA;&#x4E8E;Maven&#x6784;&#x5EFA;&#x5927;&#x6570;&#x636E;&#x5F00;&#x53D1;&#x9879;&#x76EE;</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#31创建基于maven的hadoop项目"><span class="toc-nav-number">3.1.</span> <span class="toc-nav-text">3.1&#x521B;&#x5EFA;&#x57FA;&#x4E8E;Maven&#x7684;hadoop&#x9879;&#x76EE;</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#32日志解析功能开发"><span class="toc-nav-number">3.2.</span> <span class="toc-nav-text">3.2&#x65E5;&#x5FD7;&#x89E3;&#x6790;&#x529F;&#x80FD;&#x5F00;&#x53D1;</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#33数据清洗etl功能本地测试"><span class="toc-nav-number">3.3.</span> <span class="toc-nav-text">3.3&#x6570;&#x636E;&#x6E05;&#x6D17;ETL&#x529F;&#x80FD;&#x672C;&#x5730;&#x6D4B;&#x8BD5;</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#34数据清洗功能服务器测试"><span class="toc-nav-number">3.4.</span> <span class="toc-nav-text">3.4&#x6570;&#x636E;&#x6E05;&#x6D17;&#x529F;&#x80FD;&#x670D;&#x52A1;&#x5668;&#x6D4B;&#x8BD5;</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#35使用hive完成最基本的统计分析功能"><span class="toc-nav-number">3.5.</span> <span class="toc-nav-text">3.5&#x4F7F;&#x7528;Hive&#x5B8C;&#x6210;&#x6700;&#x57FA;&#x672C;&#x7684;&#x7EDF;&#x8BA1;&#x5206;&#x6790;&#x529F;&#x80FD;</span></a></li></ol></li></ol>
        
        </div>
      </aside>
    

                
            <!-- Sidebar Container -->
            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                sidebar-container">

                <!-- Featured Tags -->
                
                <section>
                    <!-- no hr -->
                    <h5><a href="/tags/">FEATURED TAGS</a></h5>
                    <div class="tags">
                       
                          <a class="tag" href="/tags/#Hadoop" title="Hadoop">Hadoop</a>
                        
                    </div>
                </section>
                

                <!-- Friends Blog -->
                
                <hr>
                <h5>FRIENDS</h5>
                <ul class="list-inline">

                    
                        <li><a href="https://sxwanggit126.github.io" target="_blank">DoubleHappy</a></li>
                    
                </ul>
                
            </div>
        </div>
    </div>
</article>








<!-- async load function -->
<script>
    function async(u, c) {
      var d = document, t = 'script',
          o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
      s.parentNode.insertBefore(o, s);
    }
</script>
<!-- anchor-js, Doc:http://bryanbraun.github.io/anchorjs/ -->
<script>
    async("https://cdn.bootcss.com/anchor-js/1.1.1/anchor.min.js",function(){
        anchors.options = {
          visible: 'hover',
          placement: 'left',
          icon: 'rz'
        };
        anchors.add().remove('.intro-header h1').remove('.subheading').remove('.sidebar-container h5');
    })
</script>
<style>
    /* place left on bigger screen */
    @media all and (min-width: 800px) {
        .anchorjs-link{
            position: absolute;
            left: -0.75em;
            font-size: 1.1em;
            margin-top : -0.1em;
        }
    }
</style>


<!-- chrome Firefox 中文锚点定位失效-->
<script src="https://cdn.bootcss.com/jquery/3.3.1/jquery.js"></script>
<!-- smooth scroll behavior polyfill  -->
<script type="text/javascript" src="/js/smoothscroll.js"></script>
<script>
        $('#toc').on('click','a',function(a){
            // var isChrome = window.navigator.userAgent.indexOf("Chrome") !== -1;
            // console.log(window.navigator.userAgent,isChrome)
                // if(isChrome) {
                    // console.log(a.currentTarget.outerHTML);
                    // console.log($(a.currentTarget).attr("href"));
                    //跳转到指定锚点
                    // document.getElementById(a.target.innerText.toLowerCase()).scrollIntoView(true);
                    document.getElementById($(a.currentTarget).attr("href").replace("#","")).scrollIntoView({behavior: 'smooth' });
                // }
        })  
</script>


    <!-- Footer -->
    <!-- Footer -->
<footer>
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <ul class="list-inline text-center">
                
                
                

                

                

                
                    <li>
                        <a target="_blank"  href="https://github.com/LeoFWZ">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                

                

                </ul>
                <p class="copyright text-muted">
                    Copyright &copy; FWZ 2020 
                    By <a href="https://leofwz.github.io/">LeoFWZ</a> | BigData
                </p>
            </div>
        </div>
    </div>
</footer>

<!-- jQuery -->
<script src="/js/jquery.min.js"></script>

<!-- Bootstrap Core JavaScript -->
<script src="/js/bootstrap.min.js"></script>

<!-- Custom Theme JavaScript -->
<script src="/js/hux-blog.min.js"></script>


<!-- async load function -->
<script>
    function async(u, c) {
      var d = document, t = 'script',
          o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
      s.parentNode.insertBefore(o, s);
    }
</script>

<!-- 
     Because of the native support for backtick-style fenced code blocks 
     right within the Markdown is landed in Github Pages, 
     From V1.6, There is no need for Highlight.js, 
     so Huxblog drops it officially.

     - https://github.com/blog/2100-github-pages-now-faster-and-simpler-with-jekyll-3-0  
     - https://help.github.com/articles/creating-and-highlighting-code-blocks/    
-->
<!--
    <script>
        async("http://cdn.bootcss.com/highlight.js/8.6/highlight.min.js", function(){
            hljs.initHighlightingOnLoad();
        })
    </script>
    <link href="http://cdn.bootcss.com/highlight.js/8.6/styles/github.min.css" rel="stylesheet">
-->


<!-- jquery.tagcloud.js -->
<script>
    // only load tagcloud.js in tag.html
    if($('#tag_cloud').length !== 0){
        async("https://leofwz.github.io/js/jquery.tagcloud.js",function(){
            $.fn.tagcloud.defaults = {
                //size: {start: 1, end: 1, unit: 'em'},
                color: {start: '#bbbbee', end: '#0085a1'},
            };
            $('#tag_cloud a').tagcloud();
        })
    }
</script>

<!--fastClick.js -->
<script>
    async("https://cdn.bootcss.com/fastclick/1.0.6/fastclick.min.js", function(){
        var $nav = document.querySelector("nav");
        if($nav) FastClick.attach($nav);
    })
</script>


<!-- Google Analytics -->


<script>
    // dynamic User by Hux
    var _gaId = 'UA-XXXXXXXX-X';
    var _gaDomain = 'yoursite';

    // Originial
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', _gaId, _gaDomain);
    ga('send', 'pageview');
</script>




<!-- Baidu Tongji -->

<script>
    // dynamic User by Hux
    var _baId = 'xxx';

    // Originial
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "//hm.baidu.com/hm.js?" + _baId;
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
</script>






	<a id="rocket" href="#top" class=""></a>
	<script type="text/javascript" src="/js/totop.js?v=1.0.0" async=""></script>
    <script type="text/javascript" src="/js/toc.js?v=1.0.0" async=""></script>
<!-- Image to hack wechat -->
<img src="https://leofwz.github.io/img/icon_wechat.png" width="0" height="0" />
<!-- Migrate from head to bottom, no longer block render and still work -->

</body>

</html>
