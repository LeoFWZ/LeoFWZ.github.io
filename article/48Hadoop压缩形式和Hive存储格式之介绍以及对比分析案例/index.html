<!DOCTYPE html>
<html lang="en">

<!-- Head tag -->
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="google-site-verification" content="xBT4GhYoi5qRD5tr338pgPM5OWHHIDR6mNg1a3euekI" />
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="Every failure is leading towards success">
    <meta name="keyword"  content="Fangwuzhou,BigData, Hadoop,Java,Python,Spark,Flink,Kafka">
    <link rel="shortcut icon" href="/img/ironman-draw.png">
    <!-- Place this tag in your head or just before your close body tag. -->
    <script async defer src="https://buttons.github.io/buttons.js"></script>
    <title>
        
          [Hadoop压缩形式和Hive存储格式之介绍以及对比分析案例]   - Leo@FWZ| Blog
        
    </title>

    <link rel="canonical" href="https://leofwz.github.io/article/48Hadoop压缩形式和Hive存储格式之介绍以及对比分析案例/">

    <!-- Bootstrap Core CSS -->
    <link rel="stylesheet" href="/css/bootstrap.min.css">

    <!-- Custom CSS --> 
    <link rel="stylesheet" href="/css/beantech.min.css">

    <link rel="stylesheet" href="/css/donate.css">
    
    <!-- Pygments Highlight CSS -->
    <link rel="stylesheet" href="/css/highlight.css">

    <link rel="stylesheet" href="/css/widget.css">

    <link rel="stylesheet" href="/css/rocket.css">

    <link rel="stylesheet" href="/css/signature.css">

    <link rel="stylesheet" href="/css/toc.css">

    <!-- Custom Fonts -->
    <!-- <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css" rel="stylesheet" type="text/css"> -->
    <!-- Hux change font-awesome CDN to qiniu -->
    <link href="https://cdn.staticfile.org/font-awesome/4.5.0/css/font-awesome.min.css" rel="stylesheet" type="text/css">


    <!-- Hux Delete, sad but pending in China
    <link href='http://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
    <link href='http://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/
    css'>
    -->


    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

    <!-- ga & ba script hoook -->
    <script></script>
</head>


<!-- hack iOS CSS :active style -->
<body ontouchstart="">
	<!-- Modified by Yu-Hsuan Yen -->
<!-- Post Header -->
<style type="text/css">
    header.intro-header{
        
            background-image: url('/img/article_header/article_header.png')
            /*post*/
        
    }
    
</style>

<header class="intro-header" >
    <!-- Signature -->
    <div id="signature">
        <div class="container">
            <div class="row">
                <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                
                    <div class="post-heading">
                        <div class="tags">
                            
                              <a class="tag" href="/tags/#Hadoop" title="Hadoop">Hadoop</a>
                            
                        </div>
                        <h1>[Hadoop压缩形式和Hive存储格式之介绍以及对比分析案例]  </h1>
                        <h2 class="subheading"></h2>
                        <span class="meta">
                            Posted by FWZ on
                            2018-04-25
                        </span>
                    </div>
                


                </div>
            </div>
        </div>
    </div>
</header>

	
    <!-- Navigation -->
<nav class="navbar navbar-default navbar-custom navbar-fixed-top">
    <div class="container-fluid">
        <!-- Brand and toggle get grouped for better mobile display -->
        <div class="navbar-header page-scroll">
            <button type="button" class="navbar-toggle">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="/">Leo@FWZ</a>
        </div>

        <!-- Collect the nav links, forms, and other content for toggling -->
        <!-- Known Issue, found by Hux:
            <nav>'s height woule be hold on by its content.
            so, when navbar scale out, the <nav> will cover tags.
            also mask any touch event of tags, unfortunately.
        -->
        <div id="huxblog_navbar">
            <div class="navbar-collapse">
                <ul class="nav navbar-nav navbar-right">
                    <li>
                        <a href="/">Home</a>
                    </li>

                    

                        
                    

                        
                        <li>
                            <a href="/about/">About</a>
                        </li>
                        
                    

                        
                        <li>
                            <a href="/tags/">Tags</a>
                        </li>
                        
                    

                        
                        <li>
                            <a href="/archive/">Archives</a>
                        </li>
                        
                    
                    
                </ul>
            </div>
        </div>
        <!-- /.navbar-collapse -->
    </div>
    <!-- /.container -->
</nav>
<script>
    // Drop Bootstarp low-performance Navbar
    // Use customize navbar with high-quality material design animation
    // in high-perf jank-free CSS3 implementation
    var $body   = document.body;
    var $toggle = document.querySelector('.navbar-toggle');
    var $navbar = document.querySelector('#huxblog_navbar');
    var $collapse = document.querySelector('.navbar-collapse');

    $toggle.addEventListener('click', handleMagic)
    function handleMagic(e){
        if ($navbar.className.indexOf('in') > 0) {
        // CLOSE
            $navbar.className = " ";
            // wait until animation end.
            setTimeout(function(){
                // prevent frequently toggle
                if($navbar.className.indexOf('in') < 0) {
                    $collapse.style.height = "0px"
                }
            },400)
        }else{
        // OPEN
            $collapse.style.height = "auto"
            $navbar.className += " in";
        }
    }
</script>


    <!-- Main Content -->
    <!-- Modify by Yu-Hsuan Yen -->

<!-- Post Content -->
<article>
    <div class="container">
        <div class="row">

            <!-- Post Container -->
            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                post-container">

                <h1 id="前言">前言</h1>
<p>之前的离线处理，有讲到有个ETL的过程，它实际上是把数据拆开按照指定的格式输出出来。之前的离线处理项目是只有map，没有reduce的，没有reduce就没有shuffle。</p>
<p>上篇的离线处理，是通过MapReduce的ETL过程，把数据输出到HDFS之上。这个数据采用的格式还是普通的文本格式，就是简简单单的纯文本格式。但是在生产上这样做是不行的。</p>
<p>在生产上面，每天的数据量很大，比如说每天数据量是500T，副本数为3，那就是1500T的数据，这样的话要占用的block块的数量要多少？这样肯定不行。</p>
<p>所以要在ETL的过程中把压缩加进去。</p>
<h1 id="1hadoop之mapreduce用到的压缩">1.Hadoop之MapReduce用到的压缩</h1>
<h2 id="11压缩的优点缺点之间的较量">1.1压缩的优点缺点之间的较量</h2>
<p>压缩带来的第一个好处就是减少存储磁盘空间，提高磁盘空间的利用率；压缩带来的第二大好处就是加快数据在磁盘和网络中的传输速度，从而提高系统的处理速度（比如在shuffle阶段），因为文件变小了，那么传输的肯定越快了。<br>
但是压缩同时也会带来一个坏处，当你去使用数据时，需要先将数据解压，那么解压和压缩都会加重CPU的负荷。而且压缩的越狠，耗费的时间越多。</p>
<p>在大数据中，任何一种调优，都不是万能的。有些场景适合这种调优，但是换一种场景，这种调优方式适得其反。<br>
不同的场景选择不同的压缩方式，肯定没有一个一劳永逸的方法，如果选择高压缩比，那么对于cpu的性能要求要高，同时压缩、解压时间耗费也多；选择压缩比低的，对于磁盘io、网络io的时间要多，空间占据要多；对于支持分割的，可以实现并行处理。<br>
比如说公司的CPU负载已经非常高了，那么这个时候肯定不能用压缩的。如果CPU负载很小，强烈建议开启压缩。磁盘和CPU之间的一个取舍，是压缩的优点和缺点的一个较量。</p>
<p>压缩技术有两种：有损和无损。</p>
<h2 id="12压缩使用的场景">1.2压缩使用的场景</h2>
<p>压缩场景：</p>
<ul>
<li>①input数据采集：用Flume 采集到 HDFS（后面交给MapReduce/spark进行处理）这个过程可以用到压缩。</li>
<li>②temp中间过程：map之后数据有落到磁盘的过程可以用到压缩。</li>
<li>③output: 用Spark/MapReduce处理的数据输出到Hadoop上，这个过程可以用到压缩。</li>
</ul>
<p>这三种场景，是不是需要用到压缩？用到的话用什么压缩格式最合理？不同的压缩格式，速度、压缩比、是否支持split都不一样。该怎样选择？</p>
<p>做一下 hadoop源码编译 让hadoop支持常用的压缩。</p>
<h2 id="13压缩方式以及split分片">1.3压缩方式以及split分片</h2>
<p>压缩方式:Bzip2、Gzip、Lzo、Lz4、Snappy、Zlip等<br>
它们的压缩比、压缩速度以及是否支持split分片这三个点的比较在之前的博客已经写的很详细。</p>
<p>之前的博客：<a href="https://blog.csdn.net/liweihope/article/details/89672763" target="_blank" rel="noopener">https://blog.csdn.net/liweihope/article/details/89672763</a></p>
<p>（非常重要）支持分片的只有BIZP2和LZO(LZO默认不支持，需要建index，才支持)。假如说，现在一个Snappy压缩的文件1T的大小，作为map的输入，但是它是不支持分片的，那么它就只能由一个map去处理，不能多个map并行处理了，那这1T的文件要处理猴年马月了。</p>
<p>之前的离线处理并没有采用压缩，所以它是默认的文本格式，它是支持分片的。<br>
所以不同的场景选择不同的压缩方式，没有哪一个压缩方式是一劳永逸。</p>
<h2 id="14解析mapreduce阶段的压缩解压以及压缩方式选择">1.4解析MapReduce阶段的压缩解压以及压缩方式选择</h2>
<p>下面用张图来解析一下MapReduce阶段的压缩解压：</p>
<p><img src="https://img-blog.csdnimg.cn/2019111319415595.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzIxMjM2NQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>1.第一次传入压缩文件，应选用可以切片的压缩方式，否则整个文件将只有一个Map执行。Use Compressd Map Input:从HDFS中读取文件进行Mapreuce作业，如果数据很大，可以使用压缩并且选择支持分片的压缩方式（Bzip2,LZO），可以实现并行处理，提高效率，减少磁盘读取时间，同时选择合适的存储格式例如Sequence Files，RC,ORC等。</p>
<p>2.第二次压缩应选择压缩解压速度快的压缩方式，生产中，Map阶段数据落盘通常使用snappy压缩格式（快速压缩解压）。Compress Intermediate Data:Map输出作为Reducer的输入，需要经过shuffle这一过程，需要把数据读取到一个环形缓冲区，然后读取到本地磁盘，所以选择压缩可以减少了存储文件所占空间，提升了数据传输速率，建议使用压缩速度快的压缩方式，例如Snappy和LZO。</p>
<p>3.第三次压缩有两种场景分别是：一.当输出文件为下一个job的输入，选择可切分的压缩方式例如：BZip2。二.当输出文件直接存到HDFS，作为归档，选择压缩比高的压缩方式。reduce阶段数据落盘通常使用gzip或bzip2进行压缩（减少磁盘使用）。Compress Reducer Output:进行归档处理或者链接Mapreduce的工作（该作业的输出作为下个作业的输入），压缩可以减少了存储文件所占空间，提升了数据传输速率，如果作为归档处理，可以采用高的压缩比（Gzip,Bzip2），如果作为下个作业的输入，考虑是否要分片进行选择。</p>
<h1 id="2如何配置压缩方式">2.如何配置压缩方式</h1>
<ul>
<li>core-site.xml里面配置压缩</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;io.compression.codecs&lt;/name&gt;</span><br><span class="line">&lt;value&gt;</span><br><span class="line">org.apache.hadoop.io.compress.GzipCodec,</span><br><span class="line">org.apache.hadoop.io.compress.DefaultCodec,</span><br><span class="line">org.apache.hadoop.io.compress.BZip2Codec,</span><br><span class="line">org.apache.hadoop.io.compress.SnappyCodec</span><br><span class="line">&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>
<p>用不同的压缩方式要配置不同的CodeC<br>
可以去官网的core-default.xml的默认配置里看io.compression.codecs相关说明。<br>
在这里只是配置了各种压缩方式的codec，还没有使用，真正使用是在mapred-site.xml配置。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">&lt;!-- 配置MapReduce段最终输出的压缩,BZip2，官方默认为false，不压缩--&gt;	</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;mapreduce.output.fileoutputformat.compress&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;mapreduce.output.fileoutputformat.compress.codec&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;org.apache.hadoop.io.compress.BZip2Codec&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;	</span><br><span class="line"></span><br><span class="line">&lt;!-- 配置 Map段输出的压缩,snappy,这里先配置上，为false，后面需要再改成true--&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">      &lt;name&gt;mapreduce.map.output.compress&lt;/name&gt; </span><br><span class="line">      &lt;value&gt;false&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">              </span><br><span class="line">  &lt;property&gt;</span><br><span class="line">      &lt;name&gt;mapreduce.map.output.compress.codec&lt;/name&gt; </span><br><span class="line">      &lt;value&gt;org.apache.hadoop.io.compress.SnappyCodec&lt;/value&gt;</span><br><span class="line">   &lt;/property&gt;</span><br></pre></td></tr></table></figure>
<p>可以去官网的mapred-default.xml的默认配置里看mapreduce.output.fileoutputformat.compress相关说明。</p>
<p><img src="https://img-blog.csdnimg.cn/20191113194259137.png" alt="在这里插入图片描述"></p>
<p>另外注意：<value>com.hadoop.compression.lzo.LzoCodec</value>如果value这样配置默认结尾是lzo_deflate，如果com.hadoop.compression.lzo.LzopCodec这样配置会生成.lzo结尾的文件。<br>
LzoCodec和LzopCodec区别：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">1.LzoCodec比LzopCodec更快， LzopCodec为了兼容LZOP程序添加了如 bytes signature, header等信息</span><br><span class="line">2.如果使用 LzoCodec作为Reduce输出，则输出文件扩展名为”.lzo_deflate”，它无法被lzop读取；</span><br><span class="line">3.如果使用LzopCodec作为Reduce输出，则扩展名为”.lzo”，它可以被lzop读取 生成lzo index job的”DistributedLzoIndexer“无法为 LzoCodec即 “.lzo_deflate”扩展名的文件创建index</span><br><span class="line">4.”.lzo_deflate“文件无法作为MapReduce输入，”.LZO”文件则可以。</span><br></pre></td></tr></table></figure>
<p>综上所述得出最佳实践：map输出的中间数据使用 LzoCodec，reduce输出使用 LzopCodec</p>
<h1 id="3压缩方式的使用">3.压缩方式的使用</h1>
<p>上面配置的是MapReduce输出是Bzip2压缩，运行一个Wordcount案例：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 mapreduce]$ hadoop jar hadoop-mapreduce-examples-2.6.0-cdh5.7.0.jar wordcount /tmp/input/hive_wc.txt /tmp/output/</span><br><span class="line"></span><br><span class="line">[hadoop@hadoop001 mapreduce]$ hdfs dfs -ls /tmp/output</span><br><span class="line">Found 2 items</span><br><span class="line">-rw-r--r--   1 hadoop supergroup          0 2019-04-30 02:46 /tmp/output/_SUCCESS</span><br><span class="line">-rw-r--r--   1 hadoop supergroup         61 2019-04-30 02:46 /tmp/output/part-r-00000.bz2</span><br><span class="line">#注意这里查看压缩文件，它要用Bzip2工厂，加载数据并初始化bzip2本地库，然后看到数据</span><br><span class="line">[hadoop@hadoop001 mapreduce]$ hdfs dfs -text /tmp/output/part-r-00000.bz2</span><br><span class="line">19/04/30 02:49:24 INFO bzip2.Bzip2Factory: Successfully loaded &amp; initialized native-bzip2 library system-native</span><br><span class="line">19/04/30 02:49:24 INFO compress.CodecPool: Got brand-new decompressor [.bz2]</span><br><span class="line">hello   4</span><br><span class="line">welcome 1</span><br><span class="line">world   2</span><br><span class="line">[hadoop@hadoop001 mapreduce]$</span><br></pre></td></tr></table></figure>
<p>从上面可以看出，压缩格式目前是BZip2格式了。而且从上面可以看出，对于压缩或者不压缩，对于用户来说，它是不感知的，没有感觉的。</p>
<h1 id="4压缩在hive中的使用方法">4.压缩在Hive中的使用方法</h1>
<h2 id="41不用压缩的情况">4.1不用压缩的情况</h2>
<p>现在在hive上创建一张表，然后把数据加载进去。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">create table page_views(</span><br><span class="line">track_time string,</span><br><span class="line">url string,</span><br><span class="line">session_id string,</span><br><span class="line">referer string,</span><br><span class="line">ip string,</span><br><span class="line">end_user_id string,</span><br><span class="line">city_id string</span><br><span class="line">) row format delimited fields terminated by &apos;\t&apos;;</span><br><span class="line"></span><br><span class="line">hive (g6)&gt; load data local inpath &apos;/home/hadoop/data/page_views.dat&apos; overwrite into table page_views;</span><br><span class="line">Loading data to table g6.page_views</span><br><span class="line">Table g6.page_views stats: [numFiles=1, numRows=0, totalSize=19014993, rawDataSize=0]</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.9 seconds</span><br><span class="line">hive (g6)&gt; select * from page_views limit 1;</span><br><span class="line">OK</span><br><span class="line">track_time      url     session_id      referer ip      end_user_id     city_id</span><br><span class="line">2013-05-19 13:00:00     http://www.taobao.com/17/?tracker_u=1624169&amp;type=1      B58W48U4WKZCJ5D1T3Z9ZY88RU7QA7B1       http://hao.360.cn/       1.196.34.243    NULL    -1</span><br><span class="line">Time taken: 0.483 seconds, Fetched: 1 row(s)</span><br><span class="line">hive (g6)&gt;</span><br></pre></td></tr></table></figure>
<p>然后看一下没有用压缩的大小：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 ~]$ hdfs dfs -du -s -h /user/hive/warehouse/g6.db/page_views</span><br><span class="line">18.1 M  18.1 M  /user/hive/warehouse/g6.db/page_views</span><br><span class="line">[hadoop@hadoop001 ~]$</span><br></pre></td></tr></table></figure>
<h2 id="42hive表中使用压缩的情况">4.2hive表中使用压缩的情况</h2>
<p>在Hive中如何使用压缩呢？<br>
官网上有相关说明：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">#网址：https://cwiki.apache.org/confluence/display/Hive/CompressedStorage</span><br><span class="line">#在Hive表中保持压缩，比不压缩存储具有更好的性能，无论是在磁盘使用率还是查询性能方面</span><br><span class="line">Compressed Data Storage</span><br><span class="line">Keeping data compressed in Hive tables has, in some cases, </span><br><span class="line">been known to give better performance than uncompressed storage; </span><br><span class="line">both in terms of disk usage and query performance.</span><br><span class="line"></span><br><span class="line">You can import text files compressed with Gzip or Bzip2 directly into a table stored as TextFile. </span><br><span class="line">The compression will be detected automatically </span><br><span class="line">and the file will be decompressed on-the-fly during query execution.</span><br></pre></td></tr></table></figure>
<p>在hive中用如下命令设置压缩：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">#默认是不压缩的，你要自己去设置成true</span><br><span class="line">hive (g6)&gt; SET hive.exec.compress.output;</span><br><span class="line">hive.exec.compress.output=false</span><br><span class="line">hive (g6)&gt; SET hive.exec.compress.output=true;</span><br></pre></td></tr></table></figure>
<p>而且可以看一下使用的压缩方式：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">hive (g6)&gt; set io.compression.codecs;</span><br><span class="line">io.compression.codecs=</span><br><span class="line">            org.apache.hadoop.io.compress.GzipCodec,</span><br><span class="line">            org.apache.hadoop.io.compress.DefaultCodec,</span><br><span class="line">            org.apache.hadoop.io.compress.BZip2Codec,</span><br><span class="line">            org.apache.hadoop.io.compress.SnappyCodec</span><br><span class="line">         </span><br><span class="line">hive (g6)&gt; set mapreduce.output.fileoutputformat.compress.codec;</span><br><span class="line">mapreduce.output.fileoutputformat.compress.codec=org.apache.hadoop.io.compress.BZip2Codec</span><br><span class="line">hive (g6)&gt;</span><br></pre></td></tr></table></figure>
<p>如果你想修改成什么压缩，自己都可以去修改：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">hive (g6)&gt; set mapreduce.output.fileoutputformat.compress.codec=org.apache.hadoop.io.compress.GzipCodec;</span><br><span class="line">hive (g6)&gt; set mapreduce.output.fileoutputformat.compress.codec;</span><br><span class="line">mapreduce.output.fileoutputformat.compress.codec=org.apache.hadoop.io.compress.GzipCodec</span><br><span class="line">hive (g6)&gt; set mapreduce.output.fileoutputformat.compress.codec=org.apache.hadoop.io.compress.BZip2Codec;</span><br><span class="line">hive (g6)&gt; set mapreduce.output.fileoutputformat.compress.codec;</span><br><span class="line">mapreduce.output.fileoutputformat.compress.codec=org.apache.hadoop.io.compress.BZip2Codec</span><br><span class="line">hive (g6)&gt;</span><br></pre></td></tr></table></figure>
<p>上面设置了hive压缩为Bzip2，现在再创建一张表，</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (g6)&gt; create table page_views_bzip2</span><br><span class="line">         &gt; as select * from page_views;</span><br></pre></td></tr></table></figure>
<p>然后看一下大小：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">#Hive未开启压缩之前是18.1M，开启Bzip2压缩后是3.6M</span><br><span class="line">[hadoop@hadoop001 ~]$ hdfs dfs -du -s -h /user/hive/warehouse/g6.db/page_views_bzip2</span><br><span class="line">3.6 M  3.6 M  /user/hive/warehouse/g6.db/page_views_bzip2</span><br><span class="line">[hadoop@hadoop001 ~]$</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/20191113194536788.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzIxMjM2NQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h1 id="5hive中数据文件的存储结构或者说存储格式文件格式">5.Hive中数据文件的存储结构（或者说存储格式/文件格式）</h1>
<h2 id="51hive常见的存储结构">5.1hive常见的存储结构</h2>
<p>去官网可以看到hive里常用的文件格式：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">#这里要知道SEQUENCEFILE、TEXTFILE、RCFILE、ORC、PARQUET这五个</span><br><span class="line">file_format:</span><br><span class="line">  : SEQUENCEFILE</span><br><span class="line">  | TEXTFILE    -- (Default, depending on hive.default.fileformat configuration)</span><br><span class="line">  | RCFILE      -- (Note: Available in Hive 0.6.0 and later)</span><br><span class="line">  | ORC         -- (Note: Available in Hive 0.11.0 and later)</span><br><span class="line">  | PARQUET     -- (Note: Available in Hive 0.13.0 and later)</span><br><span class="line">  | AVRO        -- (Note: Available in Hive 0.14.0 and later)</span><br><span class="line">  | JSONFILE    -- (Note: Available in Hive 4.0.0 and later)</span><br><span class="line">  | INPUTFORMAT input_format_classname OUTPUTFORMAT output_format_classname</span><br></pre></td></tr></table></figure>
<p>生产上如果采用列式存储（行业里面差不多99%都使用列式存储），用的最多的是：ORC和PARQUET。<br>
另外</p>
<ul>
<li>SEQUENCEFILE：生产中绝对不会用，key-vvalue格式，比源文本格式占用磁盘更多</li>
<li>TEXTFILE：生产中用的多，行式存储</li>
<li>RCFILE：生产中不会用，行列混合存储</li>
<li>ORC：生产中最常用，列式存储，基于ORC优化的</li>
<li>PARQUET：生产中最常用，列式存储</li>
<li>AVRO：生产中几乎不用，不用考虑</li>
<li>JSONFILE：生产中几乎不用，不用考虑</li>
<li>INPUTFORMAT：生产中几乎不用，不用考虑</li>
</ul>
<h2 id="52行式存储与列式存储">5.2行式存储与列式存储</h2>
<p>什么是行式存储？什么叫列式存储？各自的优缺点是什么？</p>
<p><img src="https://img-blog.csdnimg.cn/2019111319464351.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzIxMjM2NQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>从图上可以看出，</p>
<p>行式存储：一行数据一定是在同一个block块中，可能是多行存储在一个块中，任何的select 底层都是全字段查询。</p>
<p>列式存储：一行数据不同列可以在不同的block块中。</p>
<p><strong>行式存储：</strong></p>
<p>可以进行压缩。</p>
<ul>
<li>优点：select *from xxx 所有数据都出来了，因为它所有的数据都在一个块里面。</li>
<li>缺点：select a,c from xxx 你仅仅只想要a、c两列，但是它也会把所有数据都加载进来，因为它是行式存储，它是一行一行的读的。</li>
</ul>
<p><strong>列式存储：</strong></p>
<p>假如A、B存在一个块里面，C在一个块里面，D在一个块里面</p>
<ul>
<li>优点：如果你去select a,c from xxx ，那么它只会前面两个块去读，跟第三个块之间不会发生IO</li>
<li>缺点：select *from xxx 你去查询所有列，那么现在它的列分别在三个块里面，会发生数据重组<br>
行业里面差不多99%都使用列式存储</li>
</ul>
<h2 id="53各种存储结构的举例对比">5.3各种存储结构的举例对比</h2>
<h3 id="531文本格式textfile">5.3.1文本格式TEXTFILE</h3>
<p>存储方式：行存储；</p>
<p>默认格式，数据不做压缩，磁盘开销大，数据解析开销大。可结合Gzip、Bzip2使用(系统自动检查，执行查询时自动解压)，但使用这种方式，hive不会对数据进行切分，从而无法对数据进行并行操作。</p>
<h3 id="532-sequencefile">5.3.2 SEQUENCEFILE</h3>
<p>SEQUENCEFILE：它是以序列化的方式存储格式，一种&lt;key,value&gt;的格式。<br>
简单了解一下即可。</p>
<p>现在创建一张存储格式为sequencefile 的表：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">create table page_views_seq(</span><br><span class="line">track_time string,</span><br><span class="line">url string,</span><br><span class="line">session_id string,</span><br><span class="line">referer string,</span><br><span class="line">ip string,</span><br><span class="line">end_user_id string,</span><br><span class="line">city_id string</span><br><span class="line">) row format delimited fields terminated by &apos;\t&apos;</span><br><span class="line">stored as sequencefile ;</span><br></pre></td></tr></table></figure>
<p>然后把数据加载进来：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">hive (g6)&gt; load data local inpath &apos;/home/hadoop/data/page_views.dat&apos; into table page_views_seq;</span><br><span class="line">Loading data to table g6.page_views_seq</span><br><span class="line">Failed with exception Wrong file format. Please check the file&apos;s format.</span><br><span class="line">FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.MoveTask</span><br><span class="line">hive (g6)&gt; </span><br><span class="line">#报错原因是：原始page_views.dat文件是textfile文本文件，直接导入到sequencefile，是导不进去的。</span><br><span class="line">#这个时候需要借助一张临时表把它导进去</span><br><span class="line">hive (g6)&gt; insert overwrite table ppage_views_seq select * from page_views ; </span><br><span class="line">#这样就可以了</span><br></pre></td></tr></table></figure>
<p>然后看一下大小</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">#sequencefile 文件大小20.6 M </span><br><span class="line">[hadoop@hadoop001 ~]$ hdfs dfs -du -s -h /user/hive/warehouse/g6.db/page_views_seq</span><br><span class="line">20.6 M  20.6 M  /user/hive/warehouse/g6.db/page_views_seq</span><br><span class="line">#sequencefile 文件名</span><br><span class="line">[hadoop@hadoop001 ~]$ hdfs dfs -ls /user/hive/warehouse/g6.db/page_views_seq</span><br><span class="line">Found 1 items</span><br><span class="line">-rwxr-xr-x   1 hadoop supergroup   21603675 2019-04-30 14:58 /user/hive/warehouse/g6.db/page_views_seq/000000_0</span><br><span class="line">#原始文本文件大小18.1 M</span><br><span class="line">[hadoop@hadoop001 ~]$ hdfs dfs -du -s -h /user/hive/warehouse/g6.db/page_views/</span><br><span class="line">18.1 M  18.1 M  /user/hive/warehouse/g6.db/page_views</span><br><span class="line">[hadoop@hadoop001 ~]$</span><br></pre></td></tr></table></figure>
<p>可以看出，<strong>用了sequencefile 存储格式之后，文件更大了，生产上面是绝对不会用的。</strong></p>
<p>为什么用了之后文件更大了？</p>
<p>因为它存储的东西更多了，比如记录的长度、key的长度、key的值、value的值等。<br>
简单了解一下即可。</p>
<h3 id="533-rcfile">5.3.3 RCFILE</h3>
<p>不是严格意义的列式存储，它是行列混合的一个存储，这个架构是利用了先行式存储再列式存储的方式，它的性能很低。<br>
对查询并没有什么大的提升，只是节省了10%的空间。</p>
<p><strong>生产上绝对不会用的。</strong></p>
<p>建一个存储为rcfile的表：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">create table page_views_rc(</span><br><span class="line">track_time string,</span><br><span class="line">url string,</span><br><span class="line">session_id string,</span><br><span class="line">referer string,</span><br><span class="line">ip string,</span><br><span class="line">end_user_id string,</span><br><span class="line">city_id string</span><br><span class="line">) row format delimited fields terminated by &apos;\t&apos;</span><br><span class="line">stored as rcfile ;</span><br><span class="line">#插入数据：</span><br><span class="line">hive (g6)&gt; insert overwrite table page_views_rc select * from page_views ;</span><br><span class="line">#看一下大小：</span><br><span class="line">[hadoop@hadoop001 ~]$  hdfs dfs -du -s -h /user/hive/warehouse/g6.db/page_views_rc</span><br><span class="line">17.9 M  17.9 M  /user/hive/warehouse/g6.db/page_views_rc</span><br><span class="line">[hadoop@hadoop001 ~]$</span><br></pre></td></tr></table></figure>
<h3 id="534-orc重点掌握">5.3.4 ORC（重点掌握）</h3>
<p>它是优化过后的列式存储，它是基于RC又做了更好的底层的优化。它提供了一种非常高效的方式去存储hive上的数据。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">#官网</span><br><span class="line">https://cwiki.apache.org/confluence/display/Hive/LanguageManual+ORC#LanguageManualORC-ORCFiles</span><br><span class="line">#官网解释</span><br><span class="line">The Optimized Row Columnar (ORC) file format provides a highly efficient way to store Hive data.</span><br><span class="line">It was designed to overcome limitations of the other Hive file formats. </span><br><span class="line">Using ORC files improves performance when Hive is reading, writing, and processing data.</span><br></pre></td></tr></table></figure>
<p>具体原理看官网和百度。<br>
现在创建一张存储为ORC的表：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">create table page_views_orc(</span><br><span class="line">track_time string,</span><br><span class="line">url string,</span><br><span class="line">session_id string,</span><br><span class="line">referer string,</span><br><span class="line">ip string,</span><br><span class="line">end_user_id string,</span><br><span class="line">city_id string</span><br><span class="line">) row format delimited fields terminated by &apos;\t&apos;</span><br><span class="line">stored as orc ;</span><br><span class="line"></span><br><span class="line">hive (g6)&gt; insert overwrite table page_views_orc select * from page_views;</span><br></pre></td></tr></table></figure>
<p>然后查看大小：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#原来18.1M，现在2.8M，压缩还是比较厉害的</span><br><span class="line">[hadoop@hadoop001 ~]$ hdfs dfs -du -s -h /user/hive/warehouse/g6.db/page_views_orc</span><br><span class="line">2.8 M  2.8 M  /user/hive/warehouse/g6.db/page_views_orc</span><br></pre></td></tr></table></figure>
<p>原来18.1M，现在2.8M，压缩还是比较厉害的，因为看官网，它默认会自动使用ZLIB压缩。ZLIB压缩也可以手动去掉，或者手动指定其它压缩。</p>
<p><img src="https://img-blog.csdnimg.cn/20191113195027520.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzIxMjM2NQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>现在来手动指定不压缩，创建一张表：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">create table page_views_orc_none</span><br><span class="line">stored as orc tblproperties (&quot;orc.compress&quot;=&quot;NONE&quot;) </span><br><span class="line">as select * from page_views;</span><br></pre></td></tr></table></figure>
<p>然后看一下大小：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">#能看出来，原来18.1M，现在即使不压缩，也是7.7M，用zlib压缩的话是2.8M，这种压缩比是非常高的</span><br><span class="line">[hadoop@hadoop001 ~]$ hdfs dfs -du -s -h /user/hive/warehouse/g6.db/page_views_orc_none</span><br><span class="line">7.7 M  7.7 M  /user/hive/warehouse/g6.db/page_views_orc_none</span><br><span class="line">[hadoop@hadoop001 ~]$</span><br></pre></td></tr></table></figure>
<p><strong>能看出来，采用ORC，原来18.1M，现在即使不压缩，也是7.7M，用zlib压缩的话是2.8M，这种压缩比是非常高的。</strong></p>
<h3 id="535-parquet重点掌握">5.3.5 Parquet（重点掌握）</h3>
<p>生产上ORC和Parquet半斤八两，都可以。但是从空间上来看，ORC可能会好一些。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#官网</span><br><span class="line">https://cwiki.apache.org/confluence/display/Hive/Parquet</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/20191113195122554.png" alt="在这里插入图片描述"></p>
<p>这些引擎都支持Parquet。</p>
<p>下面通过案例来演示一遍如何使用以及效果。</p>
<p>对于Parquet不同的版本用不同的语法。不过在这里，直接按照下面这样就可以了。<br>
而且Parquet设置压缩和以上的存储形式也不一样。记住就行了。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">hive (g6)&gt; set parquet.compression=gzip;</span><br><span class="line">hive (g6)&gt; set parquet.compression;</span><br><span class="line">parquet.compression=gzip</span><br><span class="line"></span><br><span class="line">create table page_views_parquet_gzip</span><br><span class="line">stored as parquet </span><br><span class="line">as select * from page_views;</span><br></pre></td></tr></table></figure>
<p>然后看一下大小：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">#可以看出原来18.1M，现在使用parquet后是3.9M</span><br><span class="line">[hadoop@hadoop001 ~]$ hdfs dfs -du -s -h /user/hive/warehouse/g6.db/page_views_parquet_gzip</span><br><span class="line">3.9 M  3.9 M  /user/hive/warehouse/g6.db/page_views_parquet_gzip</span><br><span class="line">[hadoop@hadoop001 ~]$</span><br></pre></td></tr></table></figure>
<h3 id="536-总结">5.3.6 总结</h3>
<p>他们各自使用压缩，加上自己的存储格式，能够使磁盘空间减少非常多，效果非常好。<br>
三个选择，进来的时候，是textfile文本格式，不用动，如果要是列式存储的话，就选择ORC和Parquet。</p>
<h3 id="537-几种存储格式的查询性能举例比较">5.3.7 几种存储格式的查询性能举例比较</h3>
<p>以上都是从存储的角度来考虑，那么查询的性能如何考虑？</p>
<p>下面通过一个案例来进行分析;<br>
现在通过一个session_id来查询统计数据量</p>
<h4 id="5371查询普通格式的表textfile格式">5.3.7.1查询普通格式的表（textfile格式）</h4>
<p>先查一个普通的表：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">hive (g6)&gt; select count(*) from page_views where session_id=&apos;B58W48U4WKZCJ5D1T3Z9ZY88RU7QA7B1&apos;;</span><br><span class="line">...此处省略100字</span><br><span class="line">MapReduce Jobs Launched: </span><br><span class="line">Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 3.54 sec   HDFS Read: 19022693 HDFS Write: 2 SUCCESS</span><br><span class="line">Total MapReduce CPU Time Spent: 3 seconds 540 msec</span><br><span class="line">OK</span><br><span class="line">_c0</span><br><span class="line">6</span><br><span class="line">Time taken: 33.775 seconds, Fetched: 1 row(s)</span><br><span class="line">hive (g6)&gt;</span><br></pre></td></tr></table></figure>
<p>从HDFS Read: 19022693（就是那个18.1M）这里可以看出，去查询普通的表，它会全表扫描。</p>
<h4 id="5372-查询存储为rc格式的表">5.3.7.2 查询存储为RC格式的表</h4>
<p>然后现在查询一下RC的表：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">hive (g6)&gt; select count(*) from page_views_rc where session_id=&apos;B58W48U4WKZCJ5D1T3Z9ZY88RU7QA7B1&apos;;</span><br><span class="line">...此处省略100字</span><br><span class="line">MapReduce Jobs Launched: </span><br><span class="line">Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 3.47 sec   HDFS Read: 3725349 HDFS Write: 2 SUCCESS</span><br><span class="line">Total MapReduce CPU Time Spent: 3 seconds 470 msec</span><br><span class="line">OK</span><br><span class="line">_c0</span><br><span class="line">6</span><br><span class="line">Time taken: 33.473 seconds, Fetched: 1 row(s)</span><br><span class="line">hive (g6)&gt;</span><br></pre></td></tr></table></figure>
<p>从HDFS Read: 3725349这里可以看出，它少了很多。</p>
<h4 id="5373-查询存储为orc格式的表">5.3.7.3 查询存储为ORC格式的表</h4>
<p>然后现在查询一下ORC的表(带有zlib压缩的)：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">hive (g6)&gt; select count(*) from page_views_orc where session_id=&apos;B58W48U4WKZCJ5D1T3Z9ZY88RU7QA7B1&apos;;</span><br><span class="line">...此处省略100字</span><br><span class="line">MapReduce Jobs Launched: </span><br><span class="line">Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 3.24 sec   HDFS Read: 1257469 HDFS Write: 2 SUCCESS</span><br><span class="line">Total MapReduce CPU Time Spent: 3 seconds 240 msec</span><br><span class="line">OK</span><br><span class="line">_c0</span><br><span class="line">6</span><br><span class="line">Time taken: 33.933 seconds, Fetched: 1 row(s)</span><br><span class="line">hive (g6)&gt;</span><br></pre></td></tr></table></figure>
<p>从HDFS Read: 1257469这里可以看出，它扫描的更少了，因为它里面是带了索引的。</p>
<h4 id="5374-查询存储为parquet格式的表">5.3.7.4 查询存储为Parquet格式的表</h4>
<p>然后现在查询一下parquet的表（带有gzip压缩的）：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">hive (g6)&gt; select count(*) from page_views_parquet_gzip where session_id=&apos;B58W48U4WKZCJ5D1T3Z9ZY88RU7QA7B1&apos;;</span><br><span class="line">...此处省略100字</span><br><span class="line">MapReduce Jobs Launched: </span><br><span class="line">Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 4.32 sec   HDFS Read: 1624470 HDFS Write: 2 SUCCESS</span><br><span class="line">Total MapReduce CPU Time Spent: 4 seconds 320 msec</span><br><span class="line">OK</span><br><span class="line">_c0</span><br><span class="line">6</span><br><span class="line">Time taken: 35.694 seconds, Fetched: 1 row(s)</span><br><span class="line">hive (g6)&gt;</span><br></pre></td></tr></table></figure>
<p>从HDFS Read: 1624470这里可以看出，它扫描的比ORC的稍微多一些。<br>
以后要测性能，要从两方面考虑，一个是存储一个查询。</p>
<h1 id="总结">总结</h1>
<p>相同数据量的数据，列式存储（比如ORC、PARQUET）占的磁盘空间远低于行式存储（如textfile、Sequencefile），当查询部分字段是，列式存储的数据只需加载对应的列数据即可。 存少读少，极大磁盘使用以及IO，相对减少资源（内存，cpu）不必要的浪费，故列式存储在大数据领域完爆行式存储，行业里面差不多99%都使用列式存储。</p>
<p>参考博客：<br>
<a href="https://blog.csdn.net/yu0_zhang0/article/details/79538398" target="_blank" rel="noopener">https://blog.csdn.net/yu0_zhang0/article/details/79538398</a><br>
<a href="https://blog.csdn.net/qq_32641659/article/details/89339143" target="_blank" rel="noopener">https://blog.csdn.net/qq_32641659/article/details/89339143</a><br>
<a href="https://blog.csdn.net/qq_26442553/article/details/80300714" target="_blank" rel="noopener">https://blog.csdn.net/qq_26442553/article/details/80300714</a></p>

                

                <hr>
                <!-- Pager -->
                <ul class="pager">
                    
                        <li class="previous">
                            <a href="/article/49压缩的好处和坏处/" data-toggle="tooltip" data-placement="top" title="[压缩的好处和坏处]  ">&larr; Previous Post</a>
                        </li>
                    
                    
                        <li class="next">
                            <a href="/article/47Hadoop源码分析之WordCount/" data-toggle="tooltip" data-placement="top" title="[Hadoop源码分析之WordCount]  ">Next Post &rarr;</a>
                        </li>
                    
                </ul>

                <br>

                <!--打赏-->
                
                <!--打赏-->

                <br>
                <!--分享-->
                
                    <div class="social-share"  data-wechat-qrcode-helper="" align="center"></div>
                    <!--  css & js -->
                    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/social-share.js/1.0.16/css/share.min.css">
                    <script src="https://cdnjs.cloudflare.com/ajax/libs/social-share.js/1.0.16/js/social-share.min.js"></script>
                
                <!--分享-->
                <br>                       
                
                <!-- require APlayer -->
                

                <!-- duoshuo Share start -->
                
                <!-- 多说 Share end-->

                <!-- 多说评论框 start -->
                
                <!-- 多说评论框 end -->

                <!-- disqus comment start -->
                
                <!-- disqus comment end -->

                

            </div>
            
            <!-- Tabe of Content -->
            <!-- Table of Contents -->

  
    
      <aside id="sidebar">
        <div id="toc" class="toc-article">
        <strong class="toc-title">Contents</strong>
        
          <ol class="toc-nav"><li class="toc-nav-item toc-nav-level-1"><a class="toc-nav-link" href="#前言"><span class="toc-nav-number">1.</span> <span class="toc-nav-text">&#x524D;&#x8A00;</span></a></li><li class="toc-nav-item toc-nav-level-1"><a class="toc-nav-link" href="#1hadoop之mapreduce用到的压缩"><span class="toc-nav-number">2.</span> <span class="toc-nav-text">1.Hadoop&#x4E4B;MapReduce&#x7528;&#x5230;&#x7684;&#x538B;&#x7F29;</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#11压缩的优点缺点之间的较量"><span class="toc-nav-number">2.1.</span> <span class="toc-nav-text">1.1&#x538B;&#x7F29;&#x7684;&#x4F18;&#x70B9;&#x7F3A;&#x70B9;&#x4E4B;&#x95F4;&#x7684;&#x8F83;&#x91CF;</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#12压缩使用的场景"><span class="toc-nav-number">2.2.</span> <span class="toc-nav-text">1.2&#x538B;&#x7F29;&#x4F7F;&#x7528;&#x7684;&#x573A;&#x666F;</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#13压缩方式以及split分片"><span class="toc-nav-number">2.3.</span> <span class="toc-nav-text">1.3&#x538B;&#x7F29;&#x65B9;&#x5F0F;&#x4EE5;&#x53CA;split&#x5206;&#x7247;</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#14解析mapreduce阶段的压缩解压以及压缩方式选择"><span class="toc-nav-number">2.4.</span> <span class="toc-nav-text">1.4&#x89E3;&#x6790;MapReduce&#x9636;&#x6BB5;&#x7684;&#x538B;&#x7F29;&#x89E3;&#x538B;&#x4EE5;&#x53CA;&#x538B;&#x7F29;&#x65B9;&#x5F0F;&#x9009;&#x62E9;</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-1"><a class="toc-nav-link" href="#2如何配置压缩方式"><span class="toc-nav-number">3.</span> <span class="toc-nav-text">2.&#x5982;&#x4F55;&#x914D;&#x7F6E;&#x538B;&#x7F29;&#x65B9;&#x5F0F;</span></a></li><li class="toc-nav-item toc-nav-level-1"><a class="toc-nav-link" href="#3压缩方式的使用"><span class="toc-nav-number">4.</span> <span class="toc-nav-text">3.&#x538B;&#x7F29;&#x65B9;&#x5F0F;&#x7684;&#x4F7F;&#x7528;</span></a></li><li class="toc-nav-item toc-nav-level-1"><a class="toc-nav-link" href="#4压缩在hive中的使用方法"><span class="toc-nav-number">5.</span> <span class="toc-nav-text">4.&#x538B;&#x7F29;&#x5728;Hive&#x4E2D;&#x7684;&#x4F7F;&#x7528;&#x65B9;&#x6CD5;</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#41不用压缩的情况"><span class="toc-nav-number">5.1.</span> <span class="toc-nav-text">4.1&#x4E0D;&#x7528;&#x538B;&#x7F29;&#x7684;&#x60C5;&#x51B5;</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#42hive表中使用压缩的情况"><span class="toc-nav-number">5.2.</span> <span class="toc-nav-text">4.2hive&#x8868;&#x4E2D;&#x4F7F;&#x7528;&#x538B;&#x7F29;&#x7684;&#x60C5;&#x51B5;</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-1"><a class="toc-nav-link" href="#5hive中数据文件的存储结构或者说存储格式文件格式"><span class="toc-nav-number">6.</span> <span class="toc-nav-text">5.Hive&#x4E2D;&#x6570;&#x636E;&#x6587;&#x4EF6;&#x7684;&#x5B58;&#x50A8;&#x7ED3;&#x6784;&#xFF08;&#x6216;&#x8005;&#x8BF4;&#x5B58;&#x50A8;&#x683C;&#x5F0F;/&#x6587;&#x4EF6;&#x683C;&#x5F0F;&#xFF09;</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#51hive常见的存储结构"><span class="toc-nav-number">6.1.</span> <span class="toc-nav-text">5.1hive&#x5E38;&#x89C1;&#x7684;&#x5B58;&#x50A8;&#x7ED3;&#x6784;</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#52行式存储与列式存储"><span class="toc-nav-number">6.2.</span> <span class="toc-nav-text">5.2&#x884C;&#x5F0F;&#x5B58;&#x50A8;&#x4E0E;&#x5217;&#x5F0F;&#x5B58;&#x50A8;</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#53各种存储结构的举例对比"><span class="toc-nav-number">6.3.</span> <span class="toc-nav-text">5.3&#x5404;&#x79CD;&#x5B58;&#x50A8;&#x7ED3;&#x6784;&#x7684;&#x4E3E;&#x4F8B;&#x5BF9;&#x6BD4;</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#531文本格式textfile"><span class="toc-nav-number">6.3.1.</span> <span class="toc-nav-text">5.3.1&#x6587;&#x672C;&#x683C;&#x5F0F;TEXTFILE</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#532-sequencefile"><span class="toc-nav-number">6.3.2.</span> <span class="toc-nav-text">5.3.2 SEQUENCEFILE</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#533-rcfile"><span class="toc-nav-number">6.3.3.</span> <span class="toc-nav-text">5.3.3 RCFILE</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#534-orc重点掌握"><span class="toc-nav-number">6.3.4.</span> <span class="toc-nav-text">5.3.4 ORC&#xFF08;&#x91CD;&#x70B9;&#x638C;&#x63E1;&#xFF09;</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#535-parquet重点掌握"><span class="toc-nav-number">6.3.5.</span> <span class="toc-nav-text">5.3.5 Parquet&#xFF08;&#x91CD;&#x70B9;&#x638C;&#x63E1;&#xFF09;</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#536-总结"><span class="toc-nav-number">6.3.6.</span> <span class="toc-nav-text">5.3.6 &#x603B;&#x7ED3;</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#537-几种存储格式的查询性能举例比较"><span class="toc-nav-number">6.3.7.</span> <span class="toc-nav-text">5.3.7 &#x51E0;&#x79CD;&#x5B58;&#x50A8;&#x683C;&#x5F0F;&#x7684;&#x67E5;&#x8BE2;&#x6027;&#x80FD;&#x4E3E;&#x4F8B;&#x6BD4;&#x8F83;</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#5371查询普通格式的表textfile格式"><span class="toc-nav-number">6.3.7.1.</span> <span class="toc-nav-text">5.3.7.1&#x67E5;&#x8BE2;&#x666E;&#x901A;&#x683C;&#x5F0F;&#x7684;&#x8868;&#xFF08;textfile&#x683C;&#x5F0F;&#xFF09;</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#5372-查询存储为rc格式的表"><span class="toc-nav-number">6.3.7.2.</span> <span class="toc-nav-text">5.3.7.2 &#x67E5;&#x8BE2;&#x5B58;&#x50A8;&#x4E3A;RC&#x683C;&#x5F0F;&#x7684;&#x8868;</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#5373-查询存储为orc格式的表"><span class="toc-nav-number">6.3.7.3.</span> <span class="toc-nav-text">5.3.7.3 &#x67E5;&#x8BE2;&#x5B58;&#x50A8;&#x4E3A;ORC&#x683C;&#x5F0F;&#x7684;&#x8868;</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#5374-查询存储为parquet格式的表"><span class="toc-nav-number">6.3.7.4.</span> <span class="toc-nav-text">5.3.7.4 &#x67E5;&#x8BE2;&#x5B58;&#x50A8;&#x4E3A;Parquet&#x683C;&#x5F0F;&#x7684;&#x8868;</span></a></li></ol></li></ol></li></ol></li><li class="toc-nav-item toc-nav-level-1"><a class="toc-nav-link" href="#总结"><span class="toc-nav-number">7.</span> <span class="toc-nav-text">&#x603B;&#x7ED3;</span></a></li></ol>
        
        </div>
      </aside>
    

                
            <!-- Sidebar Container -->
            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                sidebar-container">

                <!-- Featured Tags -->
                
                <section>
                    <!-- no hr -->
                    <h5><a href="/tags/">FEATURED TAGS</a></h5>
                    <div class="tags">
                       
                          <a class="tag" href="/tags/#Hadoop" title="Hadoop">Hadoop</a>
                        
                    </div>
                </section>
                

                <!-- Friends Blog -->
                
                <hr>
                <h5>FRIENDS</h5>
                <ul class="list-inline">

                    
                        <li><a href="https://sxwanggit126.github.io" target="_blank">DoubleHappy</a></li>
                    
                </ul>
                
            </div>
        </div>
    </div>
</article>








<!-- async load function -->
<script>
    function async(u, c) {
      var d = document, t = 'script',
          o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
      s.parentNode.insertBefore(o, s);
    }
</script>
<!-- anchor-js, Doc:http://bryanbraun.github.io/anchorjs/ -->
<script>
    async("https://cdn.bootcss.com/anchor-js/1.1.1/anchor.min.js",function(){
        anchors.options = {
          visible: 'hover',
          placement: 'left',
          icon: 'rz'
        };
        anchors.add().remove('.intro-header h1').remove('.subheading').remove('.sidebar-container h5');
    })
</script>
<style>
    /* place left on bigger screen */
    @media all and (min-width: 800px) {
        .anchorjs-link{
            position: absolute;
            left: -0.75em;
            font-size: 1.1em;
            margin-top : -0.1em;
        }
    }
</style>


<!-- chrome Firefox 中文锚点定位失效-->
<script src="https://cdn.bootcss.com/jquery/3.3.1/jquery.js"></script>
<!-- smooth scroll behavior polyfill  -->
<script type="text/javascript" src="/js/smoothscroll.js"></script>
<script>
        $('#toc').on('click','a',function(a){
            // var isChrome = window.navigator.userAgent.indexOf("Chrome") !== -1;
            // console.log(window.navigator.userAgent,isChrome)
                // if(isChrome) {
                    // console.log(a.currentTarget.outerHTML);
                    // console.log($(a.currentTarget).attr("href"));
                    //跳转到指定锚点
                    // document.getElementById(a.target.innerText.toLowerCase()).scrollIntoView(true);
                    document.getElementById($(a.currentTarget).attr("href").replace("#","")).scrollIntoView({behavior: 'smooth' });
                // }
        })  
</script>


    <!-- Footer -->
    <!-- Footer -->
<footer>
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <ul class="list-inline text-center">
                
                
                

                

                

                
                    <li>
                        <a target="_blank"  href="https://github.com/LeoFWZ">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                

                

                </ul>
                <p class="copyright text-muted">
                    Copyright &copy; FWZ 2020 
                    By <a href="https://leofwz.github.io/">LeoFWZ</a> | BigData
                </p>
            </div>
        </div>
    </div>
</footer>

<!-- jQuery -->
<script src="/js/jquery.min.js"></script>

<!-- Bootstrap Core JavaScript -->
<script src="/js/bootstrap.min.js"></script>

<!-- Custom Theme JavaScript -->
<script src="/js/hux-blog.min.js"></script>


<!-- async load function -->
<script>
    function async(u, c) {
      var d = document, t = 'script',
          o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
      s.parentNode.insertBefore(o, s);
    }
</script>

<!-- 
     Because of the native support for backtick-style fenced code blocks 
     right within the Markdown is landed in Github Pages, 
     From V1.6, There is no need for Highlight.js, 
     so Huxblog drops it officially.

     - https://github.com/blog/2100-github-pages-now-faster-and-simpler-with-jekyll-3-0  
     - https://help.github.com/articles/creating-and-highlighting-code-blocks/    
-->
<!--
    <script>
        async("http://cdn.bootcss.com/highlight.js/8.6/highlight.min.js", function(){
            hljs.initHighlightingOnLoad();
        })
    </script>
    <link href="http://cdn.bootcss.com/highlight.js/8.6/styles/github.min.css" rel="stylesheet">
-->


<!-- jquery.tagcloud.js -->
<script>
    // only load tagcloud.js in tag.html
    if($('#tag_cloud').length !== 0){
        async("https://leofwz.github.io/js/jquery.tagcloud.js",function(){
            $.fn.tagcloud.defaults = {
                //size: {start: 1, end: 1, unit: 'em'},
                color: {start: '#bbbbee', end: '#0085a1'},
            };
            $('#tag_cloud a').tagcloud();
        })
    }
</script>

<!--fastClick.js -->
<script>
    async("https://cdn.bootcss.com/fastclick/1.0.6/fastclick.min.js", function(){
        var $nav = document.querySelector("nav");
        if($nav) FastClick.attach($nav);
    })
</script>


<!-- Google Analytics -->


<script>
    // dynamic User by Hux
    var _gaId = 'UA-XXXXXXXX-X';
    var _gaDomain = 'yoursite';

    // Originial
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', _gaId, _gaDomain);
    ga('send', 'pageview');
</script>




<!-- Baidu Tongji -->

<script>
    // dynamic User by Hux
    var _baId = 'xxx';

    // Originial
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "//hm.baidu.com/hm.js?" + _baId;
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
</script>






	<a id="rocket" href="#top" class=""></a>
	<script type="text/javascript" src="/js/totop.js?v=1.0.0" async=""></script>
    <script type="text/javascript" src="/js/toc.js?v=1.0.0" async=""></script>
<!-- Image to hack wechat -->
<img src="https://leofwz.github.io/img/icon_wechat.png" width="0" height="0" />
<!-- Migrate from head to bottom, no longer block render and still work -->

</body>

</html>
